{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8938265,"sourceType":"datasetVersion","datasetId":5377911}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Модель предсказывающая оценку по отзыву","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\nimport torch\nfrom torch import nn\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\n\n# Устанавливаем сид для воспроизводимости результатов\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\ndf = pd.read_csv('cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1  # Классы от 0 до 4\n\ndef preprocess_text(text):\n    return text.lower().strip() if isinstance(text, str) else ''\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df.dropna(subset=['processed_text', 'rating_class'])\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\nprint(f\"Размер обучающей выборки: {len(train_df)}\")\nprint(f\"Размер тестовой выборки: {len(test_df)}\")\n\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=256)\n\ntokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\ntokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\n\ntokenized_train = tokenized_train.rename_column(\"rating_class\", \"labels\")\ntokenized_test = tokenized_test.rename_column(\"rating_class\", \"labels\")\n\nnum_labels = df['rating_class'].nunique()\n\nclass ClassificationModel(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        \n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n        \n        return (loss, logits) if loss is not None else logits\n\nmodel = ClassificationModel('cointegrated/rubert-tiny', num_labels)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    spearman_corr, _ = spearmanr(labels, preds)\n\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"spearman\": spearman_corr\n    }\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    warmup_steps=500,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    gradient_accumulation_steps=2,\n    fp16=True if torch.cuda.is_available() else False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\neval_results = trainer.evaluate()\nprint(\"Результаты оценки:\", eval_results)\n\ntrainer.save_model(\"./final_model\")\n\npredictions = trainer.predict(tokenized_test).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = tokenized_test['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\nprint(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\nprint(f\"F1 (weighted): {eval_results['eval_f1']:.4f}\") \nprint(f\"Precision (weighted): {eval_results['eval_precision']:.4f}\")\nprint(f\"Recall (weighted): {eval_results['eval_recall']:.4f}\")\nprint(f\"Spearman correlation: {eval_results['eval_spearman']:.4f}\")\n\n# Дополнительная визуализация\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\n# Распределение предсказаний\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=num_labels, kde=True)\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:23.546286Z","iopub.execute_input":"2024-07-12T20:22:23.546836Z","iopub.status.idle":"2024-07-12T20:22:24.116033Z","shell.execute_reply.started":"2024-07-12T20:22:23.546786Z","shell.execute_reply":"2024-07-12T20:22:24.113471Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Используется устройство: cpu\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mИспользуется устройство: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_kaspi_reviews.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrussian\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating_class\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Классы от 0 до 4\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleaned_kaspi_reviews.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'cleaned_kaspi_reviews.csv'","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nimport pymorphy2\nimport re\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Устанавливаем сид для воспроизводимости результатов\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1  # Классы от 0 до 4\n\n# Инициализация анализатора pymorphy2\nmorph = pymorphy2.MorphAnalyzer()\n\n# Список стоп-слов (можно расширить при необходимости)\nstop_words = [\n    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так',\n    'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было',\n    'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг',\n    'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж',\n    'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть',\n    'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего',\n    'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого',\n    'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас',\n    'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть',\n    'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве',\n    'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том',\n    'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'\n]\n\ndef preprocess_text(text):\n    if pd.isna(text):\n        return ''\n    if not isinstance(text, str):\n        text = str(text)\n    \n    # Приведение к нижнему регистру и удаление лишних пробелов\n    text = text.lower().strip()\n    \n    # Удаление специальных символов и цифр\n    text = re.sub(r'[^а-яёa-z\\s]', '', text)\n    \n    # Лемматизация\n    words = text.split()\n    lemmatized_words = [morph.parse(word)[0].normal_form for word in words]\n    \n    # Удаление стоп-слов\n    filtered_words = [word for word in lemmatized_words if word not in stop_words]\n    \n    return ' '.join(filtered_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']]\ndf = df.dropna()\n\ntokenizer = AutoTokenizer.from_pretrained('sberbank-ai/ruRoBERTa-large')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=512)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    spearman_corr, _ = spearmanr(labels, preds)\n\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"spearman\": spearman_corr  \n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    # Подбор гиперпараметров\n    lr = trial.suggest_loguniform('lr', 1e-6, 1e-4)\n    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 1000)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [4, 8, 16])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('sberbank-ai/ruRoBERTa-large', num_labels=5)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=5,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=16,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    return eval_results[\"eval_f1\"]\n\n# Кросс-валидация\nn_splits = 5\nkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfinal_predictions = []\nall_actual_classes = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, df['rating_class']), 1):\n    print(f\"Fold {fold}\")\n    \n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n    \n    train_dataset = Dataset.from_pandas(train_df)\n    val_dataset = Dataset.from_pandas(val_df)\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n\n    tokenized_train = tokenized_train.rename_column(\"rating_class\", \"labels\")\n    tokenized_val = tokenized_val.rename_column(\"rating_class\", \"labels\")\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(lambda trial: objective(trial, tokenized_train, tokenized_val), n_trials=20)\n\n    print(f\"Fold {fold} - Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: \", trial.value)\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n\n    # Обучение модели с лучшими параметрами\n    best_model = AutoModelForSequenceClassification.from_pretrained('sberbank-ai/ruRoBERTa-large', num_labels=5)\n    \n    best_training_args = TrainingArguments(\n        output_dir=f\"./results/fold{fold}\",\n        num_train_epochs=10,\n        per_device_train_batch_size=trial.params['per_device_train_batch_size'],\n        per_device_eval_batch_size=16,\n        learning_rate=trial.params['lr'],\n        weight_decay=trial.params['weight_decay'],\n        warmup_steps=trial.params['warmup_steps'],\n        logging_dir=f'./logs/fold{fold}',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n        fp16=True,\n    )\n\n    best_trainer = Trainer(\n        model=best_model,\n        args=best_training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics\n    )\n\n    best_trainer.train()\n\n    eval_results = best_trainer.evaluate()\n    print(f\"Fold {fold} Evaluation Results:\")\n    print(eval_results)\n    \n    best_trainer.save_model(f\"./final_model/fold{fold}\")\n\n    # Предсказания для текущего фолда\n    predictions = best_trainer.predict(tokenized_val).predictions\n    final_predictions.append(predictions)\n    all_actual_classes.extend(val_df['rating_class'])\n\nprint(\"Training completed. Models saved in ./final_model/ directory\")\n\n# Усреднение предсказаний моделей\nfinal_predictions = np.mean(final_predictions, axis=0)\npredicted_classes = np.argmax(final_predictions, axis=1)\nactual_classes = np.array(all_actual_classes)\n\n# Матрица ошибок\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\n# Метрики \nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\") \nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# Дополнительная визуализация\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\n# Распределение предсказаний\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.117126Z","iopub.status.idle":"2024-07-12T20:22:24.117542Z","shell.execute_reply.started":"2024-07-12T20:22:24.117348Z","shell.execute_reply":"2024-07-12T20:22:24.117366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nfrom optuna.samplers import TPESampler\nimport pymorphy2\nimport re\nfrom functools import partial, lru_cache\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, Общая память: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"Общая память GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"Свободная память GPU: {free_memory / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\nstop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\n@lru_cache(maxsize=None)\ndef lemmatize(word):\n    return morph.parse(word)[0].normal_form\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n    return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\ndel df\ngc.collect()\n\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], truncation=True, max_length=256)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [32, 64, 128])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay, \n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"no\", \n        metric_for_best_model=\"f1\",\n        greater_is_better=True, \n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=4,\n        optim=\"adamw_torch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\ntrain_dataset = Dataset.from_pandas(train_df, preserve_index=False)\nval_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\ndel train_df, val_df\ngc.collect()\n\nencoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\nencoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\nencoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\nencoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=3)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_steps=trial.params['warmup_steps'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=4,\n    optim=\"adamw_torch\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()  \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:11:13.118393Z","iopub.execute_input":"2024-07-13T10:11:13.118781Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Используется устройство: cuda\nGPU: Tesla P100-PCIE-16GB, Общая память: 17.06 GB\nОбщая память GPU: 16.79 GB\nСвободная память GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9bee2181e754a8b9ff54e223de9d1c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1ce9883d688427b89d2be254f2a3896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e35a4278628a4db4825bce77bbac11e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b34aad0781a340a9bc857552909ebaa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1147fea67d0149f8865f863f7d98d578"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"506aca67d70142838ee69c36c14924a9"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-13 10:11:34,127] A new study created in memory with name: no-name-0078b2db-d9be-44d0-aea0-e4a7c0686614\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4208756fd7447b58f2c8e2b3b7a54c8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='194' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 194/2100 05:06 < 50:38, 0.63 it/s, Epoch 0.28/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nimport pymorphy2\nimport re\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Устанавливаем сид для воспроизводимости результатов\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\n# Проверка доступной памяти GPU\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, Общая память: {gpu.total_memory / 1e9:.2f} GB\")\n    print(f\"Доступная память: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1  # Классы от 0 до 4\n\n# Инициализация анализатора pymorphy2\nmorph = pymorphy2.MorphAnalyzer()\n\n# Список стоп-слов (оставляем как есть)\nstop_words = [\n    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так',\n    # ... (остальные стоп-слова)\n]\n\ndef preprocess_text(text):\n    if pd.isna(text):\n        return ''\n    if not isinstance(text, str):\n        text = str(text)\n    \n    # Приведение к нижнему регистру и удаление лишних пробелов\n    text = text.lower().strip()\n    \n    # Удаление специальных символов и цифр\n    text = re.sub(r'[^а-яёa-z\\s]', '', text)\n    \n    # Лемматизация\n    words = text.split()\n    lemmatized_words = [morph.parse(word)[0].normal_form for word in words]\n    \n    # Удаление стоп-слов\n    filtered_words = [word for word in lemmatized_words if word not in stop_words]\n    \n    return ' '.join(filtered_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']]\ndf = df.dropna()\n\n# Используем модель DeepPavlov/rubert-base-cased\nmodel_name = 'DeepPavlov/rubert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=256)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    spearman_corr, _ = spearmanr(labels, preds)\n\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"spearman\": spearman_corr  \n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    # Подбор гиперпараметров\n    lr = trial.suggest_float('lr', 1e-6, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 1000)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [4, 8])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [4, 8])\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=8,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    return eval_results[\"eval_f1\"]\n\n# Кросс-валидация\nn_splits = 5\nkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfinal_predictions = []\nall_actual_classes = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, df['rating_class']), 1):\n    print(f\"Fold {fold}\")\n    \n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n    \n    train_dataset = Dataset.from_pandas(train_df)\n    val_dataset = Dataset.from_pandas(val_df)\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n\n    tokenized_train = tokenized_train.rename_column(\"rating_class\", \"labels\")\n    tokenized_val = tokenized_val.rename_column(\"rating_class\", \"labels\")\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(lambda trial: objective(trial, tokenized_train, tokenized_val), n_trials=20)\n\n    print(f\"Fold {fold} - Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: \", trial.value)\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n\n    # Обучение модели с лучшими параметрами\n    best_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n    \n    best_training_args = TrainingArguments(\n        output_dir=f\"./results/fold{fold}\",\n        num_train_epochs=5,\n        per_device_train_batch_size=trial.params['per_device_train_batch_size'],\n        per_device_eval_batch_size=8,\n        learning_rate=trial.params['lr'],\n        weight_decay=trial.params['weight_decay'],\n        warmup_steps=trial.params['warmup_steps'],\n        logging_dir=f'./logs/fold{fold}',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n        fp16=True,\n    )\n\n    best_trainer = Trainer(\n        model=best_model,\n        args=best_training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics\n    )\n\n    best_trainer.train()\n\n    eval_results = best_trainer.evaluate()\n    print(f\"Fold {fold} Evaluation Results:\")\n    print(eval_results)\n    \n    best_trainer.save_model(f\"./final_model/fold{fold}\")\n\n    # Предсказания для текущего фолда\n    predictions = best_trainer.predict(tokenized_val).predictions\n    final_predictions.append(predictions)\n    all_actual_classes.extend(val_df['rating_class'])\n\nprint(\"Training completed. Models saved in ./final_model/ directory\")\n\n# Усреднение предсказаний моделей\nfinal_predictions = np.mean(final_predictions, axis=0)\npredicted_classes = np.argmax(final_predictions, axis=1)\nactual_classes = np.array(all_actual_classes)\n\n# Матрица ошибок\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\n# Метрики \nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\") \nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# Дополнительная визуализация\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\n# Распределение предсказаний\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.119383Z","iopub.status.idle":"2024-07-12T20:22:24.119790Z","shell.execute_reply.started":"2024-07-12T20:22:24.119611Z","shell.execute_reply":"2024-07-12T20:22:24.119627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2024-07-12T21:14:35.417278Z","iopub.execute_input":"2024-07-12T21:14:35.417618Z","iopub.status.idle":"2024-07-12T21:14:35.428445Z","shell.execute_reply.started":"2024-07-12T21:14:35.417591Z","shell.execute_reply":"2024-07-12T21:14:35.427534Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T21:14:37.714636Z","iopub.execute_input":"2024-07-12T21:14:37.715575Z","iopub.status.idle":"2024-07-12T21:14:37.756988Z","shell.execute_reply.started":"2024-07-12T21:14:37.715541Z","shell.execute_reply":"2024-07-12T21:14:37.755794Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nimport pymorphy2\nimport re\nfrom functools import partial\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, Общая память: {gpu.total_memory / 1e9:.2f} GB\")\n    print(f\"Доступная память: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\n\nstop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n    words = text.split()\n    lemmatized_words = [morph.parse(word)[0].normal_form for word in words if word not in stop_words]\n    return ' '.join(lemmatized_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\nmodel_name = 'DeepPavlov/rubert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'),\n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [16, 32])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    return eval_results[\"eval_f1\"]\n\nn_splits = 3  # Уменьшаем количество фолдов\nkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfinal_predictions = []\nall_actual_classes = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, df['rating_class']), 1):\n    print(f\"Fold {fold}\")\n    \n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n    \n    train_dataset = Dataset.from_pandas(train_df)\n    val_dataset = Dataset.from_pandas(val_df)\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n\n    tokenized_train = tokenized_train.rename_column(\"rating_class\", \"labels\")\n    tokenized_val = tokenized_val.rename_column(\"rating_class\", \"labels\")\n\n    study = optuna.create_study(direction=\"maximize\")\n    objective_with_dataset = partial(objective, train_dataset=tokenized_train, val_dataset=tokenized_val)\n    study.optimize(objective_with_dataset, n_trials=10)  # Уменьшаем количество trials\n\n    print(f\"Fold {fold} - Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: \", trial.value)\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n\n    best_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n    \n    best_training_args = TrainingArguments(\n        output_dir=f\"./results/fold{fold}\",\n        num_train_epochs=5,\n        per_device_train_batch_size=trial.params['per_device_train_batch_size'],\n        per_device_eval_batch_size=64,\n        learning_rate=trial.params['lr'],\n        weight_decay=trial.params['weight_decay'],\n        warmup_steps=trial.params['warmup_steps'],\n        logging_dir=f'./logs/fold{fold}',\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n        fp16=True,\n    )\n\n    best_trainer = Trainer(\n        model=best_model,\n        args=best_training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics\n    )\n\n    best_trainer.train()\n\n    eval_results = best_trainer.evaluate()\n    print(f\"Fold {fold} Evaluation Results:\")\n    print(eval_results)\n    \n    best_trainer.save_model(f\"./final_model/fold{fold}\")\n\n    predictions = best_trainer.predict(tokenized_val).predictions\n    final_predictions.append(predictions)\n    all_actual_classes.extend(val_df['rating_class'])\n\nprint(\"Training completed. Models saved in ./final_model/ directory\")\n\nfinal_predictions = np.mean(final_predictions, axis=0)\npredicted_classes = np.argmax(final_predictions, axis=1)\nactual_classes = np.array(all_actual_classes)\n\n# Матрица ошибок\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\n# Метрики \nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\") \nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# Дополнительная визуализация\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\n# Распределение предсказаний\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.125463Z","iopub.status.idle":"2024-07-12T20:22:24.125994Z","shell.execute_reply.started":"2024-07-12T20:22:24.125731Z","shell.execute_reply":"2024-07-12T20:22:24.125755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AdamW\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:24:33.946411Z","iopub.execute_input":"2024-07-12T20:24:33.947106Z","iopub.status.idle":"2024-07-12T20:24:52.342538Z","shell.execute_reply.started":"2024-07-12T20:24:33.947072Z","shell.execute_reply":"2024-07-12T20:24:52.341227Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-12 20:24:41.748966: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-12 20:24:41.749073: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-12 20:24:41.903000: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nimport pymorphy2\nimport re\nfrom functools import partial\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, Общая память: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"Общая память GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"Свободная память GPU: {free_memory / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\nstop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n    lemmatized_words = (morph.parse(word)[0].normal_form for word in text.split() if word not in stop_words)\n    return ' '.join(lemmatized_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\ndel df\ngc.collect()\n\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], truncation=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [16, 32, 64])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [4, 8, 16])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay, \n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\", \n        metric_for_best_model=\"f1\",\n        greater_is_better=True, \n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=4,\n        optim=\"adafactor\"  \n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\ntrain_dataset = Dataset.from_pandas(train_df, preserve_index=False)\nval_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\ndel train_df, val_df\ngc.collect()\n\nencoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\nencoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\nencoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\nencoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=3)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_steps=trial.params['warmup_steps'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=50,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=4,\n    optim=\"adafactor\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\n# Матрица ошибок\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\n# Метрики\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# Дополнительная визуализация \nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\n# Распределение предсказаний\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()  \ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pymorphy2\nimport re\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:25:30.731513Z","iopub.execute_input":"2024-07-12T20:25:30.732331Z","iopub.status.idle":"2024-07-12T20:25:30.755729Z","shell.execute_reply.started":"2024-07-12T20:25:30.732281Z","shell.execute_reply":"2024-07-12T20:25:30.754980Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:04:20.466877Z","iopub.execute_input":"2024-07-13T10:04:20.467311Z","iopub.status.idle":"2024-07-13T10:04:20.473107Z","shell.execute_reply.started":"2024-07-13T10:04:20.467266Z","shell.execute_reply":"2024-07-13T10:04:20.471728Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:04:24.156095Z","iopub.execute_input":"2024-07-13T10:04:24.156523Z","iopub.status.idle":"2024-07-13T10:04:24.229662Z","shell.execute_reply.started":"2024-07-13T10:04:24.156489Z","shell.execute_reply":"2024-07-13T10:04:24.228471Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nfrom optuna.samplers import TPESampler\nimport pymorphy2\nimport re\nfrom functools import partial, lru_cache\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, Общая память: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"Общая память GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"Свободная память GPU: {free_memory / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\nstop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\n@lru_cache(maxsize=None)\ndef lemmatize(word):\n    return morph.parse(word)[0].normal_form\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n    return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\ndel df\ngc.collect()\n\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], truncation=True, max_length=256)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [32, 64, 128])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay, \n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"no\", \n        metric_for_best_model=\"f1\",\n        greater_is_better=True, \n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=4,\n        optim=\"adamw_torch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\ntrain_dataset = Dataset.from_pandas(train_df, preserve_index=False)\nval_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\ndel train_df, val_df\ngc.collect()\n\nencoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\nencoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\nencoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\nencoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=3)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_steps=trial.params['warmup_steps'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=4,\n    optim=\"adamw_torch\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()  \ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, Общая память: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"Общая память GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"Свободная память GPU: {free_memory / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\nstop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n    lemmatized_words = [morph.parse(word)[0].normal_form for word in text.split() if word not in stop_words]\n    return ' '.join(lemmatized_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\n# Используем стандартный токенизатор из transformers\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [32, 64])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=128,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\", \n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args, \n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    return eval_results[\"eval_f1\"]\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\nencoded_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\nencoded_val = val_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n\nencoded_train.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'rating_class'])\nencoded_val.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'rating_class'])\n\nencoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\nencoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\nstudy = optuna.create_study(direction=\"maximize\")\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=5)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=128,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_steps=trial.params['warmup_steps'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=50,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True, \n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'], \n    fp16=True,\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = val_df['rating_class']\n\n# Визуализация и метрики (оставляем без изменений)\n# ...\n\n# Матрица ошибок\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\n# Метрики\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# Дополнительная визуализация \nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\n# Распределение предсказаний\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:25:36.262452Z","iopub.execute_input":"2024-07-12T20:25:36.262806Z","iopub.status.idle":"2024-07-12T20:50:27.126179Z","shell.execute_reply.started":"2024-07-12T20:25:36.262779Z","shell.execute_reply":"2024-07-12T20:50:27.124584Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Используется устройство: cuda\nGPU: Tesla P100-PCIE-16GB, Общая память: 17.06 GB\nОбщая память GPU: 16.79 GB\nСвободная память GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"586b671b91814dedb64916d637579b0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"371cdd66845949d6816f27732c7d1b50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015fea03d0d44d6fac5b8dae9531a044"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"290edd9007d547f5a5cf9d8dca3131b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fd96e32ec7342c394d34b0546616c6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746061bd88824580b3810f88f158d914"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-12 20:31:41,868] A new study created in memory with name: no-name-c6354f65-a6d2-47dc-9015-f8d5a751b11f\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b448de7844454396b31ae505d97b22a6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='380' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 380/1050 18:26 < 32:41, 0.34 it/s, Epoch 1.08/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.440700</td>\n      <td>0.427637</td>\n      <td>0.855288</td>\n      <td>0.802617</td>\n      <td>0.802198</td>\n      <td>0.855288</td>\n      <td>0.442637</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n[W 2024-07-12 20:50:25,650] Trial 0 failed with parameters: {'lr': 1.629174368249374e-05, 'weight_decay': 0.00012897438760911791, 'warmup_steps': 173, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 4} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n    value_or_values = func(trial)\n  File \"/tmp/ipykernel_35/4289091445.py\", line 92, in objective\n    trainer.train()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1932, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2268, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3324, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2147, in backward\n    self.scaler.scale(loss).backward(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\n    torch.autograd.backward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nKeyboardInterrupt\n[W 2024-07-12 20:50:25,662] Trial 0 failed with value None.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 111\u001b[0m\n\u001b[1;32m    109\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m objective_with_dataset \u001b[38;5;241m=\u001b[39m partial(objective, train_dataset\u001b[38;5;241m=\u001b[39mencoded_train, val_dataset\u001b[38;5;241m=\u001b[39mencoded_val)\n\u001b[0;32m--> 111\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_with_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n","Cell \u001b[0;32mIn[6], line 92\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m     65\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     66\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/trial_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;241m.\u001b[39mnumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     84\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     85\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     86\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     90\u001b[0m )\n\u001b[0;32m---> 92\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eval_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3324\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3322\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3324\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2147\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2148\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T21:15:12.192455Z","iopub.execute_input":"2024-07-12T21:15:12.192821Z","iopub.status.idle":"2024-07-12T21:15:12.519708Z","shell.execute_reply.started":"2024-07-12T21:15:12.192793Z","shell.execute_reply":"2024-07-12T21:15:12.518423Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-07-13T02:32:18.109623Z","iopub.execute_input":"2024-07-13T02:32:18.110047Z","iopub.status.idle":"2024-07-13T02:32:18.518474Z","shell.execute_reply.started":"2024-07-13T02:32:18.110016Z","shell.execute_reply":"2024-07-13T02:32:18.517279Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(f\"Accuracy: {acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:04:11.698482Z","iopub.execute_input":"2024-07-13T08:04:11.699567Z","iopub.status.idle":"2024-07-13T08:04:12.139502Z","shell.execute_reply.started":"2024-07-13T08:04:11.699504Z","shell.execute_reply":"2024-07-13T08:04:12.137858Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43macc\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'acc' is not defined"],"ename":"NameError","evalue":"name 'acc' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nimport pymorphy2\nimport re\nfrom functools import partial\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, Общая память: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"Общая память GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"Свободная память GPU: {free_memory / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\nstop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n    lemmatized_words = (morph.parse(word)[0].normal_form for word in text.split() if word not in stop_words)\n    return ' '.join(lemmatized_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\ndel df\ngc.collect()\n\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], truncation=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [16, 32, 64])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [4, 8, 16])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay, \n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\", \n        metric_for_best_model=\"f1\",\n        greater_is_better=True, \n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=4,\n        optim=\"adafactor\"  \n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\ntrain_dataset = Dataset.from_pandas(train_df, preserve_index=False)\nval_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\ndel train_df, val_df\ngc.collect()\n\nencoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\nencoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\nencoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\nencoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=3)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_steps=trial.params['warmup_steps'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=50,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=4,\n    optim=\"adafactor\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\n# Матрица ошибок\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\n# Метрики\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# Дополнительная визуализация \nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\n# Распределение предсказаний\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()  \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T21:15:22.809588Z","iopub.execute_input":"2024-07-12T21:15:22.809922Z","iopub.status.idle":"2024-07-13T01:12:41.567640Z","shell.execute_reply.started":"2024-07-12T21:15:22.809897Z","shell.execute_reply":"2024-07-13T01:12:41.566578Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-12 21:15:30.369234: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-12 21:15:30.369344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-12 21:15:30.495830: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Используется устройство: cuda\nGPU: Tesla P100-PCIE-16GB, Общая память: 17.06 GB\nОбщая память GPU: 16.79 GB\nСвободная память GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a354c41c620496794bd121bb2c0b644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05ba4972301141cfa20b23c9ff41ffb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce86c01c0324a41bafadeac3b5e74b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c38e623b0d444cb1bd597a5eaa98f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba76674914b842618b214d79e4feec25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb3fcaafaabc4fef8d944734de25f36d"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-12 21:21:36,219] A new study created in memory with name: no-name-90add2d3-c025-42c5-9b35-c313bc53a0fe\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0a4e460e58b4192a50a48059f8a2ca1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1050/1050 42:11, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.468300</td>\n      <td>0.436497</td>\n      <td>0.853458</td>\n      <td>0.799995</td>\n      <td>0.786820</td>\n      <td>0.853458</td>\n      <td>0.400768</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.415900</td>\n      <td>0.406833</td>\n      <td>0.862115</td>\n      <td>0.836004</td>\n      <td>0.821805</td>\n      <td>0.862115</td>\n      <td>0.548216</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.386700</td>\n      <td>0.402884</td>\n      <td>0.863498</td>\n      <td>0.835314</td>\n      <td>0.823301</td>\n      <td>0.863498</td>\n      <td>0.548569</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nTOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-12 22:05:42,314] Trial 0 finished with value: 0.8353135114785433 and parameters: {'lr': 1.5908689043323806e-05, 'weight_decay': 0.0005547721817754297, 'warmup_steps': 387, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 16}. Best is trial 0 with value: 0.8353135114785433.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4200' max='4200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4200/4200 45:36, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.439400</td>\n      <td>0.403840</td>\n      <td>0.862561</td>\n      <td>0.830476</td>\n      <td>0.818269</td>\n      <td>0.862561</td>\n      <td>0.515236</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.382900</td>\n      <td>0.395511</td>\n      <td>0.864480</td>\n      <td>0.845506</td>\n      <td>0.840182</td>\n      <td>0.864480</td>\n      <td>0.578726</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.310500</td>\n      <td>0.408704</td>\n      <td>0.862829</td>\n      <td>0.846218</td>\n      <td>0.836953</td>\n      <td>0.862829</td>\n      <td>0.586902</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-12 22:52:51,305] Trial 1 finished with value: 0.8462179470344828 and parameters: {'lr': 2.6687655611407676e-05, 'weight_decay': 0.00010165203968063754, 'warmup_steps': 168, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 4}. Best is trial 1 with value: 0.8462179470344828.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [525/525 59:11, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.481900</td>\n      <td>0.460531</td>\n      <td>0.849041</td>\n      <td>0.784844</td>\n      <td>0.785110</td>\n      <td>0.849041</td>\n      <td>0.224224</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.425100</td>\n      <td>0.409776</td>\n      <td>0.860107</td>\n      <td>0.837775</td>\n      <td>0.825257</td>\n      <td>0.860107</td>\n      <td>0.563943</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.377700</td>\n      <td>0.397582</td>\n      <td>0.864257</td>\n      <td>0.840269</td>\n      <td>0.832418</td>\n      <td>0.864257</td>\n      <td>0.568114</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-12 23:53:40,635] Trial 2 finished with value: 0.8402689777374475 and parameters: {'lr': 5.310870371040237e-05, 'weight_decay': 0.001937599363616159, 'warmup_steps': 261, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 8}. Best is trial 1 with value: 0.8462179470344828.\n","output_type":"stream"},{"name":"stdout","text":"Best trial:\n  Value:  0.8462179470344828\n  Params: \n    lr: 2.6687655611407676e-05\n    weight_decay: 0.00010165203968063754\n    warmup_steps: 168\n    per_device_train_batch_size: 16\n    gradient_accumulation_steps: 4\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7000' max='7000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7000/7000 1:15:55, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.443300</td>\n      <td>0.404488</td>\n      <td>0.863320</td>\n      <td>0.829794</td>\n      <td>0.818031</td>\n      <td>0.863320</td>\n      <td>0.512466</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.388700</td>\n      <td>0.397359</td>\n      <td>0.862874</td>\n      <td>0.844235</td>\n      <td>0.832823</td>\n      <td>0.862874</td>\n      <td>0.576126</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.321400</td>\n      <td>0.408967</td>\n      <td>0.859349</td>\n      <td>0.845811</td>\n      <td>0.841690</td>\n      <td>0.859349</td>\n      <td>0.587751</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.252400</td>\n      <td>0.473034</td>\n      <td>0.855689</td>\n      <td>0.846079</td>\n      <td>0.838661</td>\n      <td>0.855689</td>\n      <td>0.584375</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Final Evaluation Results:\n{'eval_loss': 0.4423908293247223, 'eval_accuracy': 0.8592592592592593, 'eval_f1': 0.8473316495318277, 'eval_precision': 0.8390448072429096, 'eval_recall': 0.8592592592592593, 'eval_spearman': 0.588081419589789, 'eval_runtime': 89.3509, 'eval_samples_per_second': 250.809, 'eval_steps_per_second': 3.928, 'epoch': 4.997322862752097}\nTraining completed. Model saved in ./final_model/ directory\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"Accuracy: 0.8593\nF1 (weighted): 0.8473\nPrecision (weighted): 0.8390\nRecall (weighted): 0.8593\nSpearman correlation: 0.5881\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1765: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  order = pd.unique(vector)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"12059"},"metadata":{}}]},{"cell_type":"code","source":"# Функция для предсказания оценок для новых отзывов\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    # Создаем DataFrame из новых отзывов\n    new_data = pd.DataFrame({'combined_text': reviews})\n    new_data['processed_text'] = new_data['combined_text'].apply(preprocess_text)\n    new_dataset = Dataset.from_pandas(new_data)\n    \n    # Токенизируем новые отзывы\n    tokenized_new_data = new_dataset.map(tokenize_function, batched=True, remove_columns=['combined_text'])\n    \n    # Получаем предсказания модели для новых отзывов\n    predictions = trainer.predict(tokenized_new_data).predictions\n    \n    if len(predictions.shape) == 1:\n        predicted_ratings = np.argmax(predictions.reshape(1, -1), axis=1) + 1\n    else:\n        predicted_ratings = np.argmax(predictions, axis=1) + 1\n    \n    return predicted_ratings.tolist()\n\n# Функция для ввода отзывов с клавиатуры\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"Введите отзыв (или нажмите Enter для завершения): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# Вызываем функцию для ввода отзывов с клавиатуры\nnew_reviews = input_reviews()\n\n# Получаем предсказанные оценки для введенных отзывов\npredicted_ratings = predict_ratings(new_reviews)\n\n# Выводим результаты предсказаний\nif len(predicted_ratings) == 0:\n    print(\"Нет введенных отзывов.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"Отзыв: {review}\")\n        print(f\"Предсказанная оценка: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T02:32:50.288418Z","iopub.execute_input":"2024-07-13T02:32:50.289049Z","iopub.status.idle":"2024-07-13T02:32:53.402612Z","shell.execute_reply.started":"2024-07-13T02:32:50.289015Z","shell.execute_reply":"2024-07-13T02:32:53.401221Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdin","text":"Введите отзыв (или нажмите Enter для завершения):  ыв\nВведите отзыв (или нажмите Enter для завершения):  \n"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m new_reviews \u001b[38;5;241m=\u001b[39m input_reviews()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Получаем предсказанные оценки для введенных отзывов\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m predicted_ratings \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_ratings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_reviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Выводим результаты предсказаний\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predicted_ratings) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mpredict_ratings\u001b[0;34m(reviews)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Создаем DataFrame из новых отзывов\u001b[39;00m\n\u001b[1;32m      7\u001b[0m new_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m: reviews})\n\u001b[0;32m----> 8\u001b[0m new_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m new_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mpreprocess_text\u001b[49m)\n\u001b[1;32m      9\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(new_data)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Токенизируем новые отзывы\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'preprocess_text' is not defined"],"ename":"NameError","evalue":"name 'preprocess_text' is not defined","output_type":"error"}]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:39.668093Z","iopub.execute_input":"2024-07-12T20:22:39.668866Z","iopub.status.idle":"2024-07-12T20:22:39.710041Z","shell.execute_reply.started":"2024-07-12T20:22:39.668828Z","shell.execute_reply":"2024-07-12T20:22:39.708597Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n","\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"],"ename":"NameError","evalue":"name 'gc' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset, load_from_disk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nfrom optuna.samplers import TPESampler\nimport pymorphy2\nimport re\nfrom functools import partial, lru_cache\nimport gc\nfrom transformers import get_linear_schedule_with_warmup\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, Общая память: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"Общая память GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"Свободная память GPU: {free_memory / 1e9:.2f} GB\")\n\n# Проверяем, есть ли уже обработанные данные\nif os.path.exists('./processed_data'):\n    encoded_train = load_from_disk('./processed_data/train')\n    encoded_val = load_from_disk('./processed_data/val')\n    print(\"Загружены предобработанные данные\")\nelse:\n    df = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\n    df = df[df['language'] == 'russian']\n    df['rating_class'] = df['rating'].astype(int) - 1\n\n    morph = pymorphy2.MorphAnalyzer()\n    stop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\n    @lru_cache(maxsize=None)\n    def lemmatize(word):\n        return morph.parse(word)[0].normal_form\n\n    def preprocess_text(text):\n        if pd.isna(text) or not isinstance(text, str):\n            return ''\n        text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n        return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\n    df['processed_text'] = df['combined_text'].apply(preprocess_text)\n    df = df[['processed_text', 'rating_class']].dropna()\n\n    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\n    del df\n    gc.collect()\n\n    tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=512, use_fast=True)\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"processed_text\"], truncation=True, max_length=512)\n\n    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\n    del train_df, val_df\n    gc.collect()\n\n    encoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n    encoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\n    encoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\n    encoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\n    # Сохраняем обработанные данные\n    encoded_train.save_to_disk('./processed_data/train')\n    encoded_val.save_to_disk('./processed_data/val')\n    print(\"Данные обработаны и сохранены\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_ratio = trial.suggest_float('warmup_ratio', 0.1, 0.3)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [16, 32])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [2, 4, 8])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=5,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay, \n        warmup_ratio=warmup_ratio,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"no\", \n        metric_for_best_model=\"f1\",\n        greater_is_better=True, \n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=4,\n        optim=\"adamw_torch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=10)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=10,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_ratio=trial.params['warmup_ratio'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=4,\n    optim=\"adamw_torch\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()  \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:31:34.035690Z","iopub.execute_input":"2024-07-13T10:31:34.036072Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Используется устройство: cuda\nGPU: Tesla P100-PCIE-16GB, Общая память: 17.06 GB\nОбщая память GPU: 16.79 GB\nСвободная память GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a0ed2b742ab4d61846001bdddc7fc69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0e12b6a6af04fd1a0466c09b46034ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f8ab19220b3490c804d18f6dcda877f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c1d8dc2c4346068609c92921b5604e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf905e8b46324eff8582df6d4991fbeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f7f74698024d25a3f5cb45f15ad0c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e74befb903c444c87013e927d540de8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ebe98b71ca3408689949eddaed196e0"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-13 10:31:53,845] A new study created in memory with name: no-name-264cfe5b-dba0-465f-9b9d-8b6315bc84b4\n","output_type":"stream"},{"name":"stdout","text":"Данные обработаны и сохранены\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"871b969f323b4ed5b8f60bda348b963e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13340' max='14005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13340/14005 1:47:48 < 05:22, 2.06 it/s, Epoch 4.76/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.554500</td>\n      <td>0.508857</td>\n      <td>0.844578</td>\n      <td>0.778092</td>\n      <td>0.735286</td>\n      <td>0.844578</td>\n      <td>0.167706</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.478000</td>\n      <td>0.455125</td>\n      <td>0.850201</td>\n      <td>0.807494</td>\n      <td>0.789651</td>\n      <td>0.850201</td>\n      <td>0.446652</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.421500</td>\n      <td>0.444806</td>\n      <td>0.851539</td>\n      <td>0.803628</td>\n      <td>0.802166</td>\n      <td>0.851539</td>\n      <td>0.465818</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.450300</td>\n      <td>0.431311</td>\n      <td>0.855556</td>\n      <td>0.810837</td>\n      <td>0.799206</td>\n      <td>0.855556</td>\n      <td>0.466756</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.429600</td>\n      <td>0.417817</td>\n      <td>0.859349</td>\n      <td>0.825739</td>\n      <td>0.809636</td>\n      <td>0.859349</td>\n      <td>0.506089</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.390900</td>\n      <td>0.428604</td>\n      <td>0.857385</td>\n      <td>0.811276</td>\n      <td>0.802438</td>\n      <td>0.857385</td>\n      <td>0.474174</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.430600</td>\n      <td>0.414507</td>\n      <td>0.859393</td>\n      <td>0.827536</td>\n      <td>0.817012</td>\n      <td>0.859393</td>\n      <td>0.533089</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.419000</td>\n      <td>0.420860</td>\n      <td>0.859393</td>\n      <td>0.822049</td>\n      <td>0.811150</td>\n      <td>0.859393</td>\n      <td>0.517477</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.427500</td>\n      <td>0.411024</td>\n      <td>0.859750</td>\n      <td>0.830580</td>\n      <td>0.813846</td>\n      <td>0.859750</td>\n      <td>0.526442</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.449300</td>\n      <td>0.407155</td>\n      <td>0.860464</td>\n      <td>0.829218</td>\n      <td>0.820845</td>\n      <td>0.860464</td>\n      <td>0.553536</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.401300</td>\n      <td>0.410706</td>\n      <td>0.862918</td>\n      <td>0.833649</td>\n      <td>0.824584</td>\n      <td>0.862918</td>\n      <td>0.567289</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.359900</td>\n      <td>0.418243</td>\n      <td>0.848237</td>\n      <td>0.841408</td>\n      <td>0.835653</td>\n      <td>0.848237</td>\n      <td>0.583877</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.380700</td>\n      <td>0.416629</td>\n      <td>0.860196</td>\n      <td>0.835699</td>\n      <td>0.831429</td>\n      <td>0.860196</td>\n      <td>0.573228</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.361000</td>\n      <td>0.404269</td>\n      <td>0.864123</td>\n      <td>0.839007</td>\n      <td>0.828727</td>\n      <td>0.864123</td>\n      <td>0.559867</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.360000</td>\n      <td>0.405639</td>\n      <td>0.862918</td>\n      <td>0.842130</td>\n      <td>0.828364</td>\n      <td>0.862918</td>\n      <td>0.575285</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.350000</td>\n      <td>0.427751</td>\n      <td>0.859393</td>\n      <td>0.840956</td>\n      <td>0.832799</td>\n      <td>0.859393</td>\n      <td>0.578867</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.318000</td>\n      <td>0.435511</td>\n      <td>0.862294</td>\n      <td>0.840187</td>\n      <td>0.829365</td>\n      <td>0.862294</td>\n      <td>0.576460</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.322500</td>\n      <td>0.456395</td>\n      <td>0.864926</td>\n      <td>0.838778</td>\n      <td>0.829106</td>\n      <td>0.864926</td>\n      <td>0.557509</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.327900</td>\n      <td>0.438523</td>\n      <td>0.859036</td>\n      <td>0.841367</td>\n      <td>0.832458</td>\n      <td>0.859036</td>\n      <td>0.570614</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.309900</td>\n      <td>0.439526</td>\n      <td>0.862517</td>\n      <td>0.843545</td>\n      <td>0.834419</td>\n      <td>0.862517</td>\n      <td>0.563313</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.297500</td>\n      <td>0.440235</td>\n      <td>0.861000</td>\n      <td>0.845167</td>\n      <td>0.835174</td>\n      <td>0.861000</td>\n      <td>0.575021</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.304900</td>\n      <td>0.446166</td>\n      <td>0.846095</td>\n      <td>0.844310</td>\n      <td>0.843616</td>\n      <td>0.846095</td>\n      <td>0.582625</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.251000</td>\n      <td>0.485858</td>\n      <td>0.855689</td>\n      <td>0.844379</td>\n      <td>0.836422</td>\n      <td>0.855689</td>\n      <td>0.576319</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.264500</td>\n      <td>0.485677</td>\n      <td>0.857207</td>\n      <td>0.844507</td>\n      <td>0.835607</td>\n      <td>0.857207</td>\n      <td>0.573920</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.236900</td>\n      <td>0.497675</td>\n      <td>0.856448</td>\n      <td>0.843260</td>\n      <td>0.834338</td>\n      <td>0.856448</td>\n      <td>0.577886</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.250100</td>\n      <td>0.497966</td>\n      <td>0.853414</td>\n      <td>0.844607</td>\n      <td>0.838187</td>\n      <td>0.853414</td>\n      <td>0.579931</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pymorphy2","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:37:32.992233Z","iopub.execute_input":"2024-07-14T11:37:32.992607Z","iopub.status.idle":"2024-07-14T11:37:47.236966Z","shell.execute_reply.started":"2024-07-14T11:37:32.992573Z","shell.execute_reply":"2024-07-14T11:37:47.235742Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting pymorphy2\n  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\nCollecting dawg-python>=0.7.1 (from pymorphy2)\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\nCollecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.10/site-packages (from pymorphy2) (0.6.2)\nDownloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nDownloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\nSuccessfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n","output_type":"stream"}]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T13:42:52.069857Z","iopub.execute_input":"2024-07-13T13:42:52.070189Z","iopub.status.idle":"2024-07-13T13:42:52.118205Z","shell.execute_reply.started":"2024-07-13T13:42:52.070164Z","shell.execute_reply":"2024-07-13T13:42:52.117319Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\nimport torch\nfrom torch import nn\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Устанавливаем сид для воспроизводимости результатов\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1  # Классы от 0 до 4\n\ndef preprocess_text(text):\n    return text.lower().strip() if isinstance(text, str) else ''\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df.dropna(subset=['processed_text', 'rating_class'])\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\nprint(f\"Размер обучающей выборки: {len(train_df)}\")\nprint(f\"Размер тестовой выборки: {len(test_df)}\")\n\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=256)\n\ntokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\ntokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\n\ntokenized_train = tokenized_train.rename_column(\"rating_class\", \"labels\")\ntokenized_test = tokenized_test.rename_column(\"rating_class\", \"labels\")\n\nnum_labels = df['rating_class'].nunique()\n\nclass ClassificationModel(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        \n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n        \n        return (loss, logits) if loss is not None else logits\n\nmodel = ClassificationModel('cointegrated/rubert-tiny', num_labels)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    spearman_corr, _ = spearmanr(labels, preds)\n\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"spearman\": spearman_corr\n    }\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    warmup_steps=500,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    gradient_accumulation_steps=2,\n    fp16=True if torch.cuda.is_available() else False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\neval_results = trainer.evaluate()\nprint(\"Результаты оценки:\", eval_results)\n\ntrainer.save_model(\"./final_model\")\n\npredictions = trainer.predict(tokenized_test).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = tokenized_test['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\nprint(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\nprint(f\"F1 (weighted): {eval_results['eval_f1']:.4f}\") \nprint(f\"Precision (weighted): {eval_results['eval_precision']:.4f}\")\nprint(f\"Recall (weighted): {eval_results['eval_recall']:.4f}\")\nprint(f\"Spearman correlation: {eval_results['eval_spearman']:.4f}\")\n\n# Дополнительная визуализация\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\n# Распределение предсказаний\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=num_labels, kde=True)\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.137027Z","iopub.status.idle":"2024-07-12T20:22:24.137427Z","shell.execute_reply.started":"2024-07-12T20:22:24.137244Z","shell.execute_reply":"2024-07-12T20:22:24.137260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset, load_from_disk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nfrom optuna.samplers import TPESampler\nimport pymorphy2\nimport re\nfrom functools import partial, lru_cache\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, Общая память: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"Общая память GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"Свободная память GPU: {free_memory / 1e9:.2f} GB\")\n\n# Проверяем, есть ли уже обработанные данные\nif os.path.exists('./processed_data'):\n    encoded_train = load_from_disk('./processed_data/train')\n    encoded_val = load_from_disk('./processed_data/val')\n    print(\"Загружены предобработанные данные\")\nelse:\n    df = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\n    df = df[df['language'] == 'russian']\n    df['rating_class'] = df['rating'].astype(int) - 1\n\n    morph = pymorphy2.MorphAnalyzer()\n    stop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\n    @lru_cache(maxsize=None)\n    def lemmatize(word):\n        return morph.parse(word)[0].normal_form\n\n    def preprocess_text(text):\n        if pd.isna(text) or not isinstance(text, str):\n            return ''\n        text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n        return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\n    df['processed_text'] = df['combined_text'].apply(preprocess_text)\n    df = df[['processed_text', 'rating_class']].dropna()\n\n    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\n    del df\n    gc.collect()\n\n    tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"processed_text\"], truncation=True, max_length=256)\n\n    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\n    del train_df, val_df\n    gc.collect()\n\n    encoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n    encoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\n    encoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\n    encoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\n    # Сохраняем закодированные наборы данных на диск\n    encoded_train.save_to_disk('./processed_data/train')\n    encoded_val.save_to_disk('./processed_data/val')\n    print(\"Данные обработаны и сохранены\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'),\n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-3, 1e-1, log=True)\n    warmup_ratio = trial.suggest_float('warmup_ratio', 0.1, 0.3)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [32, 64])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_ratio=warmup_ratio,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"no\",\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=6,\n        optim=\"adamw_torch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=10)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",\n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'],\n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_ratio=trial.params['warmup_ratio'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True,\n    dataloader_num_workers=6,\n    optim=\"adamw_torch\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T13:43:47.580665Z","iopub.execute_input":"2024-07-13T13:43:47.581025Z","iopub.status.idle":"2024-07-14T01:05:38.655362Z","shell.execute_reply.started":"2024-07-13T13:43:47.580997Z","shell.execute_reply":"2024-07-14T01:05:38.652929Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Используется устройство: cuda\nGPU: Tesla P100-PCIE-16GB, Общая память: 17.06 GB\nОбщая память GPU: 16.79 GB\nСвободная память GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edfb12c48d454be2a1b48d6ad9ec46e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca89a7bc835242c3982e8e8cfd6a46c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a85acfff1ebf4ce8b58015e6d41dfebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28a172d2e1be4886a0574882ed026810"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-13 13:44:05,516] A new study created in memory with name: no-name-9c7103e6-b953-4aa6-834a-14e3c5b8238f\n","output_type":"stream"},{"name":"stdout","text":"Данные обработаны и сохранены\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"213f4d724ad64da292748c370d04c45f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8406' max='8406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8406/8406 1:11:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.549100</td>\n      <td>0.513470</td>\n      <td>0.844578</td>\n      <td>0.776738</td>\n      <td>0.732959</td>\n      <td>0.844578</td>\n      <td>0.143102</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.479200</td>\n      <td>0.454114</td>\n      <td>0.851049</td>\n      <td>0.799976</td>\n      <td>0.770562</td>\n      <td>0.851049</td>\n      <td>0.374745</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.421900</td>\n      <td>0.446886</td>\n      <td>0.851406</td>\n      <td>0.806747</td>\n      <td>0.813945</td>\n      <td>0.851406</td>\n      <td>0.476772</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.445400</td>\n      <td>0.427374</td>\n      <td>0.855913</td>\n      <td>0.808510</td>\n      <td>0.797749</td>\n      <td>0.855913</td>\n      <td>0.451610</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.425100</td>\n      <td>0.416201</td>\n      <td>0.860241</td>\n      <td>0.832242</td>\n      <td>0.818206</td>\n      <td>0.860241</td>\n      <td>0.534618</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.388000</td>\n      <td>0.418405</td>\n      <td>0.862115</td>\n      <td>0.831430</td>\n      <td>0.822711</td>\n      <td>0.862115</td>\n      <td>0.518005</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.423100</td>\n      <td>0.406698</td>\n      <td>0.861446</td>\n      <td>0.834909</td>\n      <td>0.824812</td>\n      <td>0.861446</td>\n      <td>0.560370</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.401800</td>\n      <td>0.407659</td>\n      <td>0.861580</td>\n      <td>0.833453</td>\n      <td>0.820418</td>\n      <td>0.861580</td>\n      <td>0.556473</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.401400</td>\n      <td>0.406532</td>\n      <td>0.857564</td>\n      <td>0.843366</td>\n      <td>0.831485</td>\n      <td>0.857564</td>\n      <td>0.586494</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.432600</td>\n      <td>0.399940</td>\n      <td>0.865105</td>\n      <td>0.838377</td>\n      <td>0.827151</td>\n      <td>0.865105</td>\n      <td>0.555853</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.381500</td>\n      <td>0.405682</td>\n      <td>0.863588</td>\n      <td>0.836620</td>\n      <td>0.827270</td>\n      <td>0.863588</td>\n      <td>0.567463</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.349800</td>\n      <td>0.411021</td>\n      <td>0.857296</td>\n      <td>0.844370</td>\n      <td>0.835723</td>\n      <td>0.857296</td>\n      <td>0.590691</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.363600</td>\n      <td>0.418416</td>\n      <td>0.863454</td>\n      <td>0.839533</td>\n      <td>0.830355</td>\n      <td>0.863454</td>\n      <td>0.561781</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.339900</td>\n      <td>0.408903</td>\n      <td>0.860152</td>\n      <td>0.843326</td>\n      <td>0.833140</td>\n      <td>0.860152</td>\n      <td>0.585685</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.346700</td>\n      <td>0.409230</td>\n      <td>0.858813</td>\n      <td>0.845371</td>\n      <td>0.838181</td>\n      <td>0.858813</td>\n      <td>0.595307</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.331100</td>\n      <td>0.406393</td>\n      <td>0.861044</td>\n      <td>0.843570</td>\n      <td>0.834343</td>\n      <td>0.861044</td>\n      <td>0.590698</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 14:57:49,874] Trial 0 finished with value: 0.8449373972480296 and parameters: {'lr': 1.2908549889168956e-05, 'weight_decay': 0.005103235594127304, 'warmup_ratio': 0.20031204046361972, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1}. Best is trial 0 with value: 0.8449373972480296.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4203' max='4203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4203/4203 1:08:31, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.458700</td>\n      <td>0.457488</td>\n      <td>0.848907</td>\n      <td>0.786391</td>\n      <td>0.766977</td>\n      <td>0.848907</td>\n      <td>0.267915</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.451600</td>\n      <td>0.424410</td>\n      <td>0.857073</td>\n      <td>0.826741</td>\n      <td>0.813072</td>\n      <td>0.857073</td>\n      <td>0.516674</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.405500</td>\n      <td>0.414798</td>\n      <td>0.856894</td>\n      <td>0.837705</td>\n      <td>0.824476</td>\n      <td>0.856894</td>\n      <td>0.549954</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.395500</td>\n      <td>0.407590</td>\n      <td>0.857787</td>\n      <td>0.835000</td>\n      <td>0.822641</td>\n      <td>0.857787</td>\n      <td>0.566061</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.409300</td>\n      <td>0.397742</td>\n      <td>0.864212</td>\n      <td>0.838941</td>\n      <td>0.829430</td>\n      <td>0.864212</td>\n      <td>0.553894</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.346800</td>\n      <td>0.405058</td>\n      <td>0.860509</td>\n      <td>0.846863</td>\n      <td>0.837640</td>\n      <td>0.860509</td>\n      <td>0.591649</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.350900</td>\n      <td>0.404855</td>\n      <td>0.864480</td>\n      <td>0.843685</td>\n      <td>0.833770</td>\n      <td>0.864480</td>\n      <td>0.578305</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.349900</td>\n      <td>0.400785</td>\n      <td>0.863498</td>\n      <td>0.844011</td>\n      <td>0.835188</td>\n      <td>0.863498</td>\n      <td>0.582050</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 16:07:54,053] Trial 1 finished with value: 0.8452050854315599 and parameters: {'lr': 1.8113121559653823e-05, 'weight_decay': 0.0016944490683774038, 'warmup_ratio': 0.17469543112886446, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 1}. Best is trial 1 with value: 0.8452050854315599.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2100/2100 1:01:17, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.437200</td>\n      <td>0.429138</td>\n      <td>0.855422</td>\n      <td>0.820127</td>\n      <td>0.810937</td>\n      <td>0.855422</td>\n      <td>0.514251</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.403300</td>\n      <td>0.416942</td>\n      <td>0.854975</td>\n      <td>0.828642</td>\n      <td>0.820820</td>\n      <td>0.854975</td>\n      <td>0.561888</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.332800</td>\n      <td>0.408509</td>\n      <td>0.858322</td>\n      <td>0.846569</td>\n      <td>0.838387</td>\n      <td>0.858322</td>\n      <td>0.588779</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.344400</td>\n      <td>0.402647</td>\n      <td>0.862695</td>\n      <td>0.845243</td>\n      <td>0.836685</td>\n      <td>0.862695</td>\n      <td>0.587676</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 17:10:45,788] Trial 2 finished with value: 0.846695782810575 and parameters: {'lr': 4.21914509537095e-05, 'weight_decay': 0.009384607119192066, 'warmup_ratio': 0.2793705261401517, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 2}. Best is trial 2 with value: 0.846695782810575.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4203' max='4203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4203/4203 1:08:23, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.488100</td>\n      <td>0.478244</td>\n      <td>0.845649</td>\n      <td>0.782522</td>\n      <td>0.740846</td>\n      <td>0.845649</td>\n      <td>0.245852</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.465300</td>\n      <td>0.434199</td>\n      <td>0.855020</td>\n      <td>0.822886</td>\n      <td>0.807591</td>\n      <td>0.855020</td>\n      <td>0.506422</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.419700</td>\n      <td>0.423430</td>\n      <td>0.856939</td>\n      <td>0.829464</td>\n      <td>0.816218</td>\n      <td>0.856939</td>\n      <td>0.506961</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.405900</td>\n      <td>0.412992</td>\n      <td>0.857385</td>\n      <td>0.833413</td>\n      <td>0.820499</td>\n      <td>0.857385</td>\n      <td>0.557531</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.420300</td>\n      <td>0.404243</td>\n      <td>0.863231</td>\n      <td>0.831326</td>\n      <td>0.816522</td>\n      <td>0.863231</td>\n      <td>0.529535</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.366600</td>\n      <td>0.404947</td>\n      <td>0.861312</td>\n      <td>0.843602</td>\n      <td>0.835607</td>\n      <td>0.861312</td>\n      <td>0.579797</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.371000</td>\n      <td>0.406148</td>\n      <td>0.863498</td>\n      <td>0.839515</td>\n      <td>0.829936</td>\n      <td>0.863498</td>\n      <td>0.565254</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.368100</td>\n      <td>0.401107</td>\n      <td>0.863454</td>\n      <td>0.840052</td>\n      <td>0.829223</td>\n      <td>0.863454</td>\n      <td>0.573669</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 18:20:41,670] Trial 3 finished with value: 0.840466066878496 and parameters: {'lr': 1.0575488202143836e-05, 'weight_decay': 0.004587317825934914, 'warmup_ratio': 0.2357886622799123, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 1}. Best is trial 2 with value: 0.846695782810575.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8406' max='8406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8406/8406 1:11:45, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.504300</td>\n      <td>0.489455</td>\n      <td>0.849264</td>\n      <td>0.786121</td>\n      <td>0.743520</td>\n      <td>0.849264</td>\n      <td>0.244277</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.466000</td>\n      <td>0.446358</td>\n      <td>0.854172</td>\n      <td>0.797076</td>\n      <td>0.778382</td>\n      <td>0.854172</td>\n      <td>0.376166</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.443400</td>\n      <td>0.455919</td>\n      <td>0.844712</td>\n      <td>0.799306</td>\n      <td>0.765619</td>\n      <td>0.844712</td>\n      <td>0.504532</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.480400</td>\n      <td>0.446641</td>\n      <td>0.851495</td>\n      <td>0.792242</td>\n      <td>0.763195</td>\n      <td>0.851495</td>\n      <td>0.332607</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.442500</td>\n      <td>0.453687</td>\n      <td>0.856983</td>\n      <td>0.826146</td>\n      <td>0.809185</td>\n      <td>0.856983</td>\n      <td>0.506147</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.419000</td>\n      <td>0.446032</td>\n      <td>0.853101</td>\n      <td>0.801979</td>\n      <td>0.814351</td>\n      <td>0.853101</td>\n      <td>0.454453</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.449100</td>\n      <td>0.457443</td>\n      <td>0.856582</td>\n      <td>0.832400</td>\n      <td>0.816678</td>\n      <td>0.856582</td>\n      <td>0.545300</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.432600</td>\n      <td>0.444031</td>\n      <td>0.859527</td>\n      <td>0.815845</td>\n      <td>0.804596</td>\n      <td>0.859527</td>\n      <td>0.482341</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.421800</td>\n      <td>0.440918</td>\n      <td>0.850602</td>\n      <td>0.833796</td>\n      <td>0.823935</td>\n      <td>0.850602</td>\n      <td>0.545837</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.447300</td>\n      <td>0.430693</td>\n      <td>0.859259</td>\n      <td>0.836189</td>\n      <td>0.822976</td>\n      <td>0.859259</td>\n      <td>0.560887</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.411600</td>\n      <td>0.443993</td>\n      <td>0.860018</td>\n      <td>0.831591</td>\n      <td>0.818932</td>\n      <td>0.860018</td>\n      <td>0.555549</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.361700</td>\n      <td>0.449977</td>\n      <td>0.840696</td>\n      <td>0.836228</td>\n      <td>0.833586</td>\n      <td>0.840696</td>\n      <td>0.572272</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.380000</td>\n      <td>0.443130</td>\n      <td>0.861758</td>\n      <td>0.831180</td>\n      <td>0.820330</td>\n      <td>0.861758</td>\n      <td>0.549944</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.355600</td>\n      <td>0.415792</td>\n      <td>0.861669</td>\n      <td>0.836915</td>\n      <td>0.823504</td>\n      <td>0.861669</td>\n      <td>0.553751</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.353100</td>\n      <td>0.425589</td>\n      <td>0.856091</td>\n      <td>0.842730</td>\n      <td>0.831583</td>\n      <td>0.856091</td>\n      <td>0.579673</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.342400</td>\n      <td>0.427108</td>\n      <td>0.860018</td>\n      <td>0.842387</td>\n      <td>0.832698</td>\n      <td>0.860018</td>\n      <td>0.574963</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 19:33:59,322] Trial 4 finished with value: 0.841624913275903 and parameters: {'lr': 8.277445575479811e-05, 'weight_decay': 0.09296085054594586, 'warmup_ratio': 0.21481125312117733, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1}. Best is trial 2 with value: 0.846695782810575.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8406' max='8406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8406/8406 1:11:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.550200</td>\n      <td>0.513551</td>\n      <td>0.844668</td>\n      <td>0.773541</td>\n      <td>0.713463</td>\n      <td>0.844668</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.482800</td>\n      <td>0.466245</td>\n      <td>0.849487</td>\n      <td>0.794583</td>\n      <td>0.764157</td>\n      <td>0.849487</td>\n      <td>0.336728</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.419700</td>\n      <td>0.441047</td>\n      <td>0.852209</td>\n      <td>0.810869</td>\n      <td>0.806179</td>\n      <td>0.852209</td>\n      <td>0.475091</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.451100</td>\n      <td>0.432932</td>\n      <td>0.855600</td>\n      <td>0.812678</td>\n      <td>0.798658</td>\n      <td>0.855600</td>\n      <td>0.463824</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.429900</td>\n      <td>0.425029</td>\n      <td>0.858724</td>\n      <td>0.823858</td>\n      <td>0.814200</td>\n      <td>0.858724</td>\n      <td>0.513370</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.389400</td>\n      <td>0.420770</td>\n      <td>0.860598</td>\n      <td>0.830620</td>\n      <td>0.818238</td>\n      <td>0.860598</td>\n      <td>0.510251</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.429500</td>\n      <td>0.410238</td>\n      <td>0.859750</td>\n      <td>0.826963</td>\n      <td>0.820555</td>\n      <td>0.859750</td>\n      <td>0.535946</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.403800</td>\n      <td>0.407433</td>\n      <td>0.861000</td>\n      <td>0.828845</td>\n      <td>0.818094</td>\n      <td>0.861000</td>\n      <td>0.537667</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.407600</td>\n      <td>0.405387</td>\n      <td>0.858858</td>\n      <td>0.841904</td>\n      <td>0.829439</td>\n      <td>0.858858</td>\n      <td>0.569454</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.435900</td>\n      <td>0.401827</td>\n      <td>0.864079</td>\n      <td>0.836297</td>\n      <td>0.824805</td>\n      <td>0.864079</td>\n      <td>0.551092</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.381500</td>\n      <td>0.407751</td>\n      <td>0.862249</td>\n      <td>0.831713</td>\n      <td>0.823634</td>\n      <td>0.862249</td>\n      <td>0.553310</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.346400</td>\n      <td>0.413183</td>\n      <td>0.855868</td>\n      <td>0.844055</td>\n      <td>0.836278</td>\n      <td>0.855868</td>\n      <td>0.592428</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.369200</td>\n      <td>0.414485</td>\n      <td>0.863052</td>\n      <td>0.836769</td>\n      <td>0.828240</td>\n      <td>0.863052</td>\n      <td>0.548106</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.346000</td>\n      <td>0.409087</td>\n      <td>0.861669</td>\n      <td>0.843693</td>\n      <td>0.833737</td>\n      <td>0.861669</td>\n      <td>0.581719</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.349900</td>\n      <td>0.407664</td>\n      <td>0.860464</td>\n      <td>0.845375</td>\n      <td>0.838025</td>\n      <td>0.860464</td>\n      <td>0.592294</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.342200</td>\n      <td>0.406115</td>\n      <td>0.862695</td>\n      <td>0.843731</td>\n      <td>0.834669</td>\n      <td>0.862695</td>\n      <td>0.582968</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n  warnings.warn(stats.ConstantInputWarning(warn_msg))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 20:47:17,956] Trial 5 finished with value: 0.8444547405419074 and parameters: {'lr': 1.1628440612281945e-05, 'weight_decay': 0.09399225385040957, 'warmup_ratio': 0.22360217043327646, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1}. Best is trial 2 with value: 0.846695782810575.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4203' max='4203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4203/4203 1:08:23, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.448000</td>\n      <td>0.452833</td>\n      <td>0.852075</td>\n      <td>0.790266</td>\n      <td>0.760251</td>\n      <td>0.852075</td>\n      <td>0.330933</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.465400</td>\n      <td>0.446676</td>\n      <td>0.851896</td>\n      <td>0.792598</td>\n      <td>0.754545</td>\n      <td>0.851896</td>\n      <td>0.349547</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.421000</td>\n      <td>0.425358</td>\n      <td>0.856627</td>\n      <td>0.823692</td>\n      <td>0.809352</td>\n      <td>0.856627</td>\n      <td>0.495121</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.412700</td>\n      <td>0.418232</td>\n      <td>0.859750</td>\n      <td>0.837370</td>\n      <td>0.823080</td>\n      <td>0.859750</td>\n      <td>0.565034</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.413900</td>\n      <td>0.410266</td>\n      <td>0.862963</td>\n      <td>0.836380</td>\n      <td>0.821001</td>\n      <td>0.862963</td>\n      <td>0.549605</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.340700</td>\n      <td>0.427696</td>\n      <td>0.859661</td>\n      <td>0.836174</td>\n      <td>0.823681</td>\n      <td>0.859661</td>\n      <td>0.569344</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.343000</td>\n      <td>0.416908</td>\n      <td>0.864971</td>\n      <td>0.840546</td>\n      <td>0.829296</td>\n      <td>0.864971</td>\n      <td>0.569095</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.332700</td>\n      <td>0.423378</td>\n      <td>0.859839</td>\n      <td>0.844904</td>\n      <td>0.836146</td>\n      <td>0.859839</td>\n      <td>0.584973</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 21:57:13,604] Trial 6 finished with value: 0.8449711487778618 and parameters: {'lr': 9.052400594277023e-05, 'weight_decay': 0.0015168796304120772, 'warmup_ratio': 0.19936912457688194, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 1}. Best is trial 2 with value: 0.846695782810575.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4203' max='4203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4203/4203 1:08:24, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.449600</td>\n      <td>0.451266</td>\n      <td>0.851049</td>\n      <td>0.796522</td>\n      <td>0.775784</td>\n      <td>0.851049</td>\n      <td>0.338741</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.453500</td>\n      <td>0.429864</td>\n      <td>0.854752</td>\n      <td>0.799174</td>\n      <td>0.791663</td>\n      <td>0.854752</td>\n      <td>0.417902</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.416100</td>\n      <td>0.422944</td>\n      <td>0.859170</td>\n      <td>0.821779</td>\n      <td>0.813291</td>\n      <td>0.859170</td>\n      <td>0.496056</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.410200</td>\n      <td>0.435102</td>\n      <td>0.850067</td>\n      <td>0.834144</td>\n      <td>0.825839</td>\n      <td>0.850067</td>\n      <td>0.571445</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.416800</td>\n      <td>0.405026</td>\n      <td>0.861892</td>\n      <td>0.834195</td>\n      <td>0.818182</td>\n      <td>0.861892</td>\n      <td>0.550739</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.345100</td>\n      <td>0.418975</td>\n      <td>0.855957</td>\n      <td>0.843700</td>\n      <td>0.839331</td>\n      <td>0.855957</td>\n      <td>0.593497</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.342500</td>\n      <td>0.409511</td>\n      <td>0.862784</td>\n      <td>0.838406</td>\n      <td>0.827959</td>\n      <td>0.862784</td>\n      <td>0.565121</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.337300</td>\n      <td>0.409752</td>\n      <td>0.861624</td>\n      <td>0.844803</td>\n      <td>0.835703</td>\n      <td>0.861624</td>\n      <td>0.583874</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 23:07:10,704] Trial 7 finished with value: 0.8472489786057343 and parameters: {'lr': 6.595368363589886e-05, 'weight_decay': 0.004428752267862051, 'warmup_ratio': 0.28267504629895923, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 1}. Best is trial 7 with value: 0.8472489786057343.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4203' max='4203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4203/4203 57:36, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.467100</td>\n      <td>0.459218</td>\n      <td>0.847880</td>\n      <td>0.781363</td>\n      <td>0.732340</td>\n      <td>0.847880</td>\n      <td>0.175924</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.457300</td>\n      <td>0.436856</td>\n      <td>0.854529</td>\n      <td>0.802031</td>\n      <td>0.782177</td>\n      <td>0.854529</td>\n      <td>0.446377</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.413700</td>\n      <td>0.422977</td>\n      <td>0.858010</td>\n      <td>0.833608</td>\n      <td>0.819327</td>\n      <td>0.858010</td>\n      <td>0.531949</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.407600</td>\n      <td>0.418664</td>\n      <td>0.855868</td>\n      <td>0.838632</td>\n      <td>0.825852</td>\n      <td>0.855868</td>\n      <td>0.574062</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.413200</td>\n      <td>0.410789</td>\n      <td>0.863141</td>\n      <td>0.828449</td>\n      <td>0.813872</td>\n      <td>0.863141</td>\n      <td>0.525547</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.336900</td>\n      <td>0.429644</td>\n      <td>0.847702</td>\n      <td>0.840123</td>\n      <td>0.833285</td>\n      <td>0.847702</td>\n      <td>0.580878</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.339800</td>\n      <td>0.421776</td>\n      <td>0.861089</td>\n      <td>0.838137</td>\n      <td>0.827341</td>\n      <td>0.861089</td>\n      <td>0.566291</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.327700</td>\n      <td>0.426641</td>\n      <td>0.858411</td>\n      <td>0.842524</td>\n      <td>0.835339</td>\n      <td>0.858411</td>\n      <td>0.574972</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-14 00:06:19,712] Trial 8 finished with value: 0.8413704791104608 and parameters: {'lr': 9.284592059747402e-05, 'weight_decay': 0.010106986253487358, 'warmup_ratio': 0.12441696281081911, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 2}. Best is trial 7 with value: 0.8472489786057343.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1050/1050 57:42, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.419700</td>\n      <td>0.414583</td>\n      <td>0.860062</td>\n      <td>0.835728</td>\n      <td>0.821073</td>\n      <td>0.860062</td>\n      <td>0.554697</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.392900</td>\n      <td>0.405321</td>\n      <td>0.860732</td>\n      <td>0.833539</td>\n      <td>0.832534</td>\n      <td>0.860732</td>\n      <td>0.554410</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-14 01:05:36,541] Trial 9 finished with value: 0.8350537223997377 and parameters: {'lr': 1.3611049248706139e-05, 'weight_decay': 0.0068580610507093884, 'warmup_ratio': 0.1183149979230736, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 4}. Best is trial 7 with value: 0.8472489786057343.\n","output_type":"stream"},{"name":"stdout","text":"Best trial:\n  Value:  0.8472489786057343\n  Params: \n    lr: 6.595368363589886e-05\n    weight_decay: 0.004428752267862051\n    warmup_ratio: 0.28267504629895923\n    per_device_train_batch_size: 64\n    gradient_accumulation_steps: 1\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 171\u001b[0m\n\u001b[1;32m    167\u001b[0m best_model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepPavlov/rubert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    169\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m best_training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results/best_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mper_device_train_batch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwarmup_ratio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs/best_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_accumulation_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw_torch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    191\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m best_trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    194\u001b[0m     model\u001b[38;5;241m=\u001b[39mbest_model,\n\u001b[1;32m    195\u001b[0m     args\u001b[38;5;241m=\u001b[39mbest_training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    202\u001b[0m best_trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m<string>:129\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1556\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_best_model_at_end:\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_strategy:\n\u001b[0;32m-> 1556\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--load_best_model_at_end requires the save and eval strategy to match, but found\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- Evaluation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1558\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- Save strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m         )\n\u001b[1;32m   1560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy \u001b[38;5;241m==\u001b[39m IntervalStrategy\u001b[38;5;241m.\u001b[39mSTEPS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","\u001b[0;31mValueError\u001b[0m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: steps\n- Save strategy: epoch"],"ename":"ValueError","evalue":"--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: steps\n- Save strategy: epoch","output_type":"error"}]},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset, load_from_disk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nfrom optuna.samplers import TPESampler\nimport pymorphy2\nimport re\nfrom functools import partial, lru_cache\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, Общая память: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"Общая память GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"Свободная память GPU: {free_memory / 1e9:.2f} GB\")\n\n# Проверяем, есть ли уже обработанные данные\nif os.path.exists('./processed_data'):\n    encoded_train = load_from_disk('./processed_data/train')\n    encoded_val = load_from_disk('./processed_data/val')\n    print(\"Загружены предобработанные данные\")\nelse:\n    df = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\n    df = df[df['language'] == 'russian']\n    df['rating_class'] = df['rating'].astype(int) - 1\n\n    morph = pymorphy2.MorphAnalyzer()\n    stop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\n    @lru_cache(maxsize=None)\n    def lemmatize(word):\n        return morph.parse(word)[0].normal_form\n\n    def preprocess_text(text):\n        if pd.isna(text) or not isinstance(text, str):\n            return ''\n        text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n        return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\n    df['processed_text'] = df['combined_text'].apply(preprocess_text)\n    df = df[['processed_text', 'rating_class']].dropna()\n\n    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\n    del df\n    gc.collect()\n\n    tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"processed_text\"], truncation=True, max_length=256)\n\n    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\n    del train_df, val_df\n    gc.collect()\n\n    encoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n    encoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\n    encoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\n    encoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\n    # Сохраняем закодированные наборы данных на диск\n    encoded_train.save_to_disk('./processed_data/train')\n    encoded_val.save_to_disk('./processed_data/val')\n    print(\"Данные обработаны и сохранены\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'),\n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-3, 1e-1, log=True)\n    warmup_ratio = trial.suggest_float('warmup_ratio', 0.1, 0.3)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [32, 64])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_ratio=warmup_ratio,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"no\",\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=0,  # Изменено на 0 для избежания создания дополнительных рабочих процессов\n        optim=\"adamw_torch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=5)  # Уменьшено количество trial до 5\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_ratio=trial.params['warmup_ratio'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=0,  # Изменено на 0 для избежания создания дополнительных рабочих процессов\n    optim=\"adamw_torch\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Фактический класс')\nplt.title('Матрица ошибок')\nplt.tight_layout()\nplt.show()\n\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('Фактический класс')\nplt.ylabel('Предсказанный класс')\nplt.title('Распределение предсказаний по классам')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Количество')\nplt.title('Распределение предсказанных классов')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T03:35:09.840620Z","iopub.execute_input":"2024-07-14T03:35:09.841012Z","iopub.status.idle":"2024-07-14T10:53:36.427092Z","shell.execute_reply.started":"2024-07-14T03:35:09.840978Z","shell.execute_reply":"2024-07-14T10:53:36.426101Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-14 03:35:17.829177: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-14 03:35:17.829372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-14 03:35:17.958531: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Используется устройство: cuda\nGPU: Tesla P100-PCIE-16GB, Общая память: 17.06 GB\nОбщая память GPU: 16.79 GB\nСвободная память GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d841ea97f7a431e81851bd510b4f221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fe4635c4f634cd8adddb859d6f1a7e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ee825851b94dbdb0643d9ffdeefa40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f2790d8e78443cb21d65a5f44afc38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43126fa6aa7849f9867e86e5a967ab35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c91851ad1047fea92a32f7af811e1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee5fbe98b92c4be89af67717bb5873ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e9d04a9b3f34272ad70e7f0fba530e0"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 03:35:55,454] A new study created in memory with name: no-name-7428978d-f6ae-4e29-99c6-141593eee4e2\n","output_type":"stream"},{"name":"stdout","text":"Данные обработаны и сохранены\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deb9866991d54bbdbd9cd11b6e95be2a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1050/1050 58:46, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.404800</td>\n      <td>0.419694</td>\n      <td>0.856627</td>\n      <td>0.830801</td>\n      <td>0.824443</td>\n      <td>0.856627</td>\n      <td>0.549787</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.342000</td>\n      <td>0.406166</td>\n      <td>0.863141</td>\n      <td>0.844933</td>\n      <td>0.835771</td>\n      <td>0.863141</td>\n      <td>0.586659</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 04:36:25,536] Trial 0 finished with value: 0.8470377699516728 and parameters: {'lr': 8.373000577537992e-05, 'weight_decay': 0.09133311151769442, 'warmup_ratio': 0.23770055385498493, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.8470377699516728.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1050/1050 58:55, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.412100</td>\n      <td>0.408677</td>\n      <td>0.861044</td>\n      <td>0.831446</td>\n      <td>0.816902</td>\n      <td>0.861044</td>\n      <td>0.534992</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.372900</td>\n      <td>0.400262</td>\n      <td>0.862918</td>\n      <td>0.837787</td>\n      <td>0.826341</td>\n      <td>0.862918</td>\n      <td>0.568555</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 05:37:00,311] Trial 1 finished with value: 0.8405681839139436 and parameters: {'lr': 2.4630040082862688e-05, 'weight_decay': 0.0687607389556874, 'warmup_ratio': 0.24717400447156507, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.8470377699516728.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8406' max='8406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8406/8406 1:14:01, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.519600</td>\n      <td>0.493736</td>\n      <td>0.845114</td>\n      <td>0.776397</td>\n      <td>0.734690</td>\n      <td>0.845114</td>\n      <td>0.136588</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.460700</td>\n      <td>0.444366</td>\n      <td>0.854261</td>\n      <td>0.802292</td>\n      <td>0.778606</td>\n      <td>0.854261</td>\n      <td>0.394692</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.423900</td>\n      <td>0.450307</td>\n      <td>0.852075</td>\n      <td>0.811416</td>\n      <td>0.811460</td>\n      <td>0.852075</td>\n      <td>0.498615</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.458300</td>\n      <td>0.433483</td>\n      <td>0.855065</td>\n      <td>0.795855</td>\n      <td>0.788600</td>\n      <td>0.855065</td>\n      <td>0.395184</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.428100</td>\n      <td>0.442286</td>\n      <td>0.855779</td>\n      <td>0.828806</td>\n      <td>0.812529</td>\n      <td>0.855779</td>\n      <td>0.515889</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.386000</td>\n      <td>0.416715</td>\n      <td>0.859393</td>\n      <td>0.820329</td>\n      <td>0.812961</td>\n      <td>0.859393</td>\n      <td>0.500301</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.425500</td>\n      <td>0.423166</td>\n      <td>0.852164</td>\n      <td>0.831703</td>\n      <td>0.824424</td>\n      <td>0.852164</td>\n      <td>0.567447</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.404800</td>\n      <td>0.409024</td>\n      <td>0.862160</td>\n      <td>0.828553</td>\n      <td>0.821619</td>\n      <td>0.862160</td>\n      <td>0.560393</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.407200</td>\n      <td>0.424950</td>\n      <td>0.849576</td>\n      <td>0.840223</td>\n      <td>0.832963</td>\n      <td>0.849576</td>\n      <td>0.572231</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.427700</td>\n      <td>0.413096</td>\n      <td>0.864212</td>\n      <td>0.831892</td>\n      <td>0.819244</td>\n      <td>0.864212</td>\n      <td>0.553932</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.384800</td>\n      <td>0.415261</td>\n      <td>0.862383</td>\n      <td>0.838844</td>\n      <td>0.828818</td>\n      <td>0.862383</td>\n      <td>0.581071</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.333200</td>\n      <td>0.425158</td>\n      <td>0.851406</td>\n      <td>0.841410</td>\n      <td>0.836053</td>\n      <td>0.851406</td>\n      <td>0.592524</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.349100</td>\n      <td>0.426853</td>\n      <td>0.863454</td>\n      <td>0.842252</td>\n      <td>0.835259</td>\n      <td>0.863454</td>\n      <td>0.577133</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.329300</td>\n      <td>0.410882</td>\n      <td>0.862651</td>\n      <td>0.844181</td>\n      <td>0.834041</td>\n      <td>0.862651</td>\n      <td>0.583614</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.334200</td>\n      <td>0.420564</td>\n      <td>0.857653</td>\n      <td>0.845764</td>\n      <td>0.841514</td>\n      <td>0.857653</td>\n      <td>0.592719</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.311500</td>\n      <td>0.423941</td>\n      <td>0.861357</td>\n      <td>0.845149</td>\n      <td>0.835827</td>\n      <td>0.861357</td>\n      <td>0.588457</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 06:52:38,186] Trial 2 finished with value: 0.8443133626381017 and parameters: {'lr': 4.175889942231109e-05, 'weight_decay': 0.02062955456547391, 'warmup_ratio': 0.24104171894041945, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1}. Best is trial 0 with value: 0.8470377699516728.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2100/2100 1:02:34, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.437900</td>\n      <td>0.442390</td>\n      <td>0.854797</td>\n      <td>0.804256</td>\n      <td>0.790514</td>\n      <td>0.854797</td>\n      <td>0.435750</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.408900</td>\n      <td>0.413400</td>\n      <td>0.858367</td>\n      <td>0.827869</td>\n      <td>0.816876</td>\n      <td>0.858367</td>\n      <td>0.539125</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.366800</td>\n      <td>0.405628</td>\n      <td>0.861446</td>\n      <td>0.842155</td>\n      <td>0.832185</td>\n      <td>0.861446</td>\n      <td>0.566971</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.387600</td>\n      <td>0.403217</td>\n      <td>0.861758</td>\n      <td>0.835918</td>\n      <td>0.826036</td>\n      <td>0.861758</td>\n      <td>0.561065</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 07:56:49,943] Trial 3 finished with value: 0.8370660165129629 and parameters: {'lr': 1.1631236085222124e-05, 'weight_decay': 0.022463551697889522, 'warmup_ratio': 0.1351252030363727, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 2}. Best is trial 0 with value: 0.8470377699516728.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8406' max='8406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8406/8406 1:14:02, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.533200</td>\n      <td>0.526537</td>\n      <td>0.844578</td>\n      <td>0.773938</td>\n      <td>0.718111</td>\n      <td>0.844578</td>\n      <td>0.070020</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.471700</td>\n      <td>0.452273</td>\n      <td>0.851272</td>\n      <td>0.795529</td>\n      <td>0.775104</td>\n      <td>0.851272</td>\n      <td>0.324987</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.420900</td>\n      <td>0.448786</td>\n      <td>0.853503</td>\n      <td>0.802906</td>\n      <td>0.805481</td>\n      <td>0.853503</td>\n      <td>0.445649</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.448800</td>\n      <td>0.432582</td>\n      <td>0.854485</td>\n      <td>0.796223</td>\n      <td>0.797905</td>\n      <td>0.854485</td>\n      <td>0.388064</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.425600</td>\n      <td>0.422924</td>\n      <td>0.859973</td>\n      <td>0.835043</td>\n      <td>0.820087</td>\n      <td>0.859973</td>\n      <td>0.534117</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.386000</td>\n      <td>0.411533</td>\n      <td>0.861490</td>\n      <td>0.831964</td>\n      <td>0.822565</td>\n      <td>0.861490</td>\n      <td>0.532780</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.421500</td>\n      <td>0.414137</td>\n      <td>0.855823</td>\n      <td>0.832044</td>\n      <td>0.823684</td>\n      <td>0.855823</td>\n      <td>0.564464</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.397500</td>\n      <td>0.411369</td>\n      <td>0.860732</td>\n      <td>0.836361</td>\n      <td>0.835969</td>\n      <td>0.860732</td>\n      <td>0.575586</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.404600</td>\n      <td>0.413351</td>\n      <td>0.855600</td>\n      <td>0.844803</td>\n      <td>0.835405</td>\n      <td>0.855600</td>\n      <td>0.582116</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.427100</td>\n      <td>0.402339</td>\n      <td>0.863632</td>\n      <td>0.837866</td>\n      <td>0.828427</td>\n      <td>0.863632</td>\n      <td>0.568740</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.375200</td>\n      <td>0.409937</td>\n      <td>0.864436</td>\n      <td>0.839628</td>\n      <td>0.829293</td>\n      <td>0.864436</td>\n      <td>0.580893</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.334300</td>\n      <td>0.412080</td>\n      <td>0.860464</td>\n      <td>0.846720</td>\n      <td>0.837605</td>\n      <td>0.860464</td>\n      <td>0.595220</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.345900</td>\n      <td>0.419908</td>\n      <td>0.862517</td>\n      <td>0.843627</td>\n      <td>0.835694</td>\n      <td>0.862517</td>\n      <td>0.582005</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.330200</td>\n      <td>0.409774</td>\n      <td>0.861981</td>\n      <td>0.847039</td>\n      <td>0.837882</td>\n      <td>0.861981</td>\n      <td>0.595252</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.333300</td>\n      <td>0.416523</td>\n      <td>0.859572</td>\n      <td>0.847778</td>\n      <td>0.841574</td>\n      <td>0.859572</td>\n      <td>0.600862</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.313100</td>\n      <td>0.414775</td>\n      <td>0.861312</td>\n      <td>0.845552</td>\n      <td>0.836167</td>\n      <td>0.861312</td>\n      <td>0.594489</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 09:12:28,064] Trial 4 finished with value: 0.8463920083764923 and parameters: {'lr': 2.5805374627532186e-05, 'weight_decay': 0.0015557066519140516, 'warmup_ratio': 0.21866189482122533, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1}. Best is trial 0 with value: 0.8470377699516728.\n","output_type":"stream"},{"name":"stdout","text":"Best trial:\n  Value:  0.8470377699516728\n  Params: \n    lr: 8.373000577537992e-05\n    weight_decay: 0.09133311151769442\n    warmup_ratio: 0.23770055385498493\n    per_device_train_batch_size: 64\n    gradient_accumulation_steps: 4\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1750/1750 1:37:52, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.421200</td>\n      <td>0.419832</td>\n      <td>0.858322</td>\n      <td>0.821793</td>\n      <td>0.816003</td>\n      <td>0.858322</td>\n      <td>0.530998</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.360900</td>\n      <td>0.410745</td>\n      <td>0.863186</td>\n      <td>0.842512</td>\n      <td>0.829087</td>\n      <td>0.863186</td>\n      <td>0.583056</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.246600</td>\n      <td>0.499953</td>\n      <td>0.850290</td>\n      <td>0.842199</td>\n      <td>0.835786</td>\n      <td>0.850290</td>\n      <td>0.571660</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Final Evaluation Results:\n{'eval_loss': 0.41074472665786743, 'eval_accuracy': 0.863186077643909, 'eval_f1': 0.842511900678718, 'eval_precision': 0.8290872852149572, 'eval_recall': 0.863186077643909, 'eval_spearman': 0.5830561139484278, 'eval_runtime': 93.7507, 'eval_samples_per_second': 239.038, 'eval_steps_per_second': 3.744, 'epoch': 4.996431120628123}\nTraining completed. Model saved in ./final_model/ directory\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1765: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  order = pd.unique(vector)\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.8632\nF1 (weighted): 0.8425\nPrecision (weighted): 0.8291\nRecall (weighted): 0.8632\nSpearman correlation: 0.5831\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"17813"},"metadata":{}}]},{"cell_type":"code","source":"\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nimport pandas as pd\nfrom datasets import Dataset\nimport re\nimport pymorphy2\n\n# Инициализация морфологического анализатора\nmorph = pymorphy2.MorphAnalyzer()\n\n# Определение стоп-слов (используйте тот же список, что и при обучении)\nstop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\n# Функция лемматизации\ndef lemmatize(word):\n    return morph.parse(word)[0].normal_form\n\n# Загрузка токенизатора и модели\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\nmodel = AutoModelForSequenceClassification.from_pretrained('./final_model')\nprint(\"Модель загружена успешно\")\nprint(f\"Конфигурация модели: {model.config}\")\n\n# Создание пайплайна для классификации\nclassifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n    return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    processed_reviews = [preprocess_text(review) for review in reviews]\n    results = classifier(processed_reviews)\n    predicted_ratings = [int(result['label'].split('_')[-1]) + 1 for result in results]\n    \n    return predicted_ratings\n\n# Функция для ввода отзывов с клавиатуры\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"Введите отзыв (или нажмите Enter для завершения): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# Вызываем функцию для ввода отзывов с клавиатуры\nnew_reviews = input_reviews()\n\n# Получаем предсказанные оценки для введенных отзывов\npredicted_ratings = predict_ratings(new_reviews)\n\n# Выводим результаты предсказаний\nif len(predicted_ratings) == 0:\n    print(\"Нет введенных отзывов.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"Отзыв: {review}\")\n        print(f\"Предсказанная оценка: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:38:12.044620Z","iopub.execute_input":"2024-07-14T11:38:12.045289Z","iopub.status.idle":"2024-07-14T11:38:17.262567Z","shell.execute_reply.started":"2024-07-14T11:38:12.045256Z","shell.execute_reply":"2024-07-14T11:38:17.261213Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a43eee278bd43f2a3c07946ba162fac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99195284d53b47b39f405c9a7201e26a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f33d843a6e4392aeed6eb2f31f9cde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c75036bfe848b1bb7328d5442d1bf6"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './final_model'.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Загрузка токенизатора и модели\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepPavlov/rubert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./final_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mМодель загружена успешно\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mКонфигурация модели: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:485\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './final_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."],"ename":"OSError","evalue":"Incorrect path_or_model_id: './final_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","output_type":"error"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nimport pandas as pd\nfrom datasets import Dataset\nimport re\nimport pymorphy2\n\n# Инициализация морфологического анализатора\nmorph = pymorphy2.MorphAnalyzer()\n\n# Определение стоп-слов (используйте тот же список, что и при обучении)\nstop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\n# Функция лемматизации\ndef lemmatize(word):\n    return morph.parse(word)[0].normal_form\n\n# Загрузка токенизатора и модели\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\nmodel = AutoModelForSequenceClassification.from_pretrained('./final_model')\n\n# Создание пайплайна для классификации\nclassifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n    return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    processed_reviews = [preprocess_text(review) for review in reviews]\n    results = classifier(processed_reviews)\n    predicted_ratings = [int(result['label'].split('_')[-1]) + 1 for result in results]\n    \n    return predicted_ratings\n\n# Функция для ввода отзывов с клавиатуры\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"Введите отзыв (или нажмите Enter для завершения): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# Вызываем функцию для ввода отзывов с клавиатуры\nnew_reviews = input_reviews()\n\n# Получаем предсказанные оценки для введенных отзывов\npredicted_ratings = predict_ratings(new_reviews)\n\n# Выводим результаты предсказаний\nif len(predicted_ratings) == 0:\n    print(\"Нет введенных отзывов.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"Отзыв: {review}\")\n        print(f\"Предсказанная оценка: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:38:56.721896Z","iopub.execute_input":"2024-07-14T11:38:56.722756Z","iopub.status.idle":"2024-07-14T11:38:57.629911Z","shell.execute_reply.started":"2024-07-14T11:38:56.722724Z","shell.execute_reply":"2024-07-14T11:38:57.628519Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './final_model'.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Загрузка токенизатора и модели\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepPavlov/rubert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./final_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Создание пайплайна для классификации\u001b[39;00m\n\u001b[1;32m     23\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:485\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './final_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."],"ename":"OSError","evalue":"Incorrect path_or_model_id: './final_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","output_type":"error"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nimport pandas as pd\nfrom datasets import Dataset\nimport re\nimport pymorphy2\n\n# Инициализация морфологического анализатора\nmorph = pymorphy2.MorphAnalyzer()\n\n# Определение стоп-слов (используйте тот же список, что и при обучении)\nstop_words = set(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так'])\n\n# Функция лемматизации\ndef lemmatize(word):\n    return morph.parse(word)[0].normal_form\n\n# Загрузка токенизатора и модели\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\nmodel = AutoModelForSequenceClassification.from_pretrained('./final_model')\n\n# Создание пайплайна для классификации\nclassifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^а-яёa-z\\s]', '', text.lower().strip())\n    return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    processed_reviews = [preprocess_text(review) for review in reviews]\n    results = classifier(processed_reviews)\n    predicted_ratings = [int(result['label'].split('_')[-1]) + 1 for result in results]\n    \n    return predicted_ratings\n\n# Функция для ввода отзывов с клавиатуры\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"Введите отзыв (или нажмите Enter для завершения): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# Вызываем функцию для ввода отзывов с клавиатуры\nnew_reviews = input_reviews()\n\n# Получаем предсказанные оценки для введенных отзывов\npredicted_ratings = predict_ratings(new_reviews)\n\n# Выводим результаты предсказаний\nif len(predicted_ratings) == 0:\n    print(\"Нет введенных отзывов.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"Отзыв: {review}\")\n        print(f\"Предсказанная оценка: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:20:18.554831Z","iopub.execute_input":"2024-07-14T11:20:18.555179Z","iopub.status.idle":"2024-07-14T11:21:14.133853Z","shell.execute_reply.started":"2024-07-14T11:20:18.555151Z","shell.execute_reply":"2024-07-14T11:21:14.132852Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdin","text":"Введите отзыв (или нажмите Enter для завершения):  Нам очень понравился креативный, но одновременно рациональный подход к задаче, умение выслушать клиента и понять потребности целевой аудитории. Отмечаем высокий профессионализм сотрудников компании “KozhinDev”, качество и сжатые сроки выполнения проекта.\nВведите отзыв (или нажмите Enter для завершения):   Глубокое погружение в проект, четкое видение и полное понимание ТЗ, а также задач, которые обсуждали на установочной сессии. Разработка заняла 3 месяца, как и планировалось, а дальше пошли докрутки и дополнения. Сотрудничаем с 2019 года и по сей день.\nВведите отзыв (или нажмите Enter для завершения):  Мы заказывали у студии KozhinDev разработку сайта и админ панель к нему. Работа сделана качественно, в срок. Менеджер всегда на связи. Правки в ТЗ обсуждались и вносились в проект быстро и без всяких проблем. Баги устраняли тоже очень быстро. Ребята очень контактные, готовы выслушать, понять, предложить варианты и реализовать.\nВведите отзыв (или нажмите Enter для завершения):  лучший сервис\nВведите отзыв (или нажмите Enter для завершения):  средний сервис\nВведите отзыв (или нажмите Enter для завершения):  худший сервис\nВведите отзыв (или нажмите Enter для завершения):  \n"},{"name":"stdout","text":"Отзыв: Нам очень понравился креативный, но одновременно рациональный подход к задаче, умение выслушать клиента и понять потребности целевой аудитории. Отмечаем высокий профессионализм сотрудников компании “KozhinDev”, качество и сжатые сроки выполнения проекта.\nПредсказанная оценка: 5\n\nОтзыв:  Глубокое погружение в проект, четкое видение и полное понимание ТЗ, а также задач, которые обсуждали на установочной сессии. Разработка заняла 3 месяца, как и планировалось, а дальше пошли докрутки и дополнения. Сотрудничаем с 2019 года и по сей день.\nПредсказанная оценка: 5\n\nОтзыв: Мы заказывали у студии KozhinDev разработку сайта и админ панель к нему. Работа сделана качественно, в срок. Менеджер всегда на связи. Правки в ТЗ обсуждались и вносились в проект быстро и без всяких проблем. Баги устраняли тоже очень быстро. Ребята очень контактные, готовы выслушать, понять, предложить варианты и реализовать.\nПредсказанная оценка: 5\n\nОтзыв: лучший сервис\nПредсказанная оценка: 5\n\nОтзыв: средний сервис\nПредсказанная оценка: 3\n\nОтзыв: худший сервис\nПредсказанная оценка: 1\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\ntry:\n    model = AutoModelForSequenceClassification.from_pretrained('./final_model')\n    print(\"Модель успешно загружена\")\n    print(f\"Количество классов: {model.num_labels}\")\n    print(f\"Название базовой модели: {model.name_or_path}\")\nexcept Exception as e:\n    print(f\"Ошибка при загрузке модели: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:29:16.135432Z","iopub.execute_input":"2024-07-14T11:29:16.135978Z","iopub.status.idle":"2024-07-14T11:29:22.258460Z","shell.execute_reply.started":"2024-07-14T11:29:16.135927Z","shell.execute_reply":"2024-07-14T11:29:22.257138Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Ошибка при загрузке модели: Incorrect path_or_model_id: './final_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\n","output_type":"stream"}]},{"cell_type":"code","source":"def find_model_dir(start_path='.'):\n    for root, dirs, files in os.walk(start_path):\n        if 'config.json' in files and 'pytorch_model.bin' in files:\n            return root\n    return None\n\nmodel_dir = find_model_dir()\nif model_dir:\n    print(f\"Директория с моделью найдена: {model_dir}\")\nelse:\n    print(\"Директория с моделью не найдена\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:32:20.810823Z","iopub.execute_input":"2024-07-14T11:32:20.811423Z","iopub.status.idle":"2024-07-14T11:32:20.823929Z","shell.execute_reply.started":"2024-07-14T11:32:20.811382Z","shell.execute_reply":"2024-07-14T11:32:20.822419Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Директория с моделью не найдена\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nif os.path.exists('./final_model'):\n    print(\"Директория ./final_model существует\")\n    print(\"Содержимое директории:\")\n    print(os.listdir('./final_model'))\nelse:\n    print(\"Директория ./final_model не существует\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:30:02.952800Z","iopub.execute_input":"2024-07-14T11:30:02.953514Z","iopub.status.idle":"2024-07-14T11:30:02.961671Z","shell.execute_reply.started":"2024-07-14T11:30:02.953475Z","shell.execute_reply":"2024-07-14T11:30:02.960137Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Директория ./final_model не существует\n","output_type":"stream"}]},{"cell_type":"code","source":"# Функция для предсказания оценок для новых отзывов\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    new_data = pd.DataFrame({'combined_text': reviews})\n    new_data['processed_text'] = new_data['combined_text'].apply(preprocess_text)\n    new_dataset = Dataset.from_pandas(new_data)\n    \n    tokenized_new_data = new_dataset.map(tokenize_function, batched=True, remove_columns=['combined_text'])\n    \n    predictions = best_trainer.predict(tokenized_new_data).predictions\n    if len(predictions.shape) == 1:\n        predicted_ratings = np.argmax(predictions.reshape(1, -1), axis=1) + 1\n    else:\n        predicted_ratings = np.argmax(predictions, axis=1) + 1\n        \n    return predicted_ratings.tolist()\n\n# Функция для ввода отзывов с клавиатуры\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"Введите отзыв (или нажмите Enter для завершения): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# Вызываем функцию для ввода отзывов с клавиатуры\nnew_reviews = input_reviews()\n\n# Получаем предсказанные оценки для введенных отзывов\npredicted_ratings = predict_ratings(new_reviews)\n\n# Выводим результаты предсказаний\nif len(predicted_ratings) == 0:\n    print(\"Нет введенных отзывов.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"Отзыв: {review}\")\n        print(f\"Предсказанная оценка: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T10:59:29.578185Z","iopub.execute_input":"2024-07-14T10:59:29.578887Z","iopub.status.idle":"2024-07-14T11:01:04.624939Z","shell.execute_reply.started":"2024-07-14T10:59:29.578854Z","shell.execute_reply":"2024-07-14T11:01:04.623673Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdin","text":"Введите отзыв (или нажмите Enter для завершения):  Нам очень понравился креативный, но одновременно рациональный подход к задаче, умение выслушать клиента и понять потребности целевой аудитории. Отмечаем высокий профессионализм сотрудников компании “KozhinDev”, качество и сжатые сроки выполнения проекта.\nВведите отзыв (или нажмите Enter для завершения):  Глубокое погружение в проект, четкое видение и полное понимание ТЗ, а также задач, которые обсуждали на установочной сессии. Разработка заняла 3 месяца, как и планировалось, а дальше пошли докрутки и дополнения. Сотрудничаем с 2019 года и по сей день.\nВведите отзыв (или нажмите Enter для завершения):  Мы заказывали у студии KozhinDev разработку сайта и админ панель к нему. Работа сделана качественно, в срок. Менеджер всегда на связи. Правки в ТЗ обсуждались и вносились в проект быстро и без всяких проблем. Баги устраняли тоже очень быстро. Ребята очень контактные, готовы выслушать, понять, предложить варианты и реализовать.\nВведите отзыв (или нажмите Enter для завершения):  лучший сервис\nВведите отзыв (или нажмите Enter для завершения):  средний сервис\nВведите отзыв (или нажмите Enter для завершения):  ужасный сервис\nВведите отзыв (или нажмите Enter для завершения):  \n"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee62b4d5458a4f0d8b61d0ad04d51f89"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m new_reviews \u001b[38;5;241m=\u001b[39m input_reviews()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Получаем предсказанные оценки для введенных отзывов\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m predicted_ratings \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_ratings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_reviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Выводим результаты предсказаний\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predicted_ratings) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mpredict_ratings\u001b[0;34m(reviews)\u001b[0m\n\u001b[1;32m      8\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(new_data)\n\u001b[1;32m     10\u001b[0m tokenized_new_data \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mbest_trainer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(tokenized_new_data)\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     14\u001b[0m     predicted_ratings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'best_trainer' is not defined"],"ename":"NameError","evalue":"name 'best_trainer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T03:32:41.189003Z","iopub.execute_input":"2024-07-14T03:32:41.189464Z","iopub.status.idle":"2024-07-14T03:32:41.240673Z","shell.execute_reply.started":"2024-07-14T03:32:41.189434Z","shell.execute_reply":"2024-07-14T03:32:41.239660Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Функция для предсказания оценок для новых отзывов\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    # Создаем DataFrame из новых отзывов\n    new_data = pd.DataFrame({'combined_text': reviews})\n    new_data['processed_text'] = new_data['combined_text'].apply(preprocess_text)\n    new_dataset = Dataset.from_pandas(new_data)\n    \n    # Токенизируем новые отзывы\n    tokenized_new_data = new_dataset.map(tokenize_function, batched=True, remove_columns=['combined_text'])\n    \n    # Получаем предсказания модели для новых отзывов\n    predictions = trainer.predict(tokenized_new_data).predictions\n    \n    if len(predictions.shape) == 1:\n        predicted_ratings = np.argmax(predictions.reshape(1, -1), axis=1) + 1\n    else:\n        predicted_ratings = np.argmax(predictions, axis=1) + 1\n    \n    return predicted_ratings.tolist()\n\n# Функция для ввода отзывов с клавиатуры\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"Введите отзыв (или нажмите Enter для завершения): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# Вызываем функцию для ввода отзывов с клавиатуры\nnew_reviews = input_reviews()\n\n# Получаем предсказанные оценки для введенных отзывов\npredicted_ratings = predict_ratings(new_reviews)\n\n# Выводим результаты предсказаний\nif len(predicted_ratings) == 0:\n    print(\"Нет введенных отзывов.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"Отзыв: {review}\")\n        print(f\"Предсказанная оценка: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.138756Z","iopub.status.idle":"2024-07-12T20:22:24.139267Z","shell.execute_reply.started":"2024-07-12T20:22:24.138995Z","shell.execute_reply":"2024-07-12T20:22:24.139017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Нам очень понравился креативный, но одновременно рациональный подход к задаче, умение выслушать клиента и понять потребности целевой аудитории. Отмечаем высокий профессионализм сотрудников компании “KozhinDev”, качество и сжатые сроки выполнения проекта.\r\n\r\n\r\n\r\nГлубокое погружение в проект, четкое видение и полное понимание ТЗ, а также задач, которые обсуждали на установочной сессии. Разработка заняла 3 месяца, как и планировалось, а дальше пошли докрутки и дополнения. Сотрудничаем с 2019 года и по сей день.\r\n\r\n\r\n\r\nМы заказывали у студии KozhinDev разработку сайта и админ панель к нему. Работа сделана качественно, в срок. Менеджер всегда на связи. Правки в ТЗ обсуждались и вносились в проект быстро и без всяких проблем. Баги устраняли тоже очень быстро. Ребята очень контактные, готовы выслушать, понять, предложить варианты и реализовать.\r\n\r\n\r\n\r\nКоманда KozhinDev реализовала интерфейс для управления настройками нейронной сети, которая осуществляла расчет для решения бизнес задачи заказчика. Ребята очень умные, быстро поняли суть задачи и проактивно сработали, предложив лучшие способы решения. Проект был сдан точно в срок. Разработка существенно влияет на максимизацию нашей прибыли.\r\n\r\n\r\n\r\nРаботали с командой KozhinDev по проекту Инвойс‑бокс, ощущение от работы исключительно положительные. Выбрал KozhinDev как партнёров, потому что знаю подход Владимира к подбору людей, и что у него работают крутые специалисты. По итогу выбор себя оправдал, разработчик, который с нами сотрудничает, крутой специалист!\r\n\r\n\r\n\r\nРазрабатывали ПО, связанное с получением данных по api со стороннего сервиса. Ребята с самого начала полностью погрузились в проект, помогли в написании ТЗ. Чувствуется высокий уровень организованности команды. Будем и дальше плотно работать, улучшать свой продукт, наполнять новым функционалом.\r\n\r\n\r\n\r\nПоложительный опыт работы с KozhinDev: сильная команда разработки, всегда на связи. На встречах и обсуждениях проекта тимлид и менеджер проекта задавали правильные вопросы. Были готовы изучать что‑то дополнительно. Будем рады поработать ещё.\r\n\r\n\r\n\r\nКоманда KozhinDev разработала АИС для оптимизации бизнес‑процессов нашей приемной комиссии, осуществляла техническую поддержку операторов на протяжении всей приемной кампании. Проявили себя как профессионалы своего дела, готовые не только прийти на помощь клиенту, но и, разобравшись в тонкостях процесса и организационной структуры, выносить свои предложения по их оптимизации; Это отзывы, которые нужно оценить исходя из обученной модели","metadata":{}},{"cell_type":"code","source":"print(\"hello\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.140644Z","iopub.status.idle":"2024-07-12T20:22:24.140993Z","shell.execute_reply.started":"2024-07-12T20:22:24.140829Z","shell.execute_reply":"2024-07-12T20:22:24.140844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"hi\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.142067Z","iopub.status.idle":"2024-07-12T20:22:24.142435Z","shell.execute_reply.started":"2024-07-12T20:22:24.142260Z","shell.execute_reply":"2024-07-12T20:22:24.142277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\nimport torch\nfrom torch import nn\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom scipy.stats import spearmanr\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")\n\ndf = pd.read_csv('cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_10'] = df['rating'] * 2\n\ndef preprocess_text(text):\n    return text.lower().strip() if isinstance(text, str) else ''\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df.dropna(subset=['processed_text', 'rating_10'])\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(f\"Размер обучающей выборки: {len(train_df)}\")\nprint(f\"Размер тестовой выборки: {len(test_df)}\")\n\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\ntokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\n\ntokenized_train = tokenized_train.rename_column(\"rating_10\", \"labels\")\ntokenized_test = tokenized_test.rename_column(\"rating_10\", \"labels\")\n\nclass RegressionModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.1)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.regressor(pooled_output)\n        \n        loss = None\n        if labels is not None:\n            loss = nn.MSELoss()(logits.squeeze(), labels)\n        \n        return loss, logits.squeeze()\n\nmodel = RegressionModel('cointegrated/rubert-tiny')\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    mse = mean_squared_error(labels, predictions)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(labels, predictions)\n    r2 = r2_score(labels, predictions)\n    spearman_corr, _ = spearmanr(labels, predictions)\n    accuracy_1 = np.mean(np.abs(predictions - labels) <= 1)\n    \n    return {\n        \"mse\": mse,\n        \"rmse\": rmse,\n        \"mae\": mae,\n        \"r2\": r2,\n        \"spearman\": spearman_corr,\n        \"accuracy_1\": accuracy_1\n    }\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=32,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_rmse\",\n    greater_is_better=False,\n    gradient_accumulation_steps=4,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\neval_results = trainer.evaluate()\nprint(\"Результаты оценки:\", eval_results)\n\ntrainer.save_model(\"./final_model\")\n\npredictions = trainer.predict(tokenized_test).predictions\nactual = tokenized_test['labels']\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=actual, y=predictions, alpha=0.5)\nplt.plot([1, 10], [1, 10], 'r--')\nplt.xlabel('Фактические оценки')\nplt.ylabel('Предсказанные оценки')\nplt.title('Фактические vs Предсказанные оценки')\nplt.xlim(1, 10)\nplt.ylim(1, 10)\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.kdeplot(actual, shade=True, label='Фактические')\nsns.kdeplot(predictions, shade=True, label='Предсказанные')\nplt.xlabel('Оценка')\nplt.ylabel('Плотность')\nplt.title('Распределение фактических и предсказанных оценок')\nplt.legend()\nplt.show()\n\nprint(f\"RMSE: {eval_results['eval_rmse']:.4f}\")\nprint(f\"MAE: {eval_results['eval_mae']:.4f}\")\nprint(f\"R2: {eval_results['eval_r2']:.4f}\")\nprint(f\"Spearman correlation: {eval_results['eval_spearman']:.4f}\")\nprint(f\"Accuracy within 1 point: {eval_results['eval_accuracy_1']:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.144701Z","iopub.status.idle":"2024-07-12T20:22:24.145100Z","shell.execute_reply.started":"2024-07-12T20:22:24.144894Z","shell.execute_reply":"2024-07-12T20:22:24.144911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}