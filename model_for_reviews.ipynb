{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8938265,"sourceType":"datasetVersion","datasetId":5377911}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∞—è –æ—Ü–µ–Ω–∫—É –ø–æ –æ—Ç–∑—ã–≤—É","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\nimport torch\nfrom torch import nn\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\n\n# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏–¥ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\ndf = pd.read_csv('cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1  # –ö–ª–∞—Å—Å—ã –æ—Ç 0 –¥–æ 4\n\ndef preprocess_text(text):\n    return text.lower().strip() if isinstance(text, str) else ''\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df.dropna(subset=['processed_text', 'rating_class'])\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\nprint(f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_df)}\")\nprint(f\"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(test_df)}\")\n\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=256)\n\ntokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\ntokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\n\ntokenized_train = tokenized_train.rename_column(\"rating_class\", \"labels\")\ntokenized_test = tokenized_test.rename_column(\"rating_class\", \"labels\")\n\nnum_labels = df['rating_class'].nunique()\n\nclass ClassificationModel(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        \n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n        \n        return (loss, logits) if loss is not None else logits\n\nmodel = ClassificationModel('cointegrated/rubert-tiny', num_labels)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    spearman_corr, _ = spearmanr(labels, preds)\n\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"spearman\": spearman_corr\n    }\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    warmup_steps=500,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    gradient_accumulation_steps=2,\n    fp16=True if torch.cuda.is_available() else False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\neval_results = trainer.evaluate()\nprint(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:\", eval_results)\n\ntrainer.save_model(\"./final_model\")\n\npredictions = trainer.predict(tokenized_test).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = tokenized_test['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\nprint(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\nprint(f\"F1 (weighted): {eval_results['eval_f1']:.4f}\") \nprint(f\"Precision (weighted): {eval_results['eval_precision']:.4f}\")\nprint(f\"Recall (weighted): {eval_results['eval_recall']:.4f}\")\nprint(f\"Spearman correlation: {eval_results['eval_spearman']:.4f}\")\n\n# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\n# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=num_labels, kde=True)\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:23.546286Z","iopub.execute_input":"2024-07-12T20:22:23.546836Z","iopub.status.idle":"2024-07-12T20:22:24.116033Z","shell.execute_reply.started":"2024-07-12T20:22:23.546786Z","shell.execute_reply":"2024-07-12T20:22:24.113471Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_kaspi_reviews.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrussian\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating_class\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# –ö–ª–∞—Å—Å—ã –æ—Ç 0 –¥–æ 4\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleaned_kaspi_reviews.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'cleaned_kaspi_reviews.csv'","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nimport pymorphy2\nimport re\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏–¥ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1  # –ö–ª–∞—Å—Å—ã –æ—Ç 0 –¥–æ 4\n\n# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ pymorphy2\nmorph = pymorphy2.MorphAnalyzer()\n\n# –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)\nstop_words = [\n    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫',\n    '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ',\n    '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥',\n    '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂',\n    '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å',\n    '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ',\n    '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ',\n    '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å',\n    '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å',\n    '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ',\n    '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º',\n    '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'\n]\n\ndef preprocess_text(text):\n    if pd.isna(text):\n        return ''\n    if not isinstance(text, str):\n        text = str(text)\n    \n    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n    text = text.lower().strip()\n    \n    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ü–∏—Ñ—Ä\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text)\n    \n    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n    words = text.split()\n    lemmatized_words = [morph.parse(word)[0].normal_form for word in words]\n    \n    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n    filtered_words = [word for word in lemmatized_words if word not in stop_words]\n    \n    return ' '.join(filtered_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']]\ndf = df.dropna()\n\ntokenizer = AutoTokenizer.from_pretrained('sberbank-ai/ruRoBERTa-large')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=512)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    spearman_corr, _ = spearmanr(labels, preds)\n\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"spearman\": spearman_corr  \n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    # –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n    lr = trial.suggest_loguniform('lr', 1e-6, 1e-4)\n    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 1000)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [4, 8, 16])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('sberbank-ai/ruRoBERTa-large', num_labels=5)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=5,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=16,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    return eval_results[\"eval_f1\"]\n\n# –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è\nn_splits = 5\nkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfinal_predictions = []\nall_actual_classes = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, df['rating_class']), 1):\n    print(f\"Fold {fold}\")\n    \n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n    \n    train_dataset = Dataset.from_pandas(train_df)\n    val_dataset = Dataset.from_pandas(val_df)\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n\n    tokenized_train = tokenized_train.rename_column(\"rating_class\", \"labels\")\n    tokenized_val = tokenized_val.rename_column(\"rating_class\", \"labels\")\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(lambda trial: objective(trial, tokenized_train, tokenized_val), n_trials=20)\n\n    print(f\"Fold {fold} - Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: \", trial.value)\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n\n    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n    best_model = AutoModelForSequenceClassification.from_pretrained('sberbank-ai/ruRoBERTa-large', num_labels=5)\n    \n    best_training_args = TrainingArguments(\n        output_dir=f\"./results/fold{fold}\",\n        num_train_epochs=10,\n        per_device_train_batch_size=trial.params['per_device_train_batch_size'],\n        per_device_eval_batch_size=16,\n        learning_rate=trial.params['lr'],\n        weight_decay=trial.params['weight_decay'],\n        warmup_steps=trial.params['warmup_steps'],\n        logging_dir=f'./logs/fold{fold}',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n        fp16=True,\n    )\n\n    best_trainer = Trainer(\n        model=best_model,\n        args=best_training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics\n    )\n\n    best_trainer.train()\n\n    eval_results = best_trainer.evaluate()\n    print(f\"Fold {fold} Evaluation Results:\")\n    print(eval_results)\n    \n    best_trainer.save_model(f\"./final_model/fold{fold}\")\n\n    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ñ–æ–ª–¥–∞\n    predictions = best_trainer.predict(tokenized_val).predictions\n    final_predictions.append(predictions)\n    all_actual_classes.extend(val_df['rating_class'])\n\nprint(\"Training completed. Models saved in ./final_model/ directory\")\n\n# –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π\nfinal_predictions = np.mean(final_predictions, axis=0)\npredicted_classes = np.argmax(final_predictions, axis=1)\nactual_classes = np.array(all_actual_classes)\n\n# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\n# –ú–µ—Ç—Ä–∏–∫–∏ \nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\") \nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\n# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.117126Z","iopub.status.idle":"2024-07-12T20:22:24.117542Z","shell.execute_reply.started":"2024-07-12T20:22:24.117348Z","shell.execute_reply":"2024-07-12T20:22:24.117366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nfrom optuna.samplers import TPESampler\nimport pymorphy2\nimport re\nfrom functools import partial, lru_cache\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_memory / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\nstop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\n@lru_cache(maxsize=None)\ndef lemmatize(word):\n    return morph.parse(word)[0].normal_form\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n    return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\ndel df\ngc.collect()\n\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], truncation=True, max_length=256)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [32, 64, 128])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay, \n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"no\", \n        metric_for_best_model=\"f1\",\n        greater_is_better=True, \n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=4,\n        optim=\"adamw_torch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\ntrain_dataset = Dataset.from_pandas(train_df, preserve_index=False)\nval_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\ndel train_df, val_df\ngc.collect()\n\nencoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\nencoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\nencoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\nencoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=3)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_steps=trial.params['warmup_steps'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=4,\n    optim=\"adamw_torch\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()  \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:11:13.118393Z","iopub.execute_input":"2024-07-13T10:11:13.118781Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\nGPU: Tesla P100-PCIE-16GB, –û–±—â–∞—è –ø–∞–º—è—Ç—å: 17.06 GB\n–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: 16.79 GB\n–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9bee2181e754a8b9ff54e223de9d1c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1ce9883d688427b89d2be254f2a3896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e35a4278628a4db4825bce77bbac11e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b34aad0781a340a9bc857552909ebaa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1147fea67d0149f8865f863f7d98d578"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"506aca67d70142838ee69c36c14924a9"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-13 10:11:34,127] A new study created in memory with name: no-name-0078b2db-d9be-44d0-aea0-e4a7c0686614\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4208756fd7447b58f2c8e2b3b7a54c8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='194' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 194/2100 05:06 < 50:38, 0.63 it/s, Epoch 0.28/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nimport pymorphy2\nimport re\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏–¥ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\n# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ GPU\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu.total_memory / 1e9:.2f} GB\")\n    print(f\"–î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1  # –ö–ª–∞—Å—Å—ã –æ—Ç 0 –¥–æ 4\n\n# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ pymorphy2\nmorph = pymorphy2.MorphAnalyzer()\n\n# –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å)\nstop_words = [\n    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫',\n    # ... (–æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞)\n]\n\ndef preprocess_text(text):\n    if pd.isna(text):\n        return ''\n    if not isinstance(text, str):\n        text = str(text)\n    \n    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n    text = text.lower().strip()\n    \n    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ü–∏—Ñ—Ä\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text)\n    \n    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n    words = text.split()\n    lemmatized_words = [morph.parse(word)[0].normal_form for word in words]\n    \n    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n    filtered_words = [word for word in lemmatized_words if word not in stop_words]\n    \n    return ' '.join(filtered_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']]\ndf = df.dropna()\n\n# –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å DeepPavlov/rubert-base-cased\nmodel_name = 'DeepPavlov/rubert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=256)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    spearman_corr, _ = spearmanr(labels, preds)\n\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"spearman\": spearman_corr  \n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    # –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n    lr = trial.suggest_float('lr', 1e-6, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 1000)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [4, 8])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [4, 8])\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=8,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    return eval_results[\"eval_f1\"]\n\n# –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è\nn_splits = 5\nkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfinal_predictions = []\nall_actual_classes = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, df['rating_class']), 1):\n    print(f\"Fold {fold}\")\n    \n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n    \n    train_dataset = Dataset.from_pandas(train_df)\n    val_dataset = Dataset.from_pandas(val_df)\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n\n    tokenized_train = tokenized_train.rename_column(\"rating_class\", \"labels\")\n    tokenized_val = tokenized_val.rename_column(\"rating_class\", \"labels\")\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(lambda trial: objective(trial, tokenized_train, tokenized_val), n_trials=20)\n\n    print(f\"Fold {fold} - Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: \", trial.value)\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n\n    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n    best_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n    \n    best_training_args = TrainingArguments(\n        output_dir=f\"./results/fold{fold}\",\n        num_train_epochs=5,\n        per_device_train_batch_size=trial.params['per_device_train_batch_size'],\n        per_device_eval_batch_size=8,\n        learning_rate=trial.params['lr'],\n        weight_decay=trial.params['weight_decay'],\n        warmup_steps=trial.params['warmup_steps'],\n        logging_dir=f'./logs/fold{fold}',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n        fp16=True,\n    )\n\n    best_trainer = Trainer(\n        model=best_model,\n        args=best_training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics\n    )\n\n    best_trainer.train()\n\n    eval_results = best_trainer.evaluate()\n    print(f\"Fold {fold} Evaluation Results:\")\n    print(eval_results)\n    \n    best_trainer.save_model(f\"./final_model/fold{fold}\")\n\n    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ñ–æ–ª–¥–∞\n    predictions = best_trainer.predict(tokenized_val).predictions\n    final_predictions.append(predictions)\n    all_actual_classes.extend(val_df['rating_class'])\n\nprint(\"Training completed. Models saved in ./final_model/ directory\")\n\n# –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π\nfinal_predictions = np.mean(final_predictions, axis=0)\npredicted_classes = np.argmax(final_predictions, axis=1)\nactual_classes = np.array(all_actual_classes)\n\n# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\n# –ú–µ—Ç—Ä–∏–∫–∏ \nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\") \nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\n# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.119383Z","iopub.status.idle":"2024-07-12T20:22:24.119790Z","shell.execute_reply.started":"2024-07-12T20:22:24.119611Z","shell.execute_reply":"2024-07-12T20:22:24.119627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2024-07-12T21:14:35.417278Z","iopub.execute_input":"2024-07-12T21:14:35.417618Z","iopub.status.idle":"2024-07-12T21:14:35.428445Z","shell.execute_reply.started":"2024-07-12T21:14:35.417591Z","shell.execute_reply":"2024-07-12T21:14:35.427534Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T21:14:37.714636Z","iopub.execute_input":"2024-07-12T21:14:37.715575Z","iopub.status.idle":"2024-07-12T21:14:37.756988Z","shell.execute_reply.started":"2024-07-12T21:14:37.715541Z","shell.execute_reply":"2024-07-12T21:14:37.755794Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nimport pymorphy2\nimport re\nfrom functools import partial\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu.total_memory / 1e9:.2f} GB\")\n    print(f\"–î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\n\nstop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n    words = text.split()\n    lemmatized_words = [morph.parse(word)[0].normal_form for word in words if word not in stop_words]\n    return ' '.join(lemmatized_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\nmodel_name = 'DeepPavlov/rubert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'),\n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [16, 32])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    return eval_results[\"eval_f1\"]\n\nn_splits = 3  # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤\nkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfinal_predictions = []\nall_actual_classes = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, df['rating_class']), 1):\n    print(f\"Fold {fold}\")\n    \n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n    \n    train_dataset = Dataset.from_pandas(train_df)\n    val_dataset = Dataset.from_pandas(val_df)\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n\n    tokenized_train = tokenized_train.rename_column(\"rating_class\", \"labels\")\n    tokenized_val = tokenized_val.rename_column(\"rating_class\", \"labels\")\n\n    study = optuna.create_study(direction=\"maximize\")\n    objective_with_dataset = partial(objective, train_dataset=tokenized_train, val_dataset=tokenized_val)\n    study.optimize(objective_with_dataset, n_trials=10)  # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ trials\n\n    print(f\"Fold {fold} - Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: \", trial.value)\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n\n    best_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n    \n    best_training_args = TrainingArguments(\n        output_dir=f\"./results/fold{fold}\",\n        num_train_epochs=5,\n        per_device_train_batch_size=trial.params['per_device_train_batch_size'],\n        per_device_eval_batch_size=64,\n        learning_rate=trial.params['lr'],\n        weight_decay=trial.params['weight_decay'],\n        warmup_steps=trial.params['warmup_steps'],\n        logging_dir=f'./logs/fold{fold}',\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n        fp16=True,\n    )\n\n    best_trainer = Trainer(\n        model=best_model,\n        args=best_training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics\n    )\n\n    best_trainer.train()\n\n    eval_results = best_trainer.evaluate()\n    print(f\"Fold {fold} Evaluation Results:\")\n    print(eval_results)\n    \n    best_trainer.save_model(f\"./final_model/fold{fold}\")\n\n    predictions = best_trainer.predict(tokenized_val).predictions\n    final_predictions.append(predictions)\n    all_actual_classes.extend(val_df['rating_class'])\n\nprint(\"Training completed. Models saved in ./final_model/ directory\")\n\nfinal_predictions = np.mean(final_predictions, axis=0)\npredicted_classes = np.argmax(final_predictions, axis=1)\nactual_classes = np.array(all_actual_classes)\n\n# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\n# –ú–µ—Ç—Ä–∏–∫–∏ \nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\") \nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\n# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.125463Z","iopub.status.idle":"2024-07-12T20:22:24.125994Z","shell.execute_reply.started":"2024-07-12T20:22:24.125731Z","shell.execute_reply":"2024-07-12T20:22:24.125755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AdamW\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:24:33.946411Z","iopub.execute_input":"2024-07-12T20:24:33.947106Z","iopub.status.idle":"2024-07-12T20:24:52.342538Z","shell.execute_reply.started":"2024-07-12T20:24:33.947072Z","shell.execute_reply":"2024-07-12T20:24:52.341227Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-12 20:24:41.748966: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-12 20:24:41.749073: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-12 20:24:41.903000: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nimport pymorphy2\nimport re\nfrom functools import partial\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_memory / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\nstop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n    lemmatized_words = (morph.parse(word)[0].normal_form for word in text.split() if word not in stop_words)\n    return ' '.join(lemmatized_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\ndel df\ngc.collect()\n\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], truncation=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [16, 32, 64])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [4, 8, 16])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay, \n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\", \n        metric_for_best_model=\"f1\",\n        greater_is_better=True, \n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=4,\n        optim=\"adafactor\"  \n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\ntrain_dataset = Dataset.from_pandas(train_df, preserve_index=False)\nval_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\ndel train_df, val_df\ngc.collect()\n\nencoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\nencoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\nencoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\nencoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=3)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_steps=trial.params['warmup_steps'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=50,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=4,\n    optim=\"adafactor\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\n# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\n# –ú–µ—Ç—Ä–∏–∫–∏\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è \nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\n# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()  \ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pymorphy2\nimport re\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:25:30.731513Z","iopub.execute_input":"2024-07-12T20:25:30.732331Z","iopub.status.idle":"2024-07-12T20:25:30.755729Z","shell.execute_reply.started":"2024-07-12T20:25:30.732281Z","shell.execute_reply":"2024-07-12T20:25:30.754980Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:04:20.466877Z","iopub.execute_input":"2024-07-13T10:04:20.467311Z","iopub.status.idle":"2024-07-13T10:04:20.473107Z","shell.execute_reply.started":"2024-07-13T10:04:20.467266Z","shell.execute_reply":"2024-07-13T10:04:20.471728Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:04:24.156095Z","iopub.execute_input":"2024-07-13T10:04:24.156523Z","iopub.status.idle":"2024-07-13T10:04:24.229662Z","shell.execute_reply.started":"2024-07-13T10:04:24.156489Z","shell.execute_reply":"2024-07-13T10:04:24.228471Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nfrom optuna.samplers import TPESampler\nimport pymorphy2\nimport re\nfrom functools import partial, lru_cache\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_memory / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\nstop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\n@lru_cache(maxsize=None)\ndef lemmatize(word):\n    return morph.parse(word)[0].normal_form\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n    return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\ndel df\ngc.collect()\n\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], truncation=True, max_length=256)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [32, 64, 128])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay, \n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"no\", \n        metric_for_best_model=\"f1\",\n        greater_is_better=True, \n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=4,\n        optim=\"adamw_torch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\ntrain_dataset = Dataset.from_pandas(train_df, preserve_index=False)\nval_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\ndel train_df, val_df\ngc.collect()\n\nencoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\nencoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\nencoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\nencoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=3)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_steps=trial.params['warmup_steps'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=4,\n    optim=\"adamw_torch\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()  \ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_memory / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\nstop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n    lemmatized_words = [morph.parse(word)[0].normal_form for word in text.split() if word not in stop_words]\n    return ' '.join(lemmatized_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\n# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ transformers\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [32, 64])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=128,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\", \n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args, \n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    return eval_results[\"eval_f1\"]\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\nencoded_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\nencoded_val = val_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text'])\n\nencoded_train.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'rating_class'])\nencoded_val.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'rating_class'])\n\nencoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\nencoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\nstudy = optuna.create_study(direction=\"maximize\")\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=5)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=128,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_steps=trial.params['warmup_steps'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=50,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True, \n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'], \n    fp16=True,\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = val_df['rating_class']\n\n# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n# ...\n\n# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\n# –ú–µ—Ç—Ä–∏–∫–∏\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è \nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\n# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:25:36.262452Z","iopub.execute_input":"2024-07-12T20:25:36.262806Z","iopub.status.idle":"2024-07-12T20:50:27.126179Z","shell.execute_reply.started":"2024-07-12T20:25:36.262779Z","shell.execute_reply":"2024-07-12T20:50:27.124584Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\nGPU: Tesla P100-PCIE-16GB, –û–±—â–∞—è –ø–∞–º—è—Ç—å: 17.06 GB\n–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: 16.79 GB\n–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"586b671b91814dedb64916d637579b0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"371cdd66845949d6816f27732c7d1b50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015fea03d0d44d6fac5b8dae9531a044"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"290edd9007d547f5a5cf9d8dca3131b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fd96e32ec7342c394d34b0546616c6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746061bd88824580b3810f88f158d914"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-12 20:31:41,868] A new study created in memory with name: no-name-c6354f65-a6d2-47dc-9015-f8d5a751b11f\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b448de7844454396b31ae505d97b22a6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='380' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 380/1050 18:26 < 32:41, 0.34 it/s, Epoch 1.08/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.440700</td>\n      <td>0.427637</td>\n      <td>0.855288</td>\n      <td>0.802617</td>\n      <td>0.802198</td>\n      <td>0.855288</td>\n      <td>0.442637</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n[W 2024-07-12 20:50:25,650] Trial 0 failed with parameters: {'lr': 1.629174368249374e-05, 'weight_decay': 0.00012897438760911791, 'warmup_steps': 173, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 4} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n    value_or_values = func(trial)\n  File \"/tmp/ipykernel_35/4289091445.py\", line 92, in objective\n    trainer.train()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1932, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2268, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3324, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2147, in backward\n    self.scaler.scale(loss).backward(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\n    torch.autograd.backward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nKeyboardInterrupt\n[W 2024-07-12 20:50:25,662] Trial 0 failed with value None.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 111\u001b[0m\n\u001b[1;32m    109\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m objective_with_dataset \u001b[38;5;241m=\u001b[39m partial(objective, train_dataset\u001b[38;5;241m=\u001b[39mencoded_train, val_dataset\u001b[38;5;241m=\u001b[39mencoded_val)\n\u001b[0;32m--> 111\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_with_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n","Cell \u001b[0;32mIn[6], line 92\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m     65\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     66\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/trial_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;241m.\u001b[39mnumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     84\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     85\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     86\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     90\u001b[0m )\n\u001b[0;32m---> 92\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eval_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3324\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3322\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3324\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2147\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2148\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T21:15:12.192455Z","iopub.execute_input":"2024-07-12T21:15:12.192821Z","iopub.status.idle":"2024-07-12T21:15:12.519708Z","shell.execute_reply.started":"2024-07-12T21:15:12.192793Z","shell.execute_reply":"2024-07-12T21:15:12.518423Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-07-13T02:32:18.109623Z","iopub.execute_input":"2024-07-13T02:32:18.110047Z","iopub.status.idle":"2024-07-13T02:32:18.518474Z","shell.execute_reply.started":"2024-07-13T02:32:18.110016Z","shell.execute_reply":"2024-07-13T02:32:18.517279Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(f\"Accuracy: {acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:04:11.698482Z","iopub.execute_input":"2024-07-13T08:04:11.699567Z","iopub.status.idle":"2024-07-13T08:04:12.139502Z","shell.execute_reply.started":"2024-07-13T08:04:11.699504Z","shell.execute_reply":"2024-07-13T08:04:12.137858Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43macc\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'acc' is not defined"],"ename":"NameError","evalue":"name 'acc' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nimport pymorphy2\nimport re\nfrom functools import partial\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_memory / 1e9:.2f} GB\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1\n\nmorph = pymorphy2.MorphAnalyzer()\nstop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n    lemmatized_words = (morph.parse(word)[0].normal_form for word in text.split() if word not in stop_words)\n    return ' '.join(lemmatized_words)\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df[['processed_text', 'rating_class']].dropna()\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\ndel df\ngc.collect()\n\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], truncation=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_steps = trial.suggest_int('warmup_steps', 100, 500)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [16, 32, 64])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [4, 8, 16])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay, \n        warmup_steps=warmup_steps,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\", \n        metric_for_best_model=\"f1\",\n        greater_is_better=True, \n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=4,\n        optim=\"adafactor\"  \n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\ntrain_dataset = Dataset.from_pandas(train_df, preserve_index=False)\nval_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\ndel train_df, val_df\ngc.collect()\n\nencoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\nencoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\nencoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\nencoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=3)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5, use_cache=False).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_steps=trial.params['warmup_steps'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=50,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=4,\n    optim=\"adafactor\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\n# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\n# –ú–µ—Ç—Ä–∏–∫–∏\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\n# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è \nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\n# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()  \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T21:15:22.809588Z","iopub.execute_input":"2024-07-12T21:15:22.809922Z","iopub.status.idle":"2024-07-13T01:12:41.567640Z","shell.execute_reply.started":"2024-07-12T21:15:22.809897Z","shell.execute_reply":"2024-07-13T01:12:41.566578Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-12 21:15:30.369234: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-12 21:15:30.369344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-12 21:15:30.495830: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\nGPU: Tesla P100-PCIE-16GB, –û–±—â–∞—è –ø–∞–º—è—Ç—å: 17.06 GB\n–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: 16.79 GB\n–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a354c41c620496794bd121bb2c0b644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05ba4972301141cfa20b23c9ff41ffb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce86c01c0324a41bafadeac3b5e74b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c38e623b0d444cb1bd597a5eaa98f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba76674914b842618b214d79e4feec25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb3fcaafaabc4fef8d944734de25f36d"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-12 21:21:36,219] A new study created in memory with name: no-name-90add2d3-c025-42c5-9b35-c313bc53a0fe\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0a4e460e58b4192a50a48059f8a2ca1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1050/1050 42:11, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.468300</td>\n      <td>0.436497</td>\n      <td>0.853458</td>\n      <td>0.799995</td>\n      <td>0.786820</td>\n      <td>0.853458</td>\n      <td>0.400768</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.415900</td>\n      <td>0.406833</td>\n      <td>0.862115</td>\n      <td>0.836004</td>\n      <td>0.821805</td>\n      <td>0.862115</td>\n      <td>0.548216</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.386700</td>\n      <td>0.402884</td>\n      <td>0.863498</td>\n      <td>0.835314</td>\n      <td>0.823301</td>\n      <td>0.863498</td>\n      <td>0.548569</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nTOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-12 22:05:42,314] Trial 0 finished with value: 0.8353135114785433 and parameters: {'lr': 1.5908689043323806e-05, 'weight_decay': 0.0005547721817754297, 'warmup_steps': 387, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 16}. Best is trial 0 with value: 0.8353135114785433.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4200' max='4200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4200/4200 45:36, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.439400</td>\n      <td>0.403840</td>\n      <td>0.862561</td>\n      <td>0.830476</td>\n      <td>0.818269</td>\n      <td>0.862561</td>\n      <td>0.515236</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.382900</td>\n      <td>0.395511</td>\n      <td>0.864480</td>\n      <td>0.845506</td>\n      <td>0.840182</td>\n      <td>0.864480</td>\n      <td>0.578726</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.310500</td>\n      <td>0.408704</td>\n      <td>0.862829</td>\n      <td>0.846218</td>\n      <td>0.836953</td>\n      <td>0.862829</td>\n      <td>0.586902</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-12 22:52:51,305] Trial 1 finished with value: 0.8462179470344828 and parameters: {'lr': 2.6687655611407676e-05, 'weight_decay': 0.00010165203968063754, 'warmup_steps': 168, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 4}. Best is trial 1 with value: 0.8462179470344828.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [525/525 59:11, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.481900</td>\n      <td>0.460531</td>\n      <td>0.849041</td>\n      <td>0.784844</td>\n      <td>0.785110</td>\n      <td>0.849041</td>\n      <td>0.224224</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.425100</td>\n      <td>0.409776</td>\n      <td>0.860107</td>\n      <td>0.837775</td>\n      <td>0.825257</td>\n      <td>0.860107</td>\n      <td>0.563943</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.377700</td>\n      <td>0.397582</td>\n      <td>0.864257</td>\n      <td>0.840269</td>\n      <td>0.832418</td>\n      <td>0.864257</td>\n      <td>0.568114</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-12 23:53:40,635] Trial 2 finished with value: 0.8402689777374475 and parameters: {'lr': 5.310870371040237e-05, 'weight_decay': 0.001937599363616159, 'warmup_steps': 261, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 8}. Best is trial 1 with value: 0.8462179470344828.\n","output_type":"stream"},{"name":"stdout","text":"Best trial:\n  Value:  0.8462179470344828\n  Params: \n    lr: 2.6687655611407676e-05\n    weight_decay: 0.00010165203968063754\n    warmup_steps: 168\n    per_device_train_batch_size: 16\n    gradient_accumulation_steps: 4\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7000' max='7000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7000/7000 1:15:55, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.443300</td>\n      <td>0.404488</td>\n      <td>0.863320</td>\n      <td>0.829794</td>\n      <td>0.818031</td>\n      <td>0.863320</td>\n      <td>0.512466</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.388700</td>\n      <td>0.397359</td>\n      <td>0.862874</td>\n      <td>0.844235</td>\n      <td>0.832823</td>\n      <td>0.862874</td>\n      <td>0.576126</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.321400</td>\n      <td>0.408967</td>\n      <td>0.859349</td>\n      <td>0.845811</td>\n      <td>0.841690</td>\n      <td>0.859349</td>\n      <td>0.587751</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.252400</td>\n      <td>0.473034</td>\n      <td>0.855689</td>\n      <td>0.846079</td>\n      <td>0.838661</td>\n      <td>0.855689</td>\n      <td>0.584375</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Final Evaluation Results:\n{'eval_loss': 0.4423908293247223, 'eval_accuracy': 0.8592592592592593, 'eval_f1': 0.8473316495318277, 'eval_precision': 0.8390448072429096, 'eval_recall': 0.8592592592592593, 'eval_spearman': 0.588081419589789, 'eval_runtime': 89.3509, 'eval_samples_per_second': 250.809, 'eval_steps_per_second': 3.928, 'epoch': 4.997322862752097}\nTraining completed. Model saved in ./final_model/ directory\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"Accuracy: 0.8593\nF1 (weighted): 0.8473\nPrecision (weighted): 0.8390\nRecall (weighted): 0.8593\nSpearman correlation: 0.5881\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1765: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  order = pd.unique(vector)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"12059"},"metadata":{}}]},{"cell_type":"code","source":"# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\n    new_data = pd.DataFrame({'combined_text': reviews})\n    new_data['processed_text'] = new_data['combined_text'].apply(preprocess_text)\n    new_dataset = Dataset.from_pandas(new_data)\n    \n    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –æ—Ç–∑—ã–≤—ã\n    tokenized_new_data = new_dataset.map(tokenize_function, batched=True, remove_columns=['combined_text'])\n    \n    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\n    predictions = trainer.predict(tokenized_new_data).predictions\n    \n    if len(predictions.shape) == 1:\n        predicted_ratings = np.argmax(predictions.reshape(1, -1), axis=1) + 1\n    else:\n        predicted_ratings = np.argmax(predictions, axis=1) + 1\n    \n    return predicted_ratings.tolist()\n\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\nnew_reviews = input_reviews()\n\n# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\npredicted_ratings = predict_ratings(new_reviews)\n\n# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nif len(predicted_ratings) == 0:\n    print(\"–ù–µ—Ç –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"–û—Ç–∑—ã–≤: {review}\")\n        print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T02:32:50.288418Z","iopub.execute_input":"2024-07-13T02:32:50.289049Z","iopub.status.idle":"2024-07-13T02:32:53.402612Z","shell.execute_reply.started":"2024-07-13T02:32:50.289015Z","shell.execute_reply":"2024-07-13T02:32:53.401221Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdin","text":"–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  —ã–≤\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  \n"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m new_reviews \u001b[38;5;241m=\u001b[39m input_reviews()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m predicted_ratings \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_ratings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_reviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predicted_ratings) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mpredict_ratings\u001b[0;34m(reviews)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\u001b[39;00m\n\u001b[1;32m      7\u001b[0m new_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m: reviews})\n\u001b[0;32m----> 8\u001b[0m new_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m new_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mpreprocess_text\u001b[49m)\n\u001b[1;32m      9\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(new_data)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –æ—Ç–∑—ã–≤—ã\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'preprocess_text' is not defined"],"ename":"NameError","evalue":"name 'preprocess_text' is not defined","output_type":"error"}]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:39.668093Z","iopub.execute_input":"2024-07-12T20:22:39.668866Z","iopub.status.idle":"2024-07-12T20:22:39.710041Z","shell.execute_reply.started":"2024-07-12T20:22:39.668828Z","shell.execute_reply":"2024-07-12T20:22:39.708597Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n","\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"],"ename":"NameError","evalue":"name 'gc' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset, load_from_disk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nfrom optuna.samplers import TPESampler\nimport pymorphy2\nimport re\nfrom functools import partial, lru_cache\nimport gc\nfrom transformers import get_linear_schedule_with_warmup\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_memory / 1e9:.2f} GB\")\n\n# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\nif os.path.exists('./processed_data'):\n    encoded_train = load_from_disk('./processed_data/train')\n    encoded_val = load_from_disk('./processed_data/val')\n    print(\"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\")\nelse:\n    df = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\n    df = df[df['language'] == 'russian']\n    df['rating_class'] = df['rating'].astype(int) - 1\n\n    morph = pymorphy2.MorphAnalyzer()\n    stop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\n    @lru_cache(maxsize=None)\n    def lemmatize(word):\n        return morph.parse(word)[0].normal_form\n\n    def preprocess_text(text):\n        if pd.isna(text) or not isinstance(text, str):\n            return ''\n        text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n        return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\n    df['processed_text'] = df['combined_text'].apply(preprocess_text)\n    df = df[['processed_text', 'rating_class']].dropna()\n\n    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\n    del df\n    gc.collect()\n\n    tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=512, use_fast=True)\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"processed_text\"], truncation=True, max_length=512)\n\n    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\n    del train_df, val_df\n    gc.collect()\n\n    encoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n    encoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\n    encoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\n    encoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\n    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n    encoded_train.save_to_disk('./processed_data/train')\n    encoded_val.save_to_disk('./processed_data/val')\n    print(\"–î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'), \n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n    warmup_ratio = trial.suggest_float('warmup_ratio', 0.1, 0.3)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [16, 32])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [2, 4, 8])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=5,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay, \n        warmup_ratio=warmup_ratio,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"no\", \n        metric_for_best_model=\"f1\",\n        greater_is_better=True, \n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=4,\n        optim=\"adamw_torch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset, \n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=10)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=10,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_ratio=trial.params['warmup_ratio'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=4,\n    optim=\"adamw_torch\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)  \nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()  \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:31:34.035690Z","iopub.execute_input":"2024-07-13T10:31:34.036072Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\nGPU: Tesla P100-PCIE-16GB, –û–±—â–∞—è –ø–∞–º—è—Ç—å: 17.06 GB\n–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: 16.79 GB\n–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a0ed2b742ab4d61846001bdddc7fc69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0e12b6a6af04fd1a0466c09b46034ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f8ab19220b3490c804d18f6dcda877f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c1d8dc2c4346068609c92921b5604e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf905e8b46324eff8582df6d4991fbeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f7f74698024d25a3f5cb45f15ad0c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e74befb903c444c87013e927d540de8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ebe98b71ca3408689949eddaed196e0"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-13 10:31:53,845] A new study created in memory with name: no-name-264cfe5b-dba0-465f-9b9d-8b6315bc84b4\n","output_type":"stream"},{"name":"stdout","text":"–î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"871b969f323b4ed5b8f60bda348b963e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13340' max='14005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13340/14005 1:47:48 < 05:22, 2.06 it/s, Epoch 4.76/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.554500</td>\n      <td>0.508857</td>\n      <td>0.844578</td>\n      <td>0.778092</td>\n      <td>0.735286</td>\n      <td>0.844578</td>\n      <td>0.167706</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.478000</td>\n      <td>0.455125</td>\n      <td>0.850201</td>\n      <td>0.807494</td>\n      <td>0.789651</td>\n      <td>0.850201</td>\n      <td>0.446652</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.421500</td>\n      <td>0.444806</td>\n      <td>0.851539</td>\n      <td>0.803628</td>\n      <td>0.802166</td>\n      <td>0.851539</td>\n      <td>0.465818</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.450300</td>\n      <td>0.431311</td>\n      <td>0.855556</td>\n      <td>0.810837</td>\n      <td>0.799206</td>\n      <td>0.855556</td>\n      <td>0.466756</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.429600</td>\n      <td>0.417817</td>\n      <td>0.859349</td>\n      <td>0.825739</td>\n      <td>0.809636</td>\n      <td>0.859349</td>\n      <td>0.506089</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.390900</td>\n      <td>0.428604</td>\n      <td>0.857385</td>\n      <td>0.811276</td>\n      <td>0.802438</td>\n      <td>0.857385</td>\n      <td>0.474174</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.430600</td>\n      <td>0.414507</td>\n      <td>0.859393</td>\n      <td>0.827536</td>\n      <td>0.817012</td>\n      <td>0.859393</td>\n      <td>0.533089</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.419000</td>\n      <td>0.420860</td>\n      <td>0.859393</td>\n      <td>0.822049</td>\n      <td>0.811150</td>\n      <td>0.859393</td>\n      <td>0.517477</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.427500</td>\n      <td>0.411024</td>\n      <td>0.859750</td>\n      <td>0.830580</td>\n      <td>0.813846</td>\n      <td>0.859750</td>\n      <td>0.526442</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.449300</td>\n      <td>0.407155</td>\n      <td>0.860464</td>\n      <td>0.829218</td>\n      <td>0.820845</td>\n      <td>0.860464</td>\n      <td>0.553536</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.401300</td>\n      <td>0.410706</td>\n      <td>0.862918</td>\n      <td>0.833649</td>\n      <td>0.824584</td>\n      <td>0.862918</td>\n      <td>0.567289</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.359900</td>\n      <td>0.418243</td>\n      <td>0.848237</td>\n      <td>0.841408</td>\n      <td>0.835653</td>\n      <td>0.848237</td>\n      <td>0.583877</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.380700</td>\n      <td>0.416629</td>\n      <td>0.860196</td>\n      <td>0.835699</td>\n      <td>0.831429</td>\n      <td>0.860196</td>\n      <td>0.573228</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.361000</td>\n      <td>0.404269</td>\n      <td>0.864123</td>\n      <td>0.839007</td>\n      <td>0.828727</td>\n      <td>0.864123</td>\n      <td>0.559867</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.360000</td>\n      <td>0.405639</td>\n      <td>0.862918</td>\n      <td>0.842130</td>\n      <td>0.828364</td>\n      <td>0.862918</td>\n      <td>0.575285</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.350000</td>\n      <td>0.427751</td>\n      <td>0.859393</td>\n      <td>0.840956</td>\n      <td>0.832799</td>\n      <td>0.859393</td>\n      <td>0.578867</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.318000</td>\n      <td>0.435511</td>\n      <td>0.862294</td>\n      <td>0.840187</td>\n      <td>0.829365</td>\n      <td>0.862294</td>\n      <td>0.576460</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.322500</td>\n      <td>0.456395</td>\n      <td>0.864926</td>\n      <td>0.838778</td>\n      <td>0.829106</td>\n      <td>0.864926</td>\n      <td>0.557509</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.327900</td>\n      <td>0.438523</td>\n      <td>0.859036</td>\n      <td>0.841367</td>\n      <td>0.832458</td>\n      <td>0.859036</td>\n      <td>0.570614</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.309900</td>\n      <td>0.439526</td>\n      <td>0.862517</td>\n      <td>0.843545</td>\n      <td>0.834419</td>\n      <td>0.862517</td>\n      <td>0.563313</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.297500</td>\n      <td>0.440235</td>\n      <td>0.861000</td>\n      <td>0.845167</td>\n      <td>0.835174</td>\n      <td>0.861000</td>\n      <td>0.575021</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.304900</td>\n      <td>0.446166</td>\n      <td>0.846095</td>\n      <td>0.844310</td>\n      <td>0.843616</td>\n      <td>0.846095</td>\n      <td>0.582625</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.251000</td>\n      <td>0.485858</td>\n      <td>0.855689</td>\n      <td>0.844379</td>\n      <td>0.836422</td>\n      <td>0.855689</td>\n      <td>0.576319</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.264500</td>\n      <td>0.485677</td>\n      <td>0.857207</td>\n      <td>0.844507</td>\n      <td>0.835607</td>\n      <td>0.857207</td>\n      <td>0.573920</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.236900</td>\n      <td>0.497675</td>\n      <td>0.856448</td>\n      <td>0.843260</td>\n      <td>0.834338</td>\n      <td>0.856448</td>\n      <td>0.577886</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.250100</td>\n      <td>0.497966</td>\n      <td>0.853414</td>\n      <td>0.844607</td>\n      <td>0.838187</td>\n      <td>0.853414</td>\n      <td>0.579931</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pymorphy2","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:37:32.992233Z","iopub.execute_input":"2024-07-14T11:37:32.992607Z","iopub.status.idle":"2024-07-14T11:37:47.236966Z","shell.execute_reply.started":"2024-07-14T11:37:32.992573Z","shell.execute_reply":"2024-07-14T11:37:47.235742Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting pymorphy2\n  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\nCollecting dawg-python>=0.7.1 (from pymorphy2)\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\nCollecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.10/site-packages (from pymorphy2) (0.6.2)\nDownloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nDownloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\nSuccessfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n","output_type":"stream"}]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T13:42:52.069857Z","iopub.execute_input":"2024-07-13T13:42:52.070189Z","iopub.status.idle":"2024-07-13T13:42:52.118205Z","shell.execute_reply.started":"2024-07-13T13:42:52.070164Z","shell.execute_reply":"2024-07-13T13:42:52.117319Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\nimport torch\nfrom torch import nn\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏–¥ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\ndf = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_class'] = df['rating'].astype(int) - 1  # –ö–ª–∞—Å—Å—ã –æ—Ç 0 –¥–æ 4\n\ndef preprocess_text(text):\n    return text.lower().strip() if isinstance(text, str) else ''\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df.dropna(subset=['processed_text', 'rating_class'])\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\nprint(f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_df)}\")\nprint(f\"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(test_df)}\")\n\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=256)\n\ntokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\ntokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\n\ntokenized_train = tokenized_train.rename_column(\"rating_class\", \"labels\")\ntokenized_test = tokenized_test.rename_column(\"rating_class\", \"labels\")\n\nnum_labels = df['rating_class'].nunique()\n\nclass ClassificationModel(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        \n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n        \n        return (loss, logits) if loss is not None else logits\n\nmodel = ClassificationModel('cointegrated/rubert-tiny', num_labels)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    spearman_corr, _ = spearmanr(labels, preds)\n\n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"spearman\": spearman_corr\n    }\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    warmup_steps=500,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    gradient_accumulation_steps=2,\n    fp16=True if torch.cuda.is_available() else False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\neval_results = trainer.evaluate()\nprint(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:\", eval_results)\n\ntrainer.save_model(\"./final_model\")\n\npredictions = trainer.predict(tokenized_test).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = tokenized_test['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\nprint(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\nprint(f\"F1 (weighted): {eval_results['eval_f1']:.4f}\") \nprint(f\"Precision (weighted): {eval_results['eval_precision']:.4f}\")\nprint(f\"Recall (weighted): {eval_results['eval_recall']:.4f}\")\nprint(f\"Spearman correlation: {eval_results['eval_spearman']:.4f}\")\n\n# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\n# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=num_labels, kde=True)\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.137027Z","iopub.status.idle":"2024-07-12T20:22:24.137427Z","shell.execute_reply.started":"2024-07-12T20:22:24.137244Z","shell.execute_reply":"2024-07-12T20:22:24.137260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset, load_from_disk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nfrom optuna.samplers import TPESampler\nimport pymorphy2\nimport re\nfrom functools import partial, lru_cache\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_memory / 1e9:.2f} GB\")\n\n# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\nif os.path.exists('./processed_data'):\n    encoded_train = load_from_disk('./processed_data/train')\n    encoded_val = load_from_disk('./processed_data/val')\n    print(\"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\")\nelse:\n    df = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\n    df = df[df['language'] == 'russian']\n    df['rating_class'] = df['rating'].astype(int) - 1\n\n    morph = pymorphy2.MorphAnalyzer()\n    stop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\n    @lru_cache(maxsize=None)\n    def lemmatize(word):\n        return morph.parse(word)[0].normal_form\n\n    def preprocess_text(text):\n        if pd.isna(text) or not isinstance(text, str):\n            return ''\n        text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n        return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\n    df['processed_text'] = df['combined_text'].apply(preprocess_text)\n    df = df[['processed_text', 'rating_class']].dropna()\n\n    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\n    del df\n    gc.collect()\n\n    tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"processed_text\"], truncation=True, max_length=256)\n\n    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\n    del train_df, val_df\n    gc.collect()\n\n    encoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n    encoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\n    encoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\n    encoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\n    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫\n    encoded_train.save_to_disk('./processed_data/train')\n    encoded_val.save_to_disk('./processed_data/val')\n    print(\"–î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'),\n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-3, 1e-1, log=True)\n    warmup_ratio = trial.suggest_float('warmup_ratio', 0.1, 0.3)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [32, 64])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_ratio=warmup_ratio,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"no\",\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=6,\n        optim=\"adamw_torch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=10)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",\n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'],\n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_ratio=trial.params['warmup_ratio'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True,\n    dataloader_num_workers=6,\n    optim=\"adamw_torch\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T13:43:47.580665Z","iopub.execute_input":"2024-07-13T13:43:47.581025Z","iopub.status.idle":"2024-07-14T01:05:38.655362Z","shell.execute_reply.started":"2024-07-13T13:43:47.580997Z","shell.execute_reply":"2024-07-14T01:05:38.652929Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\nGPU: Tesla P100-PCIE-16GB, –û–±—â–∞—è –ø–∞–º—è—Ç—å: 17.06 GB\n–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: 16.79 GB\n–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edfb12c48d454be2a1b48d6ad9ec46e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca89a7bc835242c3982e8e8cfd6a46c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a85acfff1ebf4ce8b58015e6d41dfebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28a172d2e1be4886a0574882ed026810"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-13 13:44:05,516] A new study created in memory with name: no-name-9c7103e6-b953-4aa6-834a-14e3c5b8238f\n","output_type":"stream"},{"name":"stdout","text":"–î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"213f4d724ad64da292748c370d04c45f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8406' max='8406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8406/8406 1:11:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.549100</td>\n      <td>0.513470</td>\n      <td>0.844578</td>\n      <td>0.776738</td>\n      <td>0.732959</td>\n      <td>0.844578</td>\n      <td>0.143102</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.479200</td>\n      <td>0.454114</td>\n      <td>0.851049</td>\n      <td>0.799976</td>\n      <td>0.770562</td>\n      <td>0.851049</td>\n      <td>0.374745</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.421900</td>\n      <td>0.446886</td>\n      <td>0.851406</td>\n      <td>0.806747</td>\n      <td>0.813945</td>\n      <td>0.851406</td>\n      <td>0.476772</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.445400</td>\n      <td>0.427374</td>\n      <td>0.855913</td>\n      <td>0.808510</td>\n      <td>0.797749</td>\n      <td>0.855913</td>\n      <td>0.451610</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.425100</td>\n      <td>0.416201</td>\n      <td>0.860241</td>\n      <td>0.832242</td>\n      <td>0.818206</td>\n      <td>0.860241</td>\n      <td>0.534618</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.388000</td>\n      <td>0.418405</td>\n      <td>0.862115</td>\n      <td>0.831430</td>\n      <td>0.822711</td>\n      <td>0.862115</td>\n      <td>0.518005</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.423100</td>\n      <td>0.406698</td>\n      <td>0.861446</td>\n      <td>0.834909</td>\n      <td>0.824812</td>\n      <td>0.861446</td>\n      <td>0.560370</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.401800</td>\n      <td>0.407659</td>\n      <td>0.861580</td>\n      <td>0.833453</td>\n      <td>0.820418</td>\n      <td>0.861580</td>\n      <td>0.556473</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.401400</td>\n      <td>0.406532</td>\n      <td>0.857564</td>\n      <td>0.843366</td>\n      <td>0.831485</td>\n      <td>0.857564</td>\n      <td>0.586494</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.432600</td>\n      <td>0.399940</td>\n      <td>0.865105</td>\n      <td>0.838377</td>\n      <td>0.827151</td>\n      <td>0.865105</td>\n      <td>0.555853</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.381500</td>\n      <td>0.405682</td>\n      <td>0.863588</td>\n      <td>0.836620</td>\n      <td>0.827270</td>\n      <td>0.863588</td>\n      <td>0.567463</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.349800</td>\n      <td>0.411021</td>\n      <td>0.857296</td>\n      <td>0.844370</td>\n      <td>0.835723</td>\n      <td>0.857296</td>\n      <td>0.590691</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.363600</td>\n      <td>0.418416</td>\n      <td>0.863454</td>\n      <td>0.839533</td>\n      <td>0.830355</td>\n      <td>0.863454</td>\n      <td>0.561781</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.339900</td>\n      <td>0.408903</td>\n      <td>0.860152</td>\n      <td>0.843326</td>\n      <td>0.833140</td>\n      <td>0.860152</td>\n      <td>0.585685</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.346700</td>\n      <td>0.409230</td>\n      <td>0.858813</td>\n      <td>0.845371</td>\n      <td>0.838181</td>\n      <td>0.858813</td>\n      <td>0.595307</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.331100</td>\n      <td>0.406393</td>\n      <td>0.861044</td>\n      <td>0.843570</td>\n      <td>0.834343</td>\n      <td>0.861044</td>\n      <td>0.590698</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 14:57:49,874] Trial 0 finished with value: 0.8449373972480296 and parameters: {'lr': 1.2908549889168956e-05, 'weight_decay': 0.005103235594127304, 'warmup_ratio': 0.20031204046361972, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1}. Best is trial 0 with value: 0.8449373972480296.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4203' max='4203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4203/4203 1:08:31, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.458700</td>\n      <td>0.457488</td>\n      <td>0.848907</td>\n      <td>0.786391</td>\n      <td>0.766977</td>\n      <td>0.848907</td>\n      <td>0.267915</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.451600</td>\n      <td>0.424410</td>\n      <td>0.857073</td>\n      <td>0.826741</td>\n      <td>0.813072</td>\n      <td>0.857073</td>\n      <td>0.516674</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.405500</td>\n      <td>0.414798</td>\n      <td>0.856894</td>\n      <td>0.837705</td>\n      <td>0.824476</td>\n      <td>0.856894</td>\n      <td>0.549954</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.395500</td>\n      <td>0.407590</td>\n      <td>0.857787</td>\n      <td>0.835000</td>\n      <td>0.822641</td>\n      <td>0.857787</td>\n      <td>0.566061</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.409300</td>\n      <td>0.397742</td>\n      <td>0.864212</td>\n      <td>0.838941</td>\n      <td>0.829430</td>\n      <td>0.864212</td>\n      <td>0.553894</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.346800</td>\n      <td>0.405058</td>\n      <td>0.860509</td>\n      <td>0.846863</td>\n      <td>0.837640</td>\n      <td>0.860509</td>\n      <td>0.591649</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.350900</td>\n      <td>0.404855</td>\n      <td>0.864480</td>\n      <td>0.843685</td>\n      <td>0.833770</td>\n      <td>0.864480</td>\n      <td>0.578305</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.349900</td>\n      <td>0.400785</td>\n      <td>0.863498</td>\n      <td>0.844011</td>\n      <td>0.835188</td>\n      <td>0.863498</td>\n      <td>0.582050</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 16:07:54,053] Trial 1 finished with value: 0.8452050854315599 and parameters: {'lr': 1.8113121559653823e-05, 'weight_decay': 0.0016944490683774038, 'warmup_ratio': 0.17469543112886446, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 1}. Best is trial 1 with value: 0.8452050854315599.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2100/2100 1:01:17, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.437200</td>\n      <td>0.429138</td>\n      <td>0.855422</td>\n      <td>0.820127</td>\n      <td>0.810937</td>\n      <td>0.855422</td>\n      <td>0.514251</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.403300</td>\n      <td>0.416942</td>\n      <td>0.854975</td>\n      <td>0.828642</td>\n      <td>0.820820</td>\n      <td>0.854975</td>\n      <td>0.561888</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.332800</td>\n      <td>0.408509</td>\n      <td>0.858322</td>\n      <td>0.846569</td>\n      <td>0.838387</td>\n      <td>0.858322</td>\n      <td>0.588779</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.344400</td>\n      <td>0.402647</td>\n      <td>0.862695</td>\n      <td>0.845243</td>\n      <td>0.836685</td>\n      <td>0.862695</td>\n      <td>0.587676</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 17:10:45,788] Trial 2 finished with value: 0.846695782810575 and parameters: {'lr': 4.21914509537095e-05, 'weight_decay': 0.009384607119192066, 'warmup_ratio': 0.2793705261401517, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 2}. Best is trial 2 with value: 0.846695782810575.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4203' max='4203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4203/4203 1:08:23, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.488100</td>\n      <td>0.478244</td>\n      <td>0.845649</td>\n      <td>0.782522</td>\n      <td>0.740846</td>\n      <td>0.845649</td>\n      <td>0.245852</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.465300</td>\n      <td>0.434199</td>\n      <td>0.855020</td>\n      <td>0.822886</td>\n      <td>0.807591</td>\n      <td>0.855020</td>\n      <td>0.506422</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.419700</td>\n      <td>0.423430</td>\n      <td>0.856939</td>\n      <td>0.829464</td>\n      <td>0.816218</td>\n      <td>0.856939</td>\n      <td>0.506961</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.405900</td>\n      <td>0.412992</td>\n      <td>0.857385</td>\n      <td>0.833413</td>\n      <td>0.820499</td>\n      <td>0.857385</td>\n      <td>0.557531</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.420300</td>\n      <td>0.404243</td>\n      <td>0.863231</td>\n      <td>0.831326</td>\n      <td>0.816522</td>\n      <td>0.863231</td>\n      <td>0.529535</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.366600</td>\n      <td>0.404947</td>\n      <td>0.861312</td>\n      <td>0.843602</td>\n      <td>0.835607</td>\n      <td>0.861312</td>\n      <td>0.579797</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.371000</td>\n      <td>0.406148</td>\n      <td>0.863498</td>\n      <td>0.839515</td>\n      <td>0.829936</td>\n      <td>0.863498</td>\n      <td>0.565254</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.368100</td>\n      <td>0.401107</td>\n      <td>0.863454</td>\n      <td>0.840052</td>\n      <td>0.829223</td>\n      <td>0.863454</td>\n      <td>0.573669</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 18:20:41,670] Trial 3 finished with value: 0.840466066878496 and parameters: {'lr': 1.0575488202143836e-05, 'weight_decay': 0.004587317825934914, 'warmup_ratio': 0.2357886622799123, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 1}. Best is trial 2 with value: 0.846695782810575.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8406' max='8406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8406/8406 1:11:45, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.504300</td>\n      <td>0.489455</td>\n      <td>0.849264</td>\n      <td>0.786121</td>\n      <td>0.743520</td>\n      <td>0.849264</td>\n      <td>0.244277</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.466000</td>\n      <td>0.446358</td>\n      <td>0.854172</td>\n      <td>0.797076</td>\n      <td>0.778382</td>\n      <td>0.854172</td>\n      <td>0.376166</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.443400</td>\n      <td>0.455919</td>\n      <td>0.844712</td>\n      <td>0.799306</td>\n      <td>0.765619</td>\n      <td>0.844712</td>\n      <td>0.504532</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.480400</td>\n      <td>0.446641</td>\n      <td>0.851495</td>\n      <td>0.792242</td>\n      <td>0.763195</td>\n      <td>0.851495</td>\n      <td>0.332607</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.442500</td>\n      <td>0.453687</td>\n      <td>0.856983</td>\n      <td>0.826146</td>\n      <td>0.809185</td>\n      <td>0.856983</td>\n      <td>0.506147</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.419000</td>\n      <td>0.446032</td>\n      <td>0.853101</td>\n      <td>0.801979</td>\n      <td>0.814351</td>\n      <td>0.853101</td>\n      <td>0.454453</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.449100</td>\n      <td>0.457443</td>\n      <td>0.856582</td>\n      <td>0.832400</td>\n      <td>0.816678</td>\n      <td>0.856582</td>\n      <td>0.545300</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.432600</td>\n      <td>0.444031</td>\n      <td>0.859527</td>\n      <td>0.815845</td>\n      <td>0.804596</td>\n      <td>0.859527</td>\n      <td>0.482341</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.421800</td>\n      <td>0.440918</td>\n      <td>0.850602</td>\n      <td>0.833796</td>\n      <td>0.823935</td>\n      <td>0.850602</td>\n      <td>0.545837</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.447300</td>\n      <td>0.430693</td>\n      <td>0.859259</td>\n      <td>0.836189</td>\n      <td>0.822976</td>\n      <td>0.859259</td>\n      <td>0.560887</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.411600</td>\n      <td>0.443993</td>\n      <td>0.860018</td>\n      <td>0.831591</td>\n      <td>0.818932</td>\n      <td>0.860018</td>\n      <td>0.555549</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.361700</td>\n      <td>0.449977</td>\n      <td>0.840696</td>\n      <td>0.836228</td>\n      <td>0.833586</td>\n      <td>0.840696</td>\n      <td>0.572272</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.380000</td>\n      <td>0.443130</td>\n      <td>0.861758</td>\n      <td>0.831180</td>\n      <td>0.820330</td>\n      <td>0.861758</td>\n      <td>0.549944</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.355600</td>\n      <td>0.415792</td>\n      <td>0.861669</td>\n      <td>0.836915</td>\n      <td>0.823504</td>\n      <td>0.861669</td>\n      <td>0.553751</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.353100</td>\n      <td>0.425589</td>\n      <td>0.856091</td>\n      <td>0.842730</td>\n      <td>0.831583</td>\n      <td>0.856091</td>\n      <td>0.579673</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.342400</td>\n      <td>0.427108</td>\n      <td>0.860018</td>\n      <td>0.842387</td>\n      <td>0.832698</td>\n      <td>0.860018</td>\n      <td>0.574963</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 19:33:59,322] Trial 4 finished with value: 0.841624913275903 and parameters: {'lr': 8.277445575479811e-05, 'weight_decay': 0.09296085054594586, 'warmup_ratio': 0.21481125312117733, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1}. Best is trial 2 with value: 0.846695782810575.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8406' max='8406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8406/8406 1:11:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.550200</td>\n      <td>0.513551</td>\n      <td>0.844668</td>\n      <td>0.773541</td>\n      <td>0.713463</td>\n      <td>0.844668</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.482800</td>\n      <td>0.466245</td>\n      <td>0.849487</td>\n      <td>0.794583</td>\n      <td>0.764157</td>\n      <td>0.849487</td>\n      <td>0.336728</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.419700</td>\n      <td>0.441047</td>\n      <td>0.852209</td>\n      <td>0.810869</td>\n      <td>0.806179</td>\n      <td>0.852209</td>\n      <td>0.475091</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.451100</td>\n      <td>0.432932</td>\n      <td>0.855600</td>\n      <td>0.812678</td>\n      <td>0.798658</td>\n      <td>0.855600</td>\n      <td>0.463824</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.429900</td>\n      <td>0.425029</td>\n      <td>0.858724</td>\n      <td>0.823858</td>\n      <td>0.814200</td>\n      <td>0.858724</td>\n      <td>0.513370</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.389400</td>\n      <td>0.420770</td>\n      <td>0.860598</td>\n      <td>0.830620</td>\n      <td>0.818238</td>\n      <td>0.860598</td>\n      <td>0.510251</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.429500</td>\n      <td>0.410238</td>\n      <td>0.859750</td>\n      <td>0.826963</td>\n      <td>0.820555</td>\n      <td>0.859750</td>\n      <td>0.535946</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.403800</td>\n      <td>0.407433</td>\n      <td>0.861000</td>\n      <td>0.828845</td>\n      <td>0.818094</td>\n      <td>0.861000</td>\n      <td>0.537667</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.407600</td>\n      <td>0.405387</td>\n      <td>0.858858</td>\n      <td>0.841904</td>\n      <td>0.829439</td>\n      <td>0.858858</td>\n      <td>0.569454</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.435900</td>\n      <td>0.401827</td>\n      <td>0.864079</td>\n      <td>0.836297</td>\n      <td>0.824805</td>\n      <td>0.864079</td>\n      <td>0.551092</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.381500</td>\n      <td>0.407751</td>\n      <td>0.862249</td>\n      <td>0.831713</td>\n      <td>0.823634</td>\n      <td>0.862249</td>\n      <td>0.553310</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.346400</td>\n      <td>0.413183</td>\n      <td>0.855868</td>\n      <td>0.844055</td>\n      <td>0.836278</td>\n      <td>0.855868</td>\n      <td>0.592428</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.369200</td>\n      <td>0.414485</td>\n      <td>0.863052</td>\n      <td>0.836769</td>\n      <td>0.828240</td>\n      <td>0.863052</td>\n      <td>0.548106</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.346000</td>\n      <td>0.409087</td>\n      <td>0.861669</td>\n      <td>0.843693</td>\n      <td>0.833737</td>\n      <td>0.861669</td>\n      <td>0.581719</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.349900</td>\n      <td>0.407664</td>\n      <td>0.860464</td>\n      <td>0.845375</td>\n      <td>0.838025</td>\n      <td>0.860464</td>\n      <td>0.592294</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.342200</td>\n      <td>0.406115</td>\n      <td>0.862695</td>\n      <td>0.843731</td>\n      <td>0.834669</td>\n      <td>0.862695</td>\n      <td>0.582968</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n  warnings.warn(stats.ConstantInputWarning(warn_msg))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 20:47:17,956] Trial 5 finished with value: 0.8444547405419074 and parameters: {'lr': 1.1628440612281945e-05, 'weight_decay': 0.09399225385040957, 'warmup_ratio': 0.22360217043327646, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1}. Best is trial 2 with value: 0.846695782810575.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4203' max='4203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4203/4203 1:08:23, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.448000</td>\n      <td>0.452833</td>\n      <td>0.852075</td>\n      <td>0.790266</td>\n      <td>0.760251</td>\n      <td>0.852075</td>\n      <td>0.330933</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.465400</td>\n      <td>0.446676</td>\n      <td>0.851896</td>\n      <td>0.792598</td>\n      <td>0.754545</td>\n      <td>0.851896</td>\n      <td>0.349547</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.421000</td>\n      <td>0.425358</td>\n      <td>0.856627</td>\n      <td>0.823692</td>\n      <td>0.809352</td>\n      <td>0.856627</td>\n      <td>0.495121</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.412700</td>\n      <td>0.418232</td>\n      <td>0.859750</td>\n      <td>0.837370</td>\n      <td>0.823080</td>\n      <td>0.859750</td>\n      <td>0.565034</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.413900</td>\n      <td>0.410266</td>\n      <td>0.862963</td>\n      <td>0.836380</td>\n      <td>0.821001</td>\n      <td>0.862963</td>\n      <td>0.549605</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.340700</td>\n      <td>0.427696</td>\n      <td>0.859661</td>\n      <td>0.836174</td>\n      <td>0.823681</td>\n      <td>0.859661</td>\n      <td>0.569344</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.343000</td>\n      <td>0.416908</td>\n      <td>0.864971</td>\n      <td>0.840546</td>\n      <td>0.829296</td>\n      <td>0.864971</td>\n      <td>0.569095</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.332700</td>\n      <td>0.423378</td>\n      <td>0.859839</td>\n      <td>0.844904</td>\n      <td>0.836146</td>\n      <td>0.859839</td>\n      <td>0.584973</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 21:57:13,604] Trial 6 finished with value: 0.8449711487778618 and parameters: {'lr': 9.052400594277023e-05, 'weight_decay': 0.0015168796304120772, 'warmup_ratio': 0.19936912457688194, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 1}. Best is trial 2 with value: 0.846695782810575.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4203' max='4203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4203/4203 1:08:24, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.449600</td>\n      <td>0.451266</td>\n      <td>0.851049</td>\n      <td>0.796522</td>\n      <td>0.775784</td>\n      <td>0.851049</td>\n      <td>0.338741</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.453500</td>\n      <td>0.429864</td>\n      <td>0.854752</td>\n      <td>0.799174</td>\n      <td>0.791663</td>\n      <td>0.854752</td>\n      <td>0.417902</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.416100</td>\n      <td>0.422944</td>\n      <td>0.859170</td>\n      <td>0.821779</td>\n      <td>0.813291</td>\n      <td>0.859170</td>\n      <td>0.496056</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.410200</td>\n      <td>0.435102</td>\n      <td>0.850067</td>\n      <td>0.834144</td>\n      <td>0.825839</td>\n      <td>0.850067</td>\n      <td>0.571445</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.416800</td>\n      <td>0.405026</td>\n      <td>0.861892</td>\n      <td>0.834195</td>\n      <td>0.818182</td>\n      <td>0.861892</td>\n      <td>0.550739</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.345100</td>\n      <td>0.418975</td>\n      <td>0.855957</td>\n      <td>0.843700</td>\n      <td>0.839331</td>\n      <td>0.855957</td>\n      <td>0.593497</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.342500</td>\n      <td>0.409511</td>\n      <td>0.862784</td>\n      <td>0.838406</td>\n      <td>0.827959</td>\n      <td>0.862784</td>\n      <td>0.565121</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.337300</td>\n      <td>0.409752</td>\n      <td>0.861624</td>\n      <td>0.844803</td>\n      <td>0.835703</td>\n      <td>0.861624</td>\n      <td>0.583874</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-13 23:07:10,704] Trial 7 finished with value: 0.8472489786057343 and parameters: {'lr': 6.595368363589886e-05, 'weight_decay': 0.004428752267862051, 'warmup_ratio': 0.28267504629895923, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 1}. Best is trial 7 with value: 0.8472489786057343.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4203' max='4203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4203/4203 57:36, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.467100</td>\n      <td>0.459218</td>\n      <td>0.847880</td>\n      <td>0.781363</td>\n      <td>0.732340</td>\n      <td>0.847880</td>\n      <td>0.175924</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.457300</td>\n      <td>0.436856</td>\n      <td>0.854529</td>\n      <td>0.802031</td>\n      <td>0.782177</td>\n      <td>0.854529</td>\n      <td>0.446377</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.413700</td>\n      <td>0.422977</td>\n      <td>0.858010</td>\n      <td>0.833608</td>\n      <td>0.819327</td>\n      <td>0.858010</td>\n      <td>0.531949</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.407600</td>\n      <td>0.418664</td>\n      <td>0.855868</td>\n      <td>0.838632</td>\n      <td>0.825852</td>\n      <td>0.855868</td>\n      <td>0.574062</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.413200</td>\n      <td>0.410789</td>\n      <td>0.863141</td>\n      <td>0.828449</td>\n      <td>0.813872</td>\n      <td>0.863141</td>\n      <td>0.525547</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.336900</td>\n      <td>0.429644</td>\n      <td>0.847702</td>\n      <td>0.840123</td>\n      <td>0.833285</td>\n      <td>0.847702</td>\n      <td>0.580878</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.339800</td>\n      <td>0.421776</td>\n      <td>0.861089</td>\n      <td>0.838137</td>\n      <td>0.827341</td>\n      <td>0.861089</td>\n      <td>0.566291</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.327700</td>\n      <td>0.426641</td>\n      <td>0.858411</td>\n      <td>0.842524</td>\n      <td>0.835339</td>\n      <td>0.858411</td>\n      <td>0.574972</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-14 00:06:19,712] Trial 8 finished with value: 0.8413704791104608 and parameters: {'lr': 9.284592059747402e-05, 'weight_decay': 0.010106986253487358, 'warmup_ratio': 0.12441696281081911, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 2}. Best is trial 7 with value: 0.8472489786057343.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1050/1050 57:42, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.419700</td>\n      <td>0.414583</td>\n      <td>0.860062</td>\n      <td>0.835728</td>\n      <td>0.821073</td>\n      <td>0.860062</td>\n      <td>0.554697</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.392900</td>\n      <td>0.405321</td>\n      <td>0.860732</td>\n      <td>0.833539</td>\n      <td>0.832534</td>\n      <td>0.860732</td>\n      <td>0.554410</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n[I 2024-07-14 01:05:36,541] Trial 9 finished with value: 0.8350537223997377 and parameters: {'lr': 1.3611049248706139e-05, 'weight_decay': 0.0068580610507093884, 'warmup_ratio': 0.1183149979230736, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 4}. Best is trial 7 with value: 0.8472489786057343.\n","output_type":"stream"},{"name":"stdout","text":"Best trial:\n  Value:  0.8472489786057343\n  Params: \n    lr: 6.595368363589886e-05\n    weight_decay: 0.004428752267862051\n    warmup_ratio: 0.28267504629895923\n    per_device_train_batch_size: 64\n    gradient_accumulation_steps: 1\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 171\u001b[0m\n\u001b[1;32m    167\u001b[0m best_model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepPavlov/rubert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    169\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m best_training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results/best_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mper_device_train_batch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwarmup_ratio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs/best_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_accumulation_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw_torch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    191\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m best_trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    194\u001b[0m     model\u001b[38;5;241m=\u001b[39mbest_model,\n\u001b[1;32m    195\u001b[0m     args\u001b[38;5;241m=\u001b[39mbest_training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    202\u001b[0m best_trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m<string>:129\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1556\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_best_model_at_end:\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_strategy:\n\u001b[0;32m-> 1556\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--load_best_model_at_end requires the save and eval strategy to match, but found\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- Evaluation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1558\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- Save strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m         )\n\u001b[1;32m   1560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy \u001b[38;5;241m==\u001b[39m IntervalStrategy\u001b[38;5;241m.\u001b[39mSTEPS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","\u001b[0;31mValueError\u001b[0m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: steps\n- Save strategy: epoch"],"ename":"ValueError","evalue":"--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: steps\n- Save strategy: epoch","output_type":"error"}]},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nfrom datasets import Dataset, load_from_disk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom scipy.stats import spearmanr\nimport optuna\nfrom optuna.samplers import TPESampler\nimport pymorphy2\nimport re\nfrom functools import partial, lru_cache\nimport gc\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name}, –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu.total_memory / 1e9:.2f} GB\")\n    \n    total_memory, free_memory = torch.cuda.mem_get_info(0)\n    print(f\"–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: {total_memory / 1e9:.2f} GB\")\n    print(f\"–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_memory / 1e9:.2f} GB\")\n\n# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\nif os.path.exists('./processed_data'):\n    encoded_train = load_from_disk('./processed_data/train')\n    encoded_val = load_from_disk('./processed_data/val')\n    print(\"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\")\nelse:\n    df = pd.read_csv('/kaggle/input/cleaned-kaspi-reviews/cleaned_kaspi_reviews.csv')\n    df = df[df['language'] == 'russian']\n    df['rating_class'] = df['rating'].astype(int) - 1\n\n    morph = pymorphy2.MorphAnalyzer()\n    stop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\n    @lru_cache(maxsize=None)\n    def lemmatize(word):\n        return morph.parse(word)[0].normal_form\n\n    def preprocess_text(text):\n        if pd.isna(text) or not isinstance(text, str):\n            return ''\n        text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n        return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\n    df['processed_text'] = df['combined_text'].apply(preprocess_text)\n    df = df[['processed_text', 'rating_class']].dropna()\n\n    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['rating_class'])\n\n    del df\n    gc.collect()\n\n    tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"processed_text\"], truncation=True, max_length=256)\n\n    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\n    del train_df, val_df\n    gc.collect()\n\n    encoded_train = train_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n    encoded_val = val_dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['processed_text'])\n\n    encoded_train = encoded_train.rename_column(\"rating_class\", \"labels\")\n    encoded_val = encoded_val.rename_column(\"rating_class\", \"labels\")\n\n    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫\n    encoded_train.save_to_disk('./processed_data/train')\n    encoded_val.save_to_disk('./processed_data/val')\n    print(\"–î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted'),\n        \"precision\": precision_score(labels, preds, average='weighted'),\n        \"recall\": recall_score(labels, preds, average='weighted'),\n        \"spearman\": spearmanr(labels, preds)[0]\n    }\n\ndef objective(trial, train_dataset, val_dataset):\n    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-3, 1e-1, log=True)\n    warmup_ratio = trial.suggest_float('warmup_ratio', 0.1, 0.3)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [32, 64])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n\n    model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/trial_{trial.number}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=64,\n        learning_rate=lr,\n        weight_decay=weight_decay,\n        warmup_ratio=warmup_ratio,\n        logging_dir=f'./logs/trial_{trial.number}',\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"no\",\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        load_best_model_at_end=False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        fp16=True,\n        dataloader_num_workers=0,  # –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ 0 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤\n        optim=\"adamw_torch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return eval_results[\"eval_f1\"]\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\nobjective_with_dataset = partial(objective, train_dataset=encoded_train, val_dataset=encoded_val)\nstudy.optimize(objective_with_dataset, n_trials=5)  # –£–º–µ–Ω—å—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ trial –¥–æ 5\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\ndel study\ngc.collect()\ntorch.cuda.empty_cache()\n\nbest_model = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=5).to(device)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\nbest_training_args = TrainingArguments(\n    output_dir=f\"./results/best_model\",  \n    num_train_epochs=5,\n    per_device_train_batch_size=trial.params['per_device_train_batch_size'], \n    per_device_eval_batch_size=64,\n    learning_rate=trial.params['lr'],\n    weight_decay=trial.params['weight_decay'],\n    warmup_ratio=trial.params['warmup_ratio'],\n    logging_dir=f'./logs/best_model',\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=trial.params['gradient_accumulation_steps'],\n    fp16=True, \n    dataloader_num_workers=0,  # –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ 0 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤\n    optim=\"adamw_torch\"\n)\n\nbest_trainer = Trainer(\n    model=best_model,\n    args=best_training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_val,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\nbest_trainer.train()\n\neval_results = best_trainer.evaluate()\nprint(\"Final Evaluation Results:\")\nprint(eval_results)\n\nbest_trainer.save_model(\"./final_model\")\n\nprint(\"Training completed. Model saved in ./final_model/ directory\")\n\npredictions = best_trainer.predict(encoded_val).predictions\npredicted_classes = np.argmax(predictions, axis=1)\nactual_classes = encoded_val['labels']\n\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(actual_classes, predicted_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 6), yticklabels=range(1, 6))\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\nplt.tight_layout()\nplt.show()\n\nacc = accuracy_score(actual_classes, predicted_classes)\nf1 = f1_score(actual_classes, predicted_classes, average='weighted')\nprecision = precision_score(actual_classes, predicted_classes, average='weighted')\nrecall = recall_score(actual_classes, predicted_classes, average='weighted')\nspearman_corr, _ = spearmanr(actual_classes, predicted_classes)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 (weighted): {f1:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"Spearman correlation: {spearman_corr:.4f}\")\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=actual_classes, y=predicted_classes)\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(predicted_classes, bins=5, kde=True)\nplt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\nplt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤')\nplt.show()\n\ndel best_model, best_trainer\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T03:35:09.840620Z","iopub.execute_input":"2024-07-14T03:35:09.841012Z","iopub.status.idle":"2024-07-14T10:53:36.427092Z","shell.execute_reply.started":"2024-07-14T03:35:09.840978Z","shell.execute_reply":"2024-07-14T10:53:36.426101Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-14 03:35:17.829177: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-14 03:35:17.829372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-14 03:35:17.958531: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\nGPU: Tesla P100-PCIE-16GB, –û–±—â–∞—è –ø–∞–º—è—Ç—å: 17.06 GB\n–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: 16.79 GB\n–°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: 17.06 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d841ea97f7a431e81851bd510b4f221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fe4635c4f634cd8adddb859d6f1a7e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ee825851b94dbdb0643d9ffdeefa40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f2790d8e78443cb21d65a5f44afc38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43126fa6aa7849f9867e86e5a967ab35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c91851ad1047fea92a32f7af811e1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/89639 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee5fbe98b92c4be89af67717bb5873ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/22410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e9d04a9b3f34272ad70e7f0fba530e0"}},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 03:35:55,454] A new study created in memory with name: no-name-7428978d-f6ae-4e29-99c6-141593eee4e2\n","output_type":"stream"},{"name":"stdout","text":"–î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deb9866991d54bbdbd9cd11b6e95be2a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1050/1050 58:46, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.404800</td>\n      <td>0.419694</td>\n      <td>0.856627</td>\n      <td>0.830801</td>\n      <td>0.824443</td>\n      <td>0.856627</td>\n      <td>0.549787</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.342000</td>\n      <td>0.406166</td>\n      <td>0.863141</td>\n      <td>0.844933</td>\n      <td>0.835771</td>\n      <td>0.863141</td>\n      <td>0.586659</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 04:36:25,536] Trial 0 finished with value: 0.8470377699516728 and parameters: {'lr': 8.373000577537992e-05, 'weight_decay': 0.09133311151769442, 'warmup_ratio': 0.23770055385498493, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.8470377699516728.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1050/1050 58:55, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.412100</td>\n      <td>0.408677</td>\n      <td>0.861044</td>\n      <td>0.831446</td>\n      <td>0.816902</td>\n      <td>0.861044</td>\n      <td>0.534992</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.372900</td>\n      <td>0.400262</td>\n      <td>0.862918</td>\n      <td>0.837787</td>\n      <td>0.826341</td>\n      <td>0.862918</td>\n      <td>0.568555</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 05:37:00,311] Trial 1 finished with value: 0.8405681839139436 and parameters: {'lr': 2.4630040082862688e-05, 'weight_decay': 0.0687607389556874, 'warmup_ratio': 0.24717400447156507, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.8470377699516728.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8406' max='8406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8406/8406 1:14:01, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.519600</td>\n      <td>0.493736</td>\n      <td>0.845114</td>\n      <td>0.776397</td>\n      <td>0.734690</td>\n      <td>0.845114</td>\n      <td>0.136588</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.460700</td>\n      <td>0.444366</td>\n      <td>0.854261</td>\n      <td>0.802292</td>\n      <td>0.778606</td>\n      <td>0.854261</td>\n      <td>0.394692</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.423900</td>\n      <td>0.450307</td>\n      <td>0.852075</td>\n      <td>0.811416</td>\n      <td>0.811460</td>\n      <td>0.852075</td>\n      <td>0.498615</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.458300</td>\n      <td>0.433483</td>\n      <td>0.855065</td>\n      <td>0.795855</td>\n      <td>0.788600</td>\n      <td>0.855065</td>\n      <td>0.395184</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.428100</td>\n      <td>0.442286</td>\n      <td>0.855779</td>\n      <td>0.828806</td>\n      <td>0.812529</td>\n      <td>0.855779</td>\n      <td>0.515889</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.386000</td>\n      <td>0.416715</td>\n      <td>0.859393</td>\n      <td>0.820329</td>\n      <td>0.812961</td>\n      <td>0.859393</td>\n      <td>0.500301</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.425500</td>\n      <td>0.423166</td>\n      <td>0.852164</td>\n      <td>0.831703</td>\n      <td>0.824424</td>\n      <td>0.852164</td>\n      <td>0.567447</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.404800</td>\n      <td>0.409024</td>\n      <td>0.862160</td>\n      <td>0.828553</td>\n      <td>0.821619</td>\n      <td>0.862160</td>\n      <td>0.560393</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.407200</td>\n      <td>0.424950</td>\n      <td>0.849576</td>\n      <td>0.840223</td>\n      <td>0.832963</td>\n      <td>0.849576</td>\n      <td>0.572231</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.427700</td>\n      <td>0.413096</td>\n      <td>0.864212</td>\n      <td>0.831892</td>\n      <td>0.819244</td>\n      <td>0.864212</td>\n      <td>0.553932</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.384800</td>\n      <td>0.415261</td>\n      <td>0.862383</td>\n      <td>0.838844</td>\n      <td>0.828818</td>\n      <td>0.862383</td>\n      <td>0.581071</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.333200</td>\n      <td>0.425158</td>\n      <td>0.851406</td>\n      <td>0.841410</td>\n      <td>0.836053</td>\n      <td>0.851406</td>\n      <td>0.592524</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.349100</td>\n      <td>0.426853</td>\n      <td>0.863454</td>\n      <td>0.842252</td>\n      <td>0.835259</td>\n      <td>0.863454</td>\n      <td>0.577133</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.329300</td>\n      <td>0.410882</td>\n      <td>0.862651</td>\n      <td>0.844181</td>\n      <td>0.834041</td>\n      <td>0.862651</td>\n      <td>0.583614</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.334200</td>\n      <td>0.420564</td>\n      <td>0.857653</td>\n      <td>0.845764</td>\n      <td>0.841514</td>\n      <td>0.857653</td>\n      <td>0.592719</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.311500</td>\n      <td>0.423941</td>\n      <td>0.861357</td>\n      <td>0.845149</td>\n      <td>0.835827</td>\n      <td>0.861357</td>\n      <td>0.588457</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 06:52:38,186] Trial 2 finished with value: 0.8443133626381017 and parameters: {'lr': 4.175889942231109e-05, 'weight_decay': 0.02062955456547391, 'warmup_ratio': 0.24104171894041945, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1}. Best is trial 0 with value: 0.8470377699516728.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2100/2100 1:02:34, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.437900</td>\n      <td>0.442390</td>\n      <td>0.854797</td>\n      <td>0.804256</td>\n      <td>0.790514</td>\n      <td>0.854797</td>\n      <td>0.435750</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.408900</td>\n      <td>0.413400</td>\n      <td>0.858367</td>\n      <td>0.827869</td>\n      <td>0.816876</td>\n      <td>0.858367</td>\n      <td>0.539125</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.366800</td>\n      <td>0.405628</td>\n      <td>0.861446</td>\n      <td>0.842155</td>\n      <td>0.832185</td>\n      <td>0.861446</td>\n      <td>0.566971</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.387600</td>\n      <td>0.403217</td>\n      <td>0.861758</td>\n      <td>0.835918</td>\n      <td>0.826036</td>\n      <td>0.861758</td>\n      <td>0.561065</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 07:56:49,943] Trial 3 finished with value: 0.8370660165129629 and parameters: {'lr': 1.1631236085222124e-05, 'weight_decay': 0.022463551697889522, 'warmup_ratio': 0.1351252030363727, 'per_device_train_batch_size': 64, 'gradient_accumulation_steps': 2}. Best is trial 0 with value: 0.8470377699516728.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8406' max='8406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8406/8406 1:14:02, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.533200</td>\n      <td>0.526537</td>\n      <td>0.844578</td>\n      <td>0.773938</td>\n      <td>0.718111</td>\n      <td>0.844578</td>\n      <td>0.070020</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.471700</td>\n      <td>0.452273</td>\n      <td>0.851272</td>\n      <td>0.795529</td>\n      <td>0.775104</td>\n      <td>0.851272</td>\n      <td>0.324987</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.420900</td>\n      <td>0.448786</td>\n      <td>0.853503</td>\n      <td>0.802906</td>\n      <td>0.805481</td>\n      <td>0.853503</td>\n      <td>0.445649</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.448800</td>\n      <td>0.432582</td>\n      <td>0.854485</td>\n      <td>0.796223</td>\n      <td>0.797905</td>\n      <td>0.854485</td>\n      <td>0.388064</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.425600</td>\n      <td>0.422924</td>\n      <td>0.859973</td>\n      <td>0.835043</td>\n      <td>0.820087</td>\n      <td>0.859973</td>\n      <td>0.534117</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.386000</td>\n      <td>0.411533</td>\n      <td>0.861490</td>\n      <td>0.831964</td>\n      <td>0.822565</td>\n      <td>0.861490</td>\n      <td>0.532780</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.421500</td>\n      <td>0.414137</td>\n      <td>0.855823</td>\n      <td>0.832044</td>\n      <td>0.823684</td>\n      <td>0.855823</td>\n      <td>0.564464</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.397500</td>\n      <td>0.411369</td>\n      <td>0.860732</td>\n      <td>0.836361</td>\n      <td>0.835969</td>\n      <td>0.860732</td>\n      <td>0.575586</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.404600</td>\n      <td>0.413351</td>\n      <td>0.855600</td>\n      <td>0.844803</td>\n      <td>0.835405</td>\n      <td>0.855600</td>\n      <td>0.582116</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.427100</td>\n      <td>0.402339</td>\n      <td>0.863632</td>\n      <td>0.837866</td>\n      <td>0.828427</td>\n      <td>0.863632</td>\n      <td>0.568740</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.375200</td>\n      <td>0.409937</td>\n      <td>0.864436</td>\n      <td>0.839628</td>\n      <td>0.829293</td>\n      <td>0.864436</td>\n      <td>0.580893</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.334300</td>\n      <td>0.412080</td>\n      <td>0.860464</td>\n      <td>0.846720</td>\n      <td>0.837605</td>\n      <td>0.860464</td>\n      <td>0.595220</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.345900</td>\n      <td>0.419908</td>\n      <td>0.862517</td>\n      <td>0.843627</td>\n      <td>0.835694</td>\n      <td>0.862517</td>\n      <td>0.582005</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.330200</td>\n      <td>0.409774</td>\n      <td>0.861981</td>\n      <td>0.847039</td>\n      <td>0.837882</td>\n      <td>0.861981</td>\n      <td>0.595252</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.333300</td>\n      <td>0.416523</td>\n      <td>0.859572</td>\n      <td>0.847778</td>\n      <td>0.841574</td>\n      <td>0.859572</td>\n      <td>0.600862</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.313100</td>\n      <td>0.414775</td>\n      <td>0.861312</td>\n      <td>0.845552</td>\n      <td>0.836167</td>\n      <td>0.861312</td>\n      <td>0.594489</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [351/351 01:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2024-07-14 09:12:28,064] Trial 4 finished with value: 0.8463920083764923 and parameters: {'lr': 2.5805374627532186e-05, 'weight_decay': 0.0015557066519140516, 'warmup_ratio': 0.21866189482122533, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1}. Best is trial 0 with value: 0.8470377699516728.\n","output_type":"stream"},{"name":"stdout","text":"Best trial:\n  Value:  0.8470377699516728\n  Params: \n    lr: 8.373000577537992e-05\n    weight_decay: 0.09133311151769442\n    warmup_ratio: 0.23770055385498493\n    per_device_train_batch_size: 64\n    gradient_accumulation_steps: 4\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1750/1750 1:37:52, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Spearman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.421200</td>\n      <td>0.419832</td>\n      <td>0.858322</td>\n      <td>0.821793</td>\n      <td>0.816003</td>\n      <td>0.858322</td>\n      <td>0.530998</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.360900</td>\n      <td>0.410745</td>\n      <td>0.863186</td>\n      <td>0.842512</td>\n      <td>0.829087</td>\n      <td>0.863186</td>\n      <td>0.583056</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.246600</td>\n      <td>0.499953</td>\n      <td>0.850290</td>\n      <td>0.842199</td>\n      <td>0.835786</td>\n      <td>0.850290</td>\n      <td>0.571660</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Final Evaluation Results:\n{'eval_loss': 0.41074472665786743, 'eval_accuracy': 0.863186077643909, 'eval_f1': 0.842511900678718, 'eval_precision': 0.8290872852149572, 'eval_recall': 0.863186077643909, 'eval_spearman': 0.5830561139484278, 'eval_runtime': 93.7507, 'eval_samples_per_second': 239.038, 'eval_steps_per_second': 3.744, 'epoch': 4.996431120628123}\nTraining completed. Model saved in ./final_model/ directory\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1765: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  order = pd.unique(vector)\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.8632\nF1 (weighted): 0.8425\nPrecision (weighted): 0.8291\nRecall (weighted): 0.8632\nSpearman correlation: 0.5831\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"17813"},"metadata":{}}]},{"cell_type":"code","source":"\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nimport pandas as pd\nfrom datasets import Dataset\nimport re\nimport pymorphy2\n\n# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞\nmorph = pymorphy2.MorphAnalyzer()\n\n# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ—Ç –∂–µ —Å–ø–∏—Å–æ–∫, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)\nstop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\n# –§—É–Ω–∫—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\ndef lemmatize(word):\n    return morph.parse(word)[0].normal_form\n\n# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\nmodel = AutoModelForSequenceClassification.from_pretrained('./final_model')\nprint(\"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ\")\nprint(f\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: {model.config}\")\n\n# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\nclassifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n    return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    processed_reviews = [preprocess_text(review) for review in reviews]\n    results = classifier(processed_reviews)\n    predicted_ratings = [int(result['label'].split('_')[-1]) + 1 for result in results]\n    \n    return predicted_ratings\n\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\nnew_reviews = input_reviews()\n\n# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\npredicted_ratings = predict_ratings(new_reviews)\n\n# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nif len(predicted_ratings) == 0:\n    print(\"–ù–µ—Ç –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"–û—Ç–∑—ã–≤: {review}\")\n        print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:38:12.044620Z","iopub.execute_input":"2024-07-14T11:38:12.045289Z","iopub.status.idle":"2024-07-14T11:38:17.262567Z","shell.execute_reply.started":"2024-07-14T11:38:12.045256Z","shell.execute_reply":"2024-07-14T11:38:17.261213Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a43eee278bd43f2a3c07946ba162fac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99195284d53b47b39f405c9a7201e26a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f33d843a6e4392aeed6eb2f31f9cde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c75036bfe848b1bb7328d5442d1bf6"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './final_model'.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepPavlov/rubert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./final_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:485\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './final_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."],"ename":"OSError","evalue":"Incorrect path_or_model_id: './final_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","output_type":"error"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nimport pandas as pd\nfrom datasets import Dataset\nimport re\nimport pymorphy2\n\n# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞\nmorph = pymorphy2.MorphAnalyzer()\n\n# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ—Ç –∂–µ —Å–ø–∏—Å–æ–∫, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)\nstop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\n# –§—É–Ω–∫—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\ndef lemmatize(word):\n    return morph.parse(word)[0].normal_form\n\n# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\nmodel = AutoModelForSequenceClassification.from_pretrained('./final_model')\n\n# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\nclassifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n    return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    processed_reviews = [preprocess_text(review) for review in reviews]\n    results = classifier(processed_reviews)\n    predicted_ratings = [int(result['label'].split('_')[-1]) + 1 for result in results]\n    \n    return predicted_ratings\n\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\nnew_reviews = input_reviews()\n\n# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\npredicted_ratings = predict_ratings(new_reviews)\n\n# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nif len(predicted_ratings) == 0:\n    print(\"–ù–µ—Ç –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"–û—Ç–∑—ã–≤: {review}\")\n        print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:38:56.721896Z","iopub.execute_input":"2024-07-14T11:38:56.722756Z","iopub.status.idle":"2024-07-14T11:38:57.629911Z","shell.execute_reply.started":"2024-07-14T11:38:56.722724Z","shell.execute_reply":"2024-07-14T11:38:57.628519Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './final_model'.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepPavlov/rubert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./final_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\u001b[39;00m\n\u001b[1;32m     23\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:485\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './final_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."],"ename":"OSError","evalue":"Incorrect path_or_model_id: './final_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","output_type":"error"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nimport pandas as pd\nfrom datasets import Dataset\nimport re\nimport pymorphy2\n\n# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞\nmorph = pymorphy2.MorphAnalyzer()\n\n# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ—Ç –∂–µ —Å–ø–∏—Å–æ–∫, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)\nstop_words = set(['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫'])\n\n# –§—É–Ω–∫—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\ndef lemmatize(word):\n    return morph.parse(word)[0].normal_form\n\n# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', model_max_length=256, use_fast=True)\nmodel = AutoModelForSequenceClassification.from_pretrained('./final_model')\n\n# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\nclassifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ndef preprocess_text(text):\n    if pd.isna(text) or not isinstance(text, str):\n        return ''\n    text = re.sub(r'[^–∞-—è—ëa-z\\s]', '', text.lower().strip())\n    return ' '.join(lemmatize(word) for word in text.split() if word not in stop_words)\n\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    processed_reviews = [preprocess_text(review) for review in reviews]\n    results = classifier(processed_reviews)\n    predicted_ratings = [int(result['label'].split('_')[-1]) + 1 for result in results]\n    \n    return predicted_ratings\n\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\nnew_reviews = input_reviews()\n\n# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\npredicted_ratings = predict_ratings(new_reviews)\n\n# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nif len(predicted_ratings) == 0:\n    print(\"–ù–µ—Ç –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"–û—Ç–∑—ã–≤: {review}\")\n        print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:20:18.554831Z","iopub.execute_input":"2024-07-14T11:20:18.555179Z","iopub.status.idle":"2024-07-14T11:21:14.133853Z","shell.execute_reply.started":"2024-07-14T11:20:18.555151Z","shell.execute_reply":"2024-07-14T11:21:14.132852Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdin","text":"–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  –ù–∞–º –æ—á–µ–Ω—å –ø–æ–Ω—Ä–∞–≤–∏–ª—Å—è –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π, –Ω–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ, —É–º–µ–Ω–∏–µ –≤—ã—Å–ª—É—à–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞ –∏ –ø–æ–Ω—è—Ç—å –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏. –û—Ç–º–µ—á–∞–µ–º –≤—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏ ‚ÄúKozhinDev‚Äù, –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–∂–∞—Ç—ã–µ —Å—Ä–æ–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞.\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):   –ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ –ø—Ä–æ–µ–∫—Ç, —á–µ—Ç–∫–æ–µ –≤–∏–¥–µ–Ω–∏–µ –∏ –ø–æ–ª–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¢–ó, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Å—É–∂–¥–∞–ª–∏ –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–æ—á–Ω–æ–π —Å–µ—Å—Å–∏–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–Ω—è–ª–∞ 3 –º–µ—Å—è—Ü–∞, –∫–∞–∫ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ—Å—å, –∞ –¥–∞–ª—å—à–µ –ø–æ—à–ª–∏ –¥–æ–∫—Ä—É—Ç–∫–∏ –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è. –°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å 2019 –≥–æ–¥–∞ –∏ –ø–æ —Å–µ–π –¥–µ–Ω—å.\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  –ú—ã –∑–∞–∫–∞–∑—ã–≤–∞–ª–∏ —É —Å—Ç—É–¥–∏–∏ KozhinDev —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —Å–∞–π—Ç–∞ –∏ –∞–¥–º–∏–Ω –ø–∞–Ω–µ–ª—å –∫ –Ω–µ–º—É. –†–∞–±–æ—Ç–∞ —Å–¥–µ–ª–∞–Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, –≤ —Å—Ä–æ–∫. –ú–µ–Ω–µ–¥–∂–µ—Ä –≤—Å–µ–≥–¥–∞ –Ω–∞ —Å–≤—è–∑–∏. –ü—Ä–∞–≤–∫–∏ –≤ –¢–ó –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –∏ –≤–Ω–æ—Å–∏–ª–∏—Å—å –≤ –ø—Ä–æ–µ–∫—Ç –±—ã—Å—Ç—Ä–æ –∏ –±–µ–∑ –≤—Å—è–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º. –ë–∞–≥–∏ —É—Å—Ç—Ä–∞–Ω—è–ª–∏ —Ç–æ–∂–µ –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ. –†–µ–±—è—Ç–∞ –æ—á–µ–Ω—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ, –≥–æ—Ç–æ–≤—ã –≤—ã—Å–ª—É—à–∞—Ç—å, –ø–æ–Ω—è—Ç—å, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å.\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  –ª—É—á—à–∏–π —Å–µ—Ä–≤–∏—Å\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  —Å—Ä–µ–¥–Ω–∏–π —Å–µ—Ä–≤–∏—Å\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  —Ö—É–¥—à–∏–π —Å–µ—Ä–≤–∏—Å\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  \n"},{"name":"stdout","text":"–û—Ç–∑—ã–≤: –ù–∞–º –æ—á–µ–Ω—å –ø–æ–Ω—Ä–∞–≤–∏–ª—Å—è –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π, –Ω–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ, —É–º–µ–Ω–∏–µ –≤—ã—Å–ª—É—à–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞ –∏ –ø–æ–Ω—è—Ç—å –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏. –û—Ç–º–µ—á–∞–µ–º –≤—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏ ‚ÄúKozhinDev‚Äù, –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–∂–∞—Ç—ã–µ —Å—Ä–æ–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞.\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 5\n\n–û—Ç–∑—ã–≤:  –ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ –ø—Ä–æ–µ–∫—Ç, —á–µ—Ç–∫–æ–µ –≤–∏–¥–µ–Ω–∏–µ –∏ –ø–æ–ª–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¢–ó, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Å—É–∂–¥–∞–ª–∏ –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–æ—á–Ω–æ–π —Å–µ—Å—Å–∏–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–Ω—è–ª–∞ 3 –º–µ—Å—è—Ü–∞, –∫–∞–∫ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ—Å—å, –∞ –¥–∞–ª—å—à–µ –ø–æ—à–ª–∏ –¥–æ–∫—Ä—É—Ç–∫–∏ –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è. –°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å 2019 –≥–æ–¥–∞ –∏ –ø–æ —Å–µ–π –¥–µ–Ω—å.\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 5\n\n–û—Ç–∑—ã–≤: –ú—ã –∑–∞–∫–∞–∑—ã–≤–∞–ª–∏ —É —Å—Ç—É–¥–∏–∏ KozhinDev —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —Å–∞–π—Ç–∞ –∏ –∞–¥–º–∏–Ω –ø–∞–Ω–µ–ª—å –∫ –Ω–µ–º—É. –†–∞–±–æ—Ç–∞ —Å–¥–µ–ª–∞–Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, –≤ —Å—Ä–æ–∫. –ú–µ–Ω–µ–¥–∂–µ—Ä –≤—Å–µ–≥–¥–∞ –Ω–∞ —Å–≤—è–∑–∏. –ü—Ä–∞–≤–∫–∏ –≤ –¢–ó –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –∏ –≤–Ω–æ—Å–∏–ª–∏—Å—å –≤ –ø—Ä–æ–µ–∫—Ç –±—ã—Å—Ç—Ä–æ –∏ –±–µ–∑ –≤—Å—è–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º. –ë–∞–≥–∏ —É—Å—Ç—Ä–∞–Ω—è–ª–∏ —Ç–æ–∂–µ –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ. –†–µ–±—è—Ç–∞ –æ—á–µ–Ω—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ, –≥–æ—Ç–æ–≤—ã –≤—ã—Å–ª—É—à–∞—Ç—å, –ø–æ–Ω—è—Ç—å, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å.\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 5\n\n–û—Ç–∑—ã–≤: –ª—É—á—à–∏–π —Å–µ—Ä–≤–∏—Å\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 5\n\n–û—Ç–∑—ã–≤: —Å—Ä–µ–¥–Ω–∏–π —Å–µ—Ä–≤–∏—Å\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 3\n\n–û—Ç–∑—ã–≤: —Ö—É–¥—à–∏–π —Å–µ—Ä–≤–∏—Å\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 1\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\ntry:\n    model = AutoModelForSequenceClassification.from_pretrained('./final_model')\n    print(\"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {model.num_labels}\")\n    print(f\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {model.name_or_path}\")\nexcept Exception as e:\n    print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:29:16.135432Z","iopub.execute_input":"2024-07-14T11:29:16.135978Z","iopub.status.idle":"2024-07-14T11:29:22.258460Z","shell.execute_reply.started":"2024-07-14T11:29:16.135927Z","shell.execute_reply":"2024-07-14T11:29:22.257138Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: Incorrect path_or_model_id: './final_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\n","output_type":"stream"}]},{"cell_type":"code","source":"def find_model_dir(start_path='.'):\n    for root, dirs, files in os.walk(start_path):\n        if 'config.json' in files and 'pytorch_model.bin' in files:\n            return root\n    return None\n\nmodel_dir = find_model_dir()\nif model_dir:\n    print(f\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—å—é –Ω–∞–π–¥–µ–Ω–∞: {model_dir}\")\nelse:\n    print(\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:32:20.810823Z","iopub.execute_input":"2024-07-14T11:32:20.811423Z","iopub.status.idle":"2024-07-14T11:32:20.823929Z","shell.execute_reply.started":"2024-07-14T11:32:20.811382Z","shell.execute_reply":"2024-07-14T11:32:20.822419Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nif os.path.exists('./final_model'):\n    print(\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è ./final_model —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\")\n    print(\"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:\")\n    print(os.listdir('./final_model'))\nelse:\n    print(\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è ./final_model –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:30:02.952800Z","iopub.execute_input":"2024-07-14T11:30:02.953514Z","iopub.status.idle":"2024-07-14T11:30:02.961671Z","shell.execute_reply.started":"2024-07-14T11:30:02.953475Z","shell.execute_reply":"2024-07-14T11:30:02.960137Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è ./final_model –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\n","output_type":"stream"}]},{"cell_type":"code","source":"# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    new_data = pd.DataFrame({'combined_text': reviews})\n    new_data['processed_text'] = new_data['combined_text'].apply(preprocess_text)\n    new_dataset = Dataset.from_pandas(new_data)\n    \n    tokenized_new_data = new_dataset.map(tokenize_function, batched=True, remove_columns=['combined_text'])\n    \n    predictions = best_trainer.predict(tokenized_new_data).predictions\n    if len(predictions.shape) == 1:\n        predicted_ratings = np.argmax(predictions.reshape(1, -1), axis=1) + 1\n    else:\n        predicted_ratings = np.argmax(predictions, axis=1) + 1\n        \n    return predicted_ratings.tolist()\n\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\nnew_reviews = input_reviews()\n\n# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\npredicted_ratings = predict_ratings(new_reviews)\n\n# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nif len(predicted_ratings) == 0:\n    print(\"–ù–µ—Ç –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"–û—Ç–∑—ã–≤: {review}\")\n        print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T10:59:29.578185Z","iopub.execute_input":"2024-07-14T10:59:29.578887Z","iopub.status.idle":"2024-07-14T11:01:04.624939Z","shell.execute_reply.started":"2024-07-14T10:59:29.578854Z","shell.execute_reply":"2024-07-14T11:01:04.623673Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdin","text":"–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  –ù–∞–º –æ—á–µ–Ω—å –ø–æ–Ω—Ä–∞–≤–∏–ª—Å—è –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π, –Ω–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ, —É–º–µ–Ω–∏–µ –≤—ã—Å–ª—É—à–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞ –∏ –ø–æ–Ω—è—Ç—å –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏. –û—Ç–º–µ—á–∞–µ–º –≤—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏ ‚ÄúKozhinDev‚Äù, –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–∂–∞—Ç—ã–µ —Å—Ä–æ–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞.\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  –ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ –ø—Ä–æ–µ–∫—Ç, —á–µ—Ç–∫–æ–µ –≤–∏–¥–µ–Ω–∏–µ –∏ –ø–æ–ª–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¢–ó, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Å—É–∂–¥–∞–ª–∏ –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–æ—á–Ω–æ–π —Å–µ—Å—Å–∏–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–Ω—è–ª–∞ 3 –º–µ—Å—è—Ü–∞, –∫–∞–∫ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ—Å—å, –∞ –¥–∞–ª—å—à–µ –ø–æ—à–ª–∏ –¥–æ–∫—Ä—É—Ç–∫–∏ –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è. –°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å 2019 –≥–æ–¥–∞ –∏ –ø–æ —Å–µ–π –¥–µ–Ω—å.\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  –ú—ã –∑–∞–∫–∞–∑—ã–≤–∞–ª–∏ —É —Å—Ç—É–¥–∏–∏ KozhinDev —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —Å–∞–π—Ç–∞ –∏ –∞–¥–º–∏–Ω –ø–∞–Ω–µ–ª—å –∫ –Ω–µ–º—É. –†–∞–±–æ—Ç–∞ —Å–¥–µ–ª–∞–Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, –≤ —Å—Ä–æ–∫. –ú–µ–Ω–µ–¥–∂–µ—Ä –≤—Å–µ–≥–¥–∞ –Ω–∞ —Å–≤—è–∑–∏. –ü—Ä–∞–≤–∫–∏ –≤ –¢–ó –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –∏ –≤–Ω–æ—Å–∏–ª–∏—Å—å –≤ –ø—Ä–æ–µ–∫—Ç –±—ã—Å—Ç—Ä–æ –∏ –±–µ–∑ –≤—Å—è–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º. –ë–∞–≥–∏ —É—Å—Ç—Ä–∞–Ω—è–ª–∏ —Ç–æ–∂–µ –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ. –†–µ–±—è—Ç–∞ –æ—á–µ–Ω—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ, –≥–æ—Ç–æ–≤—ã –≤—ã—Å–ª—É—à–∞—Ç—å, –ø–æ–Ω—è—Ç—å, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å.\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  –ª—É—á—à–∏–π —Å–µ—Ä–≤–∏—Å\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  —Å—Ä–µ–¥–Ω–∏–π —Å–µ—Ä–≤–∏—Å\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  —É–∂–∞—Å–Ω—ã–π —Å–µ—Ä–≤–∏—Å\n–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):  \n"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee62b4d5458a4f0d8b61d0ad04d51f89"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m new_reviews \u001b[38;5;241m=\u001b[39m input_reviews()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m predicted_ratings \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_ratings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_reviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predicted_ratings) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mpredict_ratings\u001b[0;34m(reviews)\u001b[0m\n\u001b[1;32m      8\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(new_data)\n\u001b[1;32m     10\u001b[0m tokenized_new_data \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mbest_trainer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(tokenized_new_data)\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     14\u001b[0m     predicted_ratings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'best_trainer' is not defined"],"ename":"NameError","evalue":"name 'best_trainer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T03:32:41.189003Z","iopub.execute_input":"2024-07-14T03:32:41.189464Z","iopub.status.idle":"2024-07-14T03:32:41.240673Z","shell.execute_reply.started":"2024-07-14T03:32:41.189434Z","shell.execute_reply":"2024-07-14T03:32:41.239660Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\ndef predict_ratings(reviews):\n    if len(reviews) == 0:\n        return []\n    \n    # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\n    new_data = pd.DataFrame({'combined_text': reviews})\n    new_data['processed_text'] = new_data['combined_text'].apply(preprocess_text)\n    new_dataset = Dataset.from_pandas(new_data)\n    \n    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –æ—Ç–∑—ã–≤—ã\n    tokenized_new_data = new_dataset.map(tokenize_function, batched=True, remove_columns=['combined_text'])\n    \n    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\n    predictions = trainer.predict(tokenized_new_data).predictions\n    \n    if len(predictions.shape) == 1:\n        predicted_ratings = np.argmax(predictions.reshape(1, -1), axis=1) + 1\n    else:\n        predicted_ratings = np.argmax(predictions, axis=1) + 1\n    \n    return predicted_ratings.tolist()\n\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\ndef input_reviews():\n    reviews = []\n    while True:\n        review = input(\"–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ (–∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è): \")\n        if review == \"\":\n            break\n        reviews.append(review)\n    return reviews\n\n# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤–≤–æ–¥–∞ –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã\nnew_reviews = input_reviews()\n\n# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\npredicted_ratings = predict_ratings(new_reviews)\n\n# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\nif len(predicted_ratings) == 0:\n    print(\"–ù–µ—Ç –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤.\")\nelse:\n    for review, rating in zip(new_reviews, predicted_ratings):\n        print(f\"–û—Ç–∑—ã–≤: {review}\")\n        print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {rating}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.138756Z","iopub.status.idle":"2024-07-12T20:22:24.139267Z","shell.execute_reply.started":"2024-07-12T20:22:24.138995Z","shell.execute_reply":"2024-07-12T20:22:24.139017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"–ù–∞–º –æ—á–µ–Ω—å –ø–æ–Ω—Ä–∞–≤–∏–ª—Å—è –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π, –Ω–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ, —É–º–µ–Ω–∏–µ –≤—ã—Å–ª—É—à–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞ –∏ –ø–æ–Ω—è—Ç—å –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏. –û—Ç–º–µ—á–∞–µ–º –≤—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏ ‚ÄúKozhinDev‚Äù, –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–∂–∞—Ç—ã–µ —Å—Ä–æ–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞.\r\n\r\n\r\n\r\n–ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ –ø—Ä–æ–µ–∫—Ç, —á–µ—Ç–∫–æ–µ –≤–∏–¥–µ–Ω–∏–µ –∏ –ø–æ–ª–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¢–ó, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Å—É–∂–¥–∞–ª–∏ –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–æ—á–Ω–æ–π —Å–µ—Å—Å–∏–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–Ω—è–ª–∞ 3 –º–µ—Å—è—Ü–∞, –∫–∞–∫ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ—Å—å, –∞ –¥–∞–ª—å—à–µ –ø–æ—à–ª–∏ –¥–æ–∫—Ä—É—Ç–∫–∏ –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è. –°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å 2019 –≥–æ–¥–∞ –∏ –ø–æ —Å–µ–π –¥–µ–Ω—å.\r\n\r\n\r\n\r\n–ú—ã –∑–∞–∫–∞–∑—ã–≤–∞–ª–∏ —É —Å—Ç—É–¥–∏–∏ KozhinDev —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —Å–∞–π—Ç–∞ –∏ –∞–¥–º–∏–Ω –ø–∞–Ω–µ–ª—å –∫ –Ω–µ–º—É. –†–∞–±–æ—Ç–∞ —Å–¥–µ–ª–∞–Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, –≤ —Å—Ä–æ–∫. –ú–µ–Ω–µ–¥–∂–µ—Ä –≤—Å–µ–≥–¥–∞ –Ω–∞ —Å–≤—è–∑–∏. –ü—Ä–∞–≤–∫–∏ –≤ –¢–ó –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –∏ –≤–Ω–æ—Å–∏–ª–∏—Å—å –≤ –ø—Ä–æ–µ–∫—Ç –±—ã—Å—Ç—Ä–æ –∏ –±–µ–∑ –≤—Å—è–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º. –ë–∞–≥–∏ —É—Å—Ç—Ä–∞–Ω—è–ª–∏ —Ç–æ–∂–µ –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ. –†–µ–±—è—Ç–∞ –æ—á–µ–Ω—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ, –≥–æ—Ç–æ–≤—ã –≤—ã—Å–ª—É—à–∞—Ç—å, –ø–æ–Ω—è—Ç—å, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å.\r\n\r\n\r\n\r\n–ö–æ–º–∞–Ω–¥–∞ KozhinDev —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –æ—Å—É—â–µ—Å—Ç–≤–ª—è–ª–∞ —Ä–∞—Å—á–µ—Ç –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –±–∏–∑–Ω–µ—Å –∑–∞–¥–∞—á–∏ –∑–∞–∫–∞–∑—á–∏–∫–∞. –†–µ–±—è—Ç–∞ –æ—á–µ–Ω—å —É–º–Ω—ã–µ, –±—ã—Å—Ç—Ä–æ –ø–æ–Ω—è–ª–∏ —Å—É—Ç—å –∑–∞–¥–∞—á–∏ –∏ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –ø—Ä–µ–¥–ª–æ–∂–∏–≤ –ª—É—á—à–∏–µ —Å–ø–æ—Å–æ–±—ã —Ä–µ—à–µ–Ω–∏—è. –ü—Ä–æ–µ–∫—Ç –±—ã–ª —Å–¥–∞–Ω —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞—à–µ–π –ø—Ä–∏–±—ã–ª–∏.\r\n\r\n\r\n\r\n–†–∞–±–æ—Ç–∞–ª–∏ —Å –∫–æ–º–∞–Ω–¥–æ–π KozhinDev –ø–æ –ø—Ä–æ–µ–∫—Ç—É –ò–Ω–≤–æ–π—Å‚Äë–±–æ–∫—Å, –æ—â—É—â–µ–Ω–∏–µ –æ—Ç —Ä–∞–±–æ—Ç—ã –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ. –í—ã–±—Ä–∞–ª KozhinDev –∫–∞–∫ –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤, –ø–æ—Ç–æ–º—É —á—Ç–æ –∑–Ω–∞—é –ø–æ–¥—Ö–æ–¥ –í–ª–∞–¥–∏–º–∏—Ä–∞ –∫ –ø–æ–¥–±–æ—Ä—É –ª—é–¥–µ–π, –∏ —á—Ç–æ —É –Ω–µ–≥–æ —Ä–∞–±–æ—Ç–∞—é—Ç –∫—Ä—É—Ç—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã. –ü–æ –∏—Ç–æ–≥—É –≤—ã–±–æ—Ä —Å–µ–±—è –æ–ø—Ä–∞–≤–¥–∞–ª, —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫, –∫–æ—Ç–æ—Ä—ã–π —Å –Ω–∞–º–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ—Ç, –∫—Ä—É—Ç–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç!\r\n\r\n\r\n\r\n–†–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏ –ü–û, —Å–≤—è–∑–∞–Ω–Ω–æ–µ —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –ø–æ api —Å–æ —Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ —Å–µ—Ä–≤–∏—Å–∞. –†–µ–±—è—Ç–∞ —Å —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–≥—Ä—É–∑–∏–ª–∏—Å—å –≤ –ø—Ä–æ–µ–∫—Ç, –ø–æ–º–æ–≥–ª–∏ –≤ –Ω–∞–ø–∏—Å–∞–Ω–∏–∏ –¢–ó. –ß—É–≤—Å—Ç–≤—É–µ—Ç—Å—è –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–æ–º–∞–Ω–¥—ã. –ë—É–¥–µ–º –∏ –¥–∞–ª—å—à–µ –ø–ª–æ—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å, —É–ª—É—á—à–∞—Ç—å —Å–≤–æ–π –ø—Ä–æ–¥—É–∫—Ç, –Ω–∞–ø–æ–ª–Ω—è—Ç—å –Ω–æ–≤—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º.\r\n\r\n\r\n\r\n–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å KozhinDev: —Å–∏–ª—å–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, –≤—Å–µ–≥–¥–∞ –Ω–∞ —Å–≤—è–∑–∏. –ù–∞ –≤—Å—Ç—Ä–µ—á–∞—Ö –∏ –æ–±—Å—É–∂–¥–µ–Ω–∏—è—Ö –ø—Ä–æ–µ–∫—Ç–∞ —Ç–∏–º–ª–∏–¥ –∏ –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞ –∑–∞–¥–∞–≤–∞–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –ë—ã–ª–∏ –≥–æ—Ç–æ–≤—ã –∏–∑—É—á–∞—Ç—å —á—Ç–æ‚Äë—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ. –ë—É–¥–µ–º —Ä–∞–¥—ã –ø–æ—Ä–∞–±–æ—Ç–∞—Ç—å –µ—â—ë.\r\n\r\n\r\n\r\n–ö–æ–º–∞–Ω–¥–∞ KozhinDev —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∞ –ê–ò–° –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å‚Äë–ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –Ω–∞—à–µ–π –ø—Ä–∏–µ–º–Ω–æ–π –∫–æ–º–∏—Å—Å–∏–∏, –æ—Å—É—â–µ—Å—Ç–≤–ª—è–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–π –ø—Ä–∏–µ–º–Ω–æ–π –∫–∞–º–ø–∞–Ω–∏–∏. –ü—Ä–æ—è–≤–∏–ª–∏ —Å–µ–±—è –∫–∞–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—ã —Å–≤–æ–µ–≥–æ –¥–µ–ª–∞, –≥–æ—Ç–æ–≤—ã–µ –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏–π—Ç–∏ –Ω–∞ –ø–æ–º–æ—â—å –∫–ª–∏–µ–Ω—Ç—É, –Ω–æ –∏, —Ä–∞–∑–æ–±—Ä–∞–≤—à–∏—Å—å –≤ —Ç–æ–Ω–∫–æ—Å—Ç—è—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –≤—ã–Ω–æ—Å–∏—Ç—å —Å–≤–æ–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏; –≠—Ç–æ –æ—Ç–∑—ã–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –∏—Å—Ö–æ–¥—è –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏","metadata":{}},{"cell_type":"code","source":"print(\"hello\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.140644Z","iopub.status.idle":"2024-07-12T20:22:24.140993Z","shell.execute_reply.started":"2024-07-12T20:22:24.140829Z","shell.execute_reply":"2024-07-12T20:22:24.140844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"hi\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.142067Z","iopub.status.idle":"2024-07-12T20:22:24.142435Z","shell.execute_reply.started":"2024-07-12T20:22:24.142260Z","shell.execute_reply":"2024-07-12T20:22:24.142277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\nimport torch\nfrom torch import nn\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom scipy.stats import spearmanr\n\nplt.style.use('seaborn-v0_8')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n\ndf = pd.read_csv('cleaned_kaspi_reviews.csv')\ndf = df[df['language'] == 'russian']\ndf['rating_10'] = df['rating'] * 2\n\ndef preprocess_text(text):\n    return text.lower().strip() if isinstance(text, str) else ''\n\ndf['processed_text'] = df['combined_text'].apply(preprocess_text)\ndf = df.dropna(subset=['processed_text', 'rating_10'])\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_df)}\")\nprint(f\"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(test_df)}\")\n\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"processed_text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\ntokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=['processed_text', 'combined_text'])\n\ntokenized_train = tokenized_train.rename_column(\"rating_10\", \"labels\")\ntokenized_test = tokenized_test.rename_column(\"rating_10\", \"labels\")\n\nclass RegressionModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.1)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.regressor(pooled_output)\n        \n        loss = None\n        if labels is not None:\n            loss = nn.MSELoss()(logits.squeeze(), labels)\n        \n        return loss, logits.squeeze()\n\nmodel = RegressionModel('cointegrated/rubert-tiny')\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    mse = mean_squared_error(labels, predictions)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(labels, predictions)\n    r2 = r2_score(labels, predictions)\n    spearman_corr, _ = spearmanr(labels, predictions)\n    accuracy_1 = np.mean(np.abs(predictions - labels) <= 1)\n    \n    return {\n        \"mse\": mse,\n        \"rmse\": rmse,\n        \"mae\": mae,\n        \"r2\": r2,\n        \"spearman\": spearman_corr,\n        \"accuracy_1\": accuracy_1\n    }\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=32,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_rmse\",\n    greater_is_better=False,\n    gradient_accumulation_steps=4,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\neval_results = trainer.evaluate()\nprint(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:\", eval_results)\n\ntrainer.save_model(\"./final_model\")\n\npredictions = trainer.predict(tokenized_test).predictions\nactual = tokenized_test['labels']\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=actual, y=predictions, alpha=0.5)\nplt.plot([1, 10], [1, 10], 'r--')\nplt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏')\nplt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏')\nplt.title('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏')\nplt.xlim(1, 10)\nplt.ylim(1, 10)\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.kdeplot(actual, shade=True, label='–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ')\nsns.kdeplot(predictions, shade=True, label='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ')\nplt.xlabel('–û—Ü–µ–Ω–∫–∞')\nplt.ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å')\nplt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫')\nplt.legend()\nplt.show()\n\nprint(f\"RMSE: {eval_results['eval_rmse']:.4f}\")\nprint(f\"MAE: {eval_results['eval_mae']:.4f}\")\nprint(f\"R2: {eval_results['eval_r2']:.4f}\")\nprint(f\"Spearman correlation: {eval_results['eval_spearman']:.4f}\")\nprint(f\"Accuracy within 1 point: {eval_results['eval_accuracy_1']:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:22:24.144701Z","iopub.status.idle":"2024-07-12T20:22:24.145100Z","shell.execute_reply.started":"2024-07-12T20:22:24.144894Z","shell.execute_reply":"2024-07-12T20:22:24.144911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}